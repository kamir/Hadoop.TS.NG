%ETOSHA.hint
%
%  REBUILD the nomenclature:
%     makeindex Dissertation.nlo -s nomencl.ist -o Dissertation.nls
%
% http://tex.stackexchange.com/questions/12703/how-to-create-fixed-width-table-columns-with-text-raggedright-centered-raggedlef
% http://latex.wikia.com/wiki/Int_(LaTeX_symbol)
% 
%
%
%
% ::TODO::   Checkliste zur Abagabe
%                - Führungszeugnis notwendig
%
%
\documentclass[a4paper,10pt]{scrbook}


\usepackage[a4paper, total={6.7in, 10.2in}]{geometry}

\usepackage{listings}

\usepackage{url}

% By default the URLs are put in typewriter type in the body and the
% bibliography of the document when using the \url command.  If you are
% using many long URLs you may want to uncommennt the next line so they
% are typeset a little smaller.
%    \renewcommand{\UrlFont}{\small\tt}

\usepackage{pdflscape}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{xcolor,colortbl}
\newcommand{\mc}[2]{\multicolumn{#1}{c}{#2}}

\definecolor{MBlue}{rgb}{0,0,1.0}
\definecolor{LightGray}{gray}{0.40}
\definecolor{LLGray}{gray}{0.45}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{b}{>{\columncolor{white}}c}

\usepackage[utf8]{inputenc}
%\usepackage[applemac]{inputenc}


\special{papersize=210mm,297mm}
  
\usepackage{amsmath} 
 
\usepackage{epsfig}

\usepackage[subfigure]{tocloft} 
\usepackage{subfigure} 
 
 
 

\usepackage{bbm}
\usepackage{sidecap}

\usepackage{pdfpages} 

\usepackage{floatrow}
\usepackage{multirow}

\usepackage{array}




%\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{bbm}
\usepackage[numbers]{natbib}

\usepackage[noprefix]{nomencl}
%\usepackage[refpage]{nomencl}

% we need thin lines as a border for our figures.
\usepackage{float}
%\floatstyle{boxed}
\restylefloat{figure}

\usepackage{graphicx} % Graphiken einbinden: hier f"ur pdflatex 


\usepackage{mathtools}

% side captions need this package
\usepackage{sidecap}


\usepackage{setspace}

%\graphicspath{ {/Users/kamir/Documents/github.workspace/dissertation/main/FINAL/LATEX/} }

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[EL]{\nouppercase\leftmark}
\fancyhead[OR]{\nouppercase\rightmark}
\fancyhead[ER,OL]{\thepage}

%% Dieses Package brauchen wir, um die Seitenbreite unterwegs zu verändern.
%% Titel, Inhalt, Literatur etc. sollen mit normalen Rändern gesetzt werden,
%% der eigentliche Diss-Text mit mehr Rand.
%% Wir benutzen den Befehl \changetext. Aus der Anleitung:
%     The \changetext command is for changing the size and horizontal position
% of the text block on a page. The command takes 5 arguments, each of which
% is a length or is empty. i.e.,
%
%
%\changetext{textheight}{textwidth}{evensidemargin}{oddsidemargin}{columnsep}
%
% The given lengths are added to the corresponding current lengths and
% the remainder of the current page is typeset using the changed text block
% layout. The new layout remains in effect until another \change... command
% is issued.
\usepackage{chngpage}
%% Nummerierung der Überschriften
\renewcommand \thechapter {\arabic{chapter}}
\renewcommand \thesection {\arabic{chapter}.\arabic{section}}
\renewcommand \thesubsection {\arabic{chapter}.\arabic{section}.\arabic{subsection}}
\renewcommand \thesubsubsection {\arabic{chapter}.\arabic{section}.\arabic{subsection}.\alph{subsubsection}}
\renewcommand \theparagraph {}     % oder: {(\arabic{paragraph})}

\newcommand\invisiblesection[1]{%
  \refstepcounter{section}%
  \addcontentsline{toc}{section}{\protect\numberline{\thesection}#1}%
  \sectionmark{#1}}

\renewcommand {\dictumwidth}{.65\textwidth}

%%Alternative: Paket "alnumsec" / s.u.
%% Numeriere 4 Ebenen tief (bis subsubsection) ...
\setcounter{secnumdepth}{4}
%% ... und nimm alle 4 Ebenen in das Inhaltsverzeichnis auf.
\setcounter{tocdepth}{4}

\usepackage{tabularx}
\usepackage{colortbl}

%\usepackage[rgb,table]{xcolor}
\usepackage{multirow}

\usepackage{arydshln}
 
% Farbpalette
\definecolor{LightGreen}{rgb}{0.835,1.0,0.843}
\definecolor{LightBlue}{rgb}{0.898,0.886,1.0}
\definecolor{LightRed}{rgb}{1.0,0.875,0.875}

\usepackage[toc]{glossaries}


% --- Abkürzungsverzeichnis: ----------------------------
% START % Näheres siehe http://my.opera.com/timomeinen/blog/show.dml/68644
% Befehl umbenennen in abk
%\let\abk\nomenclature
% Deutsche Überschrift
\renewcommand{\nomname}{Abbreviations and Notation}

% Punkte zw. Abkürzung und Erklärung
\setlength{\nomlabelwidth}{.10\hsize}
\renewcommand{\nomlabel}[1]{#1 \dotfill}

% Zeilenabstände verkleinern
\setlength{\nomitemsep}{-\parsep}
\nomlabelwidth=50mm
%\renewcommand*{\nomprefix}{}




%--------------------------------------------------------

%Tabellenoptimierung
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

%\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}} % linksbündig mit Breitenangabe
%\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}} % zentriert mit Breitenangabe
%\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}} % rechtsbündig mit Breitenangabe
%\newcommand{\ltab}{\raggedright\arraybackslash} % Tabellenabschnitt linksbündig
%\newcommand{\ctab}{\centering\arraybackslash} % Tabellenabschnitt zentriert
%\newcommand{\rtab}{\raggedleft\arraybackslash} % Tabellenabschnitt rechtsbündig




% allows a two line content in a table-cell
%
% FROM: http://tex.stackexchange.com/questions/2441/how-to-add-a-forced-line-break-inside-a-table-cell
\newcommand{\specialcellc}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand{\specialcelll}[2][l]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}





\usepackage{tocloft}

\setlength{\cftfignumwidth}{3em}

\makenomenclature

\makeindex


\begin{document}

\newgeometry{top=4in,bottom=1in,right=1.5in,left=1.5in}

%% Titelseite ausgeben
\begin{titlepage}

%% Den ganzen Text auf der Titelseite zentrieren
\begin{center}

\Huge Time-Series based Reconstruction and Analysis of Complex Networks -\\
\vspace{0.3cm}
\Large Methods for Quantitative Comparison of Dynamic Processes\\
\vspace{2cm}
\large Dissertation\\
zur Erlangung des \\
Doktorgrades der Naturwissenschaften\\(Dr. rer. nat.)\\
\vspace{2cm}
\large 
der Naturwissenschaftlichen Fakult\"at II \\
des Fachbereichs Phyisk \\
der Martin-Luther-Universit\"at Halle-Wittenberg\\
\vspace{1.5cm}
vorgelegt von \\
Dipl. Physiker Mirko K\"ampf\\
geb. am 16. April 1975 in Stollberg / Erzg.
\end{center}
\end{titlepage}

\newgeometry{top=0.65in,bottom=0.8in,right=0.6in,left=0.8in}

%% Inhaltsverzeichnis
%% Mit römischen Ziffern nummerieren
\pagenumbering{Roman}


%% Inhaltsverzeichnis ausgeben
\tableofcontents



\chapter{Preface}

%\label{ext.fig.preface_fig1} 
%\input{semanpix/preface_fig1/imageLS}

The Wikipedia project is a an excellent example of great success in several contexts. First to mention is the huge number of contributors and users which made Wikipedia to one of most often used web pages on the world wide web \footnote{According to \url{https://en.wikipedia.org/wiki/List_of_most_popular_websites} Wikipedia ranks as number six in the Alexa Traffic Rank in August 2015}. This is even more noticeable, sine no commercial interest is behind Wikipedia and the Wikimedia Foundation. A self organized global community of enthusiasts achieved a remarkable result within one decade - a free public encyclopedia which represents the worlds knowledge in more than 245 languages \footnote{In October 2015, Wikipedia has 245 sub projects which contain 10 or more articles and which received 10 or more edits in last month (see: \url{https://stats.wikimedia.org/EN/Sitemap.htm})}. Wikipedia's completeness and accuracy seemed to be possible only for commercial publishers before the public break through of the free global online encyclopedia Wikipedia. Instead of relying on a strong editorial process of a few experts, the contribution of a large public crowd of users (some times anonymous) is the fundamental base of Wikipedia. Second, the software behind Wikipedia was developed by a self organized group of open source developers. This illustrates, how free open software can be seen as a catalyzer for building open knowledge bases in a public space, such as the Internet. This kind of knowledge management should be considered to be a cultural achievement of recent history \footnote{According to Gesellschaft zur Förderung Freien Wissens e.V. Wikipedia deserves recognition and protection as UNESCO’s first digital World Cultural Heritage Site.}.  

A lot of different people use Wikipedia in different contexts. While the majority of Wikipedia users just reads available articles, a still large number of people contribute actively to this public knowledge base \footnote{The column labeled Participants in the first table on \url{https://stats.wikimedia.org/EN/Sitemap.htm} illustrates the ratio between active speakers of a language and the number of Wikipedia editors contributing to that language.}. Many different research projects are focused on Wikipedia. Some commercial products use Wikipedia to enrich their own data and even the recent gamification trend was not ignored by Wikipedia. "The Wikipedia Adventure" is an interactive game based tutorial to teach people how to contribute to Wikipedia. This way, the whole editorial process is less random and better organized while cultural differences and diversity still exists. 

%TODO complete the categories of Wiki Studies 
\textbf{FROM MY FOLDER WITH PAPERS about WIKIPEDIA I EXTRACT AND CITE THE MOST IMPORTANT ONES.
IMPORTANT ARE studies with relation to our work.}

%ETOSHA.note
% Details about Wikipedia research are available in Voss2005measuring: Measuring Wikipedia 
%  provides some good background and much simple count statistics
%
This work follows a generic approach as it uses Wikipedia data as a stub for arbitrary social media applications (SMA\nomenclature[R$S$]{SMA}{Social Media Application}). SMAs are usually online communication networks. They connect collaborating users and also allow a controlled communication among controversial users. This social network aspect is obviously a very dynamical part, usually embedded in a variety of complex systems. A more static or structural aspect is related to the content - usually a set of interlinked web resources. This aspect is called static, but in reality it is dynamically too. It evolves over time but on a different time scale. Both aspects can be modeled as individual networks (\textbf{see: SHOW WHICH STUDIES from above focus on USERS and which on CONTENT}). But obviously, the two different aspects can also be combined in one single complex system.
%TODO complete examples ... 

Wikipedia shows many properties of a complex system (see section \textbf{REFERENCE TO COMPLEX SYSTEMS Chapter}). The whole system should not be truncated into slices but rather be treated as a complex system, or even as a Network of Networks (NoN\nomenclature[R$N$]{NoN}{Network of Networks}), which is a representation of a complex system in more and more research projects (\textbf{CITE SOME NON ARTICLES}).
%TODO show examples ... 

This work initially started with individual sub projects focused on time series analysis. The final goal of this work is the development of a formalism and a methodology which acts as a blueprint for advanced data driven social media analysis - including dynamics, structure, and especially evolution of structural properties as a function of time within several domain specific contexts. Recent relevance studies about news articles and media coverage \cite{Segev2010} and market analysis \cite{Wang2006} are example use cases which can be generalized to technical systems such as sensor data analysis, predictive maintenance, traffic control, as well as to risk analysis and fraud detection in financial markets. This places the work into the young field of "Econophysics" introduced by Mantegna and Stanley in 1999 in theyr book "Introduction to econophysics: correlations and complexity in finance" \cite{Mantegna1999introduction}. 

A combination of different public available information sources from and about Wikipedia allows us to identify trends and to normalize measured data in a non parametric approach. We can describe the time evolution of a complex system, such as an emerging market, by calculating non accessible properties from a variety of direct measurable values. We used the emerging Hadoop market as a case study \cite{Kaempf2015}. Wikipedia provides primary data and background information around the topics belonging to the topic of interest. Furthermore, we can compare the representation of a particular topic - such as the Big Data market - within Wikipedia. This approach is built on data from arbitrary topics based on page content, structure, and usage patterns. Furthermore the growth rate and editorial dynamics of selected pages provide useful information. The growth of Wikipedia can simply be studied based on the edit history of all Wikipedia pages. The information retrieval process is studied with hourly click count data - both provided by the Wikimedia Foundation \cite{WikipediaFoundation}. In general, we combine several statistical methods from time series analysis and network analysis to describe a complex system without a need of complete segregation of aspects. 

Since the advent of Econophysics, which supports a totally new approach in interdisciplinary research and requires a connection of social science, economics, and natural science, many studies have investigated properties of social networks and especially socio-technical systems (\textbf{CITE SOCIONICAL}). However, while many social systems are intuitively connected with each other, little research exists on interconnections between coexisting dynamic aspects such as usage and growth. If a system grows, it is usually not in an equilibrium. This means, we have no stationary processes nor can we expect to measure stationary time series. On the other hand, a stable non growing system can be analyzed much easier. Often, the dependencies or relations between the sub systems is neglected or simply ignored completely for simplification. 

Another rather young research branch with a very strong inter-disciplinary focus is called analysis of Networks of Networks. This work contributes to this field as it provides data acquisition and preparation techniques to provide network representations for individual functional aspects of complex systems, which, if combined, form such NoN.  

Especially the recent emergence of large public data sets together with cloud based computation methods and highly scalable implementations of network analysis algorithms define the context for this research. Further motivation for creation of correlation and dependency networks comes from recently introduced network measures for multiplex networks.

Finally, the combination of available open data, new data preparation strategies, and new analysis methods can be seen as an important contribution to modern interdisciplinary data driven research. Wikipedia was in the focus of this research project, but it also was a great source for inspiration. The open public data, which Wikipedia consists of was produced by open software. Now we can see, that there is again a need for another kind of open software, which allows efficient analysis of this information. By establishing a closed loop data creation an analysis we are able to generate more than just more free data. Furthermore we can support knowledge creation and sustainable usage patterns for Wikipedia.  

 

\section{Motivation and Relations to Other Scientific Fields}
The graph in figure \ref{fig.ContextualizationOfTheThesis.b} shows the relationships between topics and the embedding in recent research activities, based on citations. The illustration was inspired by figure 1 in Tracey \textit{et al.} \cite{Tracey1998} (see figure \ref{fig.ContextualizationOfTheThesis.a}) This graph is already an example of a complex network which can be interpreted and analyzed in several ways. The purpose of this graph is simply to show a more detailed embedding of a scientific work with more information than a plain reference list can show. Obvious facts were derived from documents together with additional meta data - , in this case it is the structural information such as classification and the social embedding of the work. In some cases the meta data is available as explicit annotation or in a specific form, such as BibTex code. In other cases, one has to apply statistical methods to find this meta data (see chapter \textbf{FUNCTIONAL NETWORKS and KNOWLEDGE GRAPH}). And exactly this is what the following work is about.   

Information flows are in the focus of many researchers and the term attention economy is used more often. Ciampaglia et al. study the creational processes for information in this attention ecconomy also based on Wikipedia access-rate time series \cite{Ciampaglia2015}.

Information Flow in social groups 

%
%
%ETOSHA.LINK https://docs.google.com/document/d/1UipgV6NEW3RiY79FpI0AdkqTXP7kCdIhl0VyqMhWosw/edit


\label{ext.fig.ContextualizationOfTheThesis} 
\input{semanpix/ContextualizationOfTheThesis/imageLS}

SEE WIKI: \url{http://semanpix.de/opendata/wiki/index.php?title=Dissertation_Context} 

A Wikipedia-Context-Network for this work is shown in figure \ref{fig.ContextualizationOfTheThesis}.b. The purpose of such a graph is manyfold: (a) one can easily navigate to related but non obvious topics, (b) one can identify hot topics based on the evolution of the network structure if a time dimension exist, and (c) one can apply quantitative analysis to pairs of documents and their related context graphs. This allows again a context sensitive quantitative measurement as proposed in chapter ... but this approach is much more generic. It requires less specific data structures but on the other hand explicit software development is needed to provide a robust implementation. This graph is an example of a context-network of unstructured data (such as a scientific document) represented as a multi-layer network. The two layers were created as follows: 
\textbf{Layer 1:} First, the LaTex source code of the document was parsed and transformed into a tree including all parts, chapters, sections, and subsections. Within each structural element all citations (external relation), labels (internal relation), figures and tables (embedded relations) were identified and connected to the backbone. This kind of nodes represent important related facts which do not belong to the main text flow, the so called "red line". This layer contains all elements and all direct citations of the core document and covers all information as in Tracey \textit{et al.} (see fig. \label{fig.ContextualizationOfTheThesis}).

\textbf{Layer 2:} Keywords are provided by the author for each element. This activity is called tagging. Such an explicit annotation or explicit contextualization allows us to embed the document in arbitrary spaces, such as the news media or social media. Here, we connect the keywords to Wikipedia articles. An article is assumed to be relevant or related if the title matches the keyword exactly. Partial matches are not used currently since we would need a text search for page titles. Wikipedia has a rather simple REST API which allows a page lookup by name, but no direct search, but this will not be a limitation for our approach.

As soon as Wikipedia pages are identified we can use our context network approach to define the multilingual context network by following the inter-wiki links to pages in different languages. Such a graph should now represent the neighourhod of a given document and was created without any initial knowledge or classification.

\textbf{Explain the image and so explain the structure of this work.}

MORE DETAILS are in an email with subject: "Nice sketch for showing the embedding of my work" in Cloudera-mailbox.

\textbf{SEE EXTRA PRINT OUT !!!}

%\label{ext.fig.ContextualizationOfTheThesis2} 
%\label{ext.fig.ContextualizationOfTheThesis3} 

\input{semanpix/DocumentContextualization/imageLS2}

The primary focus of this work is on several data sets which represent complex systems. Beside a static representation of the system at a given point in time it is important to describe the dynamic properties. On the context of complex systems research it is even more important to handle the properties of individual elements and group properties including structural properties.

\input{semanpix/DocumentContextualization/imageLS3}

Many different algorithms and tools have been implemented as Open Source software and in commercial products. Because a system, which offers network analysis (NA), time series analysis (TSA) on top of a affordable and scalable storage and processing units a generic analysis workflow was developed in this work. For this reason, I defined the requirements for scalable time series processing methods (SEE FIGURE FROM TALK). Those methods can easily be integrated into existing network analysis tools, such as Gephi (CITE GEPHI) and Graphx (CITE GRAPHX). Specific TSA algorithms were implemented in a project called Hadoop.TS (CITE HADOOP TS).
In order to manage the analysis context and results in a consistent and traceable way the Etosha metadata management system was developed.

The TSA analysis methods were applied to data sets from Wikipedia, financial markets, a traffic management system, and from agent based simulations – one for traffic and one for pedestrians inside buildings. A comparison of average values obtained for different algorithms from specific groups allows an interpretation of process specific properties (edits vs. access process) and model validation (traffic data). Such calculations allowed us to test and study the behavior of the methods.

In a final phase we developed a procedure for contextualized complex systems analysis. Starting with time series data and some metadata, we generate networks which represent the systems structure during a time intervals. We define a generic procedure for calculation of time dependent structural measures. 

This tool allows us to study the representation of topics in a social content network, how much attention a topic attracts in one language, compared to the other languages over time and how structural changes coexist with changes of individual properties.  


 

This chapter highlights and reviews relevant concepts from the wide field of network science. Network science is an interdisciplinary scientific approach which combines Mathematics, Physics, Social science, and Economics with Computational sciences, Data Science, and Engineering. An appropriate integration of methods, tools and scientific standards is essential for success in this field. This work contributes to this integration. We improve an existing and develop a new procedure which both together allow data driven studies of large complex systems. As this is not a theoretical work, also the methodological framework and the software tools were developed. 

Results from application to several fields are presented in the third part.
This work is based on several data sets. Historical usage data of web resources, e.g., extracted from a social media application (SMA) which is Wikipedia in this case is one among others. No matter where the data comes from, there is a purpose behind collection data from a variety of different sources, which can be measurements or results from numerical simulations (see paper with Kulakowsky et al.) or even both (see our traffic paper).

Why do we use Wikipedia data? Because Wikipedia is a very large and well global online system. It is available in more than 230 languages and all kinds of mobile devices can access the service, which is hosted by the Wikimedia Foundation. Wikipedia usage is free. This means, we do not have to care about access limitations because of social status. Because Wikipedia is available since  (WANN WURDE WIKIPEDIA GESTARTET?) we can assume, that there is no disturbing effect from marketing hypes, related to individual devices. Example for such systems are the Google Play store and Apple iTunes which are restricted to specific devices. Social media systems can be restricted to a specific audience because of technical reasons or because of the focus, which can be on a very narrow group of users. Nowadays, Facebook is accepted by users from all generations, but "StudIP" was a very focused online community focused on Students and pupils from Germany (IS THIS CORRECT ???). Such restrictions do not exist for Wikipedia. But there are indirect restrictions, especially for political and economical reasons. Even if Wikipedia usage is in general for free, not all regions on earth are well connected via the Internet (SEE IMAGE ABOUT INTERNET COVERAGE). According to such factors one has to be aware of, that not all topics are represented equally, nor in all languages. But if one takes all information about a topic, which is available in any language in one of the many language specific subprojects into account, a broad and comprehensive representation of that topic can be expected. Finally, I think it is important to notice, that in Wikipedia no driving business interest is influencing what a user will see or what additional information might influence its behavior. This is not the case in Google Search. Google keyword history is a very often used system as it also enables a kind of tracebility of user attention. 

I learned during my work on Wikipedia data that one key factor for future research is availability of large data sets with well known properties. Proprietary software is not a problem in this case, but for all our data, especially as the datasets become bigger and the systems are interconnected, some better approaches are required in order to achieve transparency. It is also important to enable future researchers to build on top of data which is produced today. Public data catalogs should help to manage and conserve required knowledge about data set. As a side effect of this work, I propose a Wiki based collaborative approach to form a "data sets context graph". This data set graph can be compared to well studied citation- and co-authorship graphs \cite{TBD} (which are the result of  collaboration), to Wikipedia page networks (which are the result of community efforts to collect and conserve available knowledge), or even to Facebook and Twitter (which allow users to communicate very direct in specific contexts). The Etosha dataset graph wil become a database for researchers, which provides background information about research projects, applied methods, findings and problems. Especially interdisciplinary projects depend on existence of background information and efficient communication strategies. 

In this work we study Wikipedia as an example. It is not a long way to generalize the approach. 
Some inspiration came from traditional media analysis \cite{Segev2010} but for a very limited amount of data. Our approach based on Wikipedia allows already much larger studies on many more topics. If it would be applied to arbitrary web services in transparent way, all users could benefit from knowledge about Internet usage pattern. Beside internet usage, also the usage patterns in communication networks (see Kurtz, Potsdam und Mobile Phone usage) are in the focus of research and business development. Another driving force behind the technological (r)evolution of data analysis techniques and tools are financial institutions, the ecommerce sector and telecommunication companies. All those have the following in common: Data appears as soon as users or customers interact with the system. This interaction at a given point in time can lead to another related interaction and in many cases the system for itself changes as a result of the interaction. Either just another entity is available, a little bit more information about a user, some indirect relations between users as they did the same at the same time, or even a direct relation between users as they work on the same content or contribute to the same online resource. Future behavior of the system or the users should be predicted or at least the evolution of the system should be described in a meaningful way. To build such models is the scope of complexity science. The still growing field of Econophysics cares about the special case of economic system. Computational social sciences bridges the technical world with social science and the distance between computational science, applied physics, and mathematics is even shorter. In front of this interrelated background this work has to be understood as a starting point for an interdisciplinary starting in the field of time series analysis, preparing the data for network analysis in the context of economics or social studies. We combine new  measurement procedures, database technology, modeling techniques, and data analysis methods in order to solve a general problem: creation and description of correlation networks. 
 
\subsection{About Models: Multiple Meanings in Different Disciplines}
Analytical models - expressed as a set of differential equations - are common to describe real world phenomenon. Such formulas can usually be used only within certain boundary conditions. Simplification and incompleteness of such modela are the reasons for limitations. Limitted data sources, and limitted resolution or even unaccessable variables are another reason. Numerical simulations, such as agent agent based simulations produce also data - instead of measured data - for data analysis. Experiments are not bound to real world any longer, it is possible to conduct large scale experiments in modelled environments. Here, the term model has a different meaning. The model is not a set of formulas, but an artificial representation of reality, which allows experiments, with real people \textbf{SOCIONICAL.AllianzArena,SOCIONICAL.FAHRSimulator} \cite{SOCIONICAL.AllianzArena,SOCIONICAL.FAHRSimulator} or in a virtual space, e.g., to simulate traffic scenarios on high-ways \cite{Kantelhardt2013phases} or evacuation scenarios from buildings with complex geometry \cite{Gawronski2011evacuation} by computer software. Virtual reality combines both techniques \textbf{SOCIONICAL.Firemsim} \cite{SOCIONICAL.Firemsim}.   

The term model has another meaning in the scientific context. Engineering in general, especially construction and design, and even project management process can be modelled. The rational unified process (RUP) \cite{IBM.RUP} is an example of a very influencing process modell, which helped to overcome the software crisis (TIME). Such models can help to stay on track during day to day work. Creation of a new sattelite or a large telescope array are non standard procedures. Models of complex systems help in such cases to optimize cost efficiency and quality of results (see: \cite{Bunova2005} for an example of Multidisziplinary System Design Optimization). 

The goal of many projects is a precise prediction of future system properties (fully automatically), based on known facts (available data). Such data only represents the past. The ability to predict the future depends on: (a) availability of a deterministic model, and (b) complete knowledge about the system. This indicates clearly, which two major problems are related to predictive analysis: (a) one has to create an analytical model from a limitted amount of existing data. Machine learning is currently a very important topic. One uses data combined with adaptive algorithms to create a representation of the system which allows, to provide a response - the prediction - for new data, which was not used during the trainging period. But because the new input, representing the future, is unknown we are not in aggreement with (b) any longer. Many methods are available as a result of intense research work during the last decades. A strong focus is on Artificial Intelligence and Machine Learning. Linear regression, artificial neuronal networks, and Bayesian networks are three very prominent examples of such methods which are tightly bound to Computational Science. Nowadays, also data mining experts are working with such methods. The difficulty, coming from the different meanings of the term "model" in different disciplines can be confusing, especially in the case of interdisciplinary collaboration between mathematicians, physicists and engineers. 

\textbf{Network models} relevant for this work. A network model represents a class or a group of networks, which for itselfe are models of real systems. It is defined by some typical properties of a network (see chapter \ref{network.models} for more details) usually resulting from a typical creation procedure. A framework for complex systems research is developed in this work. Computational methods have to be combined in different ways. There is no complete formalism to validate all conditions completely automatically yet, but a set of core procedures (as described later in part II) can be combined. The so called data science process serves as a blueprint. A complete project model, described by an ontology is an ongoing task and will be finished in a future project. 
 
\textbf{Define:} Deterministic models

There are multiscale models, which combine different properties of a system. Since such properties can be relevant on some specific scales only, it is important to have a combination of models, in order to represent reality in a wider range of conditions. The FutureICT project proposes a software platform to integrate model components. It is called a "World of Modeling" and allows a combination of theoretically informed and empirically validated models on multiple scales to represent arbitrary aspects of a highly interconnected world (see: "New	 Ways to Promote Sustainability and Socia Well-Being in a Complex, Strongly Interdependent World: The	FuturICT Approach", p. 55 in \cite{Ball2012}). 

\textbf{Define:} Makroskopic, mesoscopic and microskopic models\\
 
\textbf{Define:} Semantic model and Data model 
 
\subsection{Linear Response Theory in Social Science}
\label{sec.linresponse}
\label{fig.LIN.RES.THEORY}

Beschreibe was Sornette gemacht hat und was es mit meiner Arbeit zu tun hat.


A response function $R$ describes the result of an interaction of a system $S$ with an arbitrary field $F(\vec{r}, t)$. The measureable property $R(r)$ is the expectation
value of an operator $\hat{x}(r)$. The general linear relation between x(r) and F is:
\begin{equation}
\delta x(\vec{r}, t) = \int \delta \vec{r} \int_a^b \! f(x) \, \mathrm{d}x.
\end{equation}
%http://felix.physics.sunysb.edu/~allen/Pdffiles/linres.pdf


Siehe handschriftliche Notizen in grüner mappe.

\subsection{Statistical Analysis of Media Presence and Media Coverage}
%ETOSHA.master.link Segev2010
Since online news portals and social networks emerged as an important source for distributing news in near real time to a broad (but still restriced) audience it is even more important to understand the dynamics of this distribution process. According to Elad Segev \cite{Segev2010}, “a report by Pew Research Center for People and the Press (2008) reveals that 40$\%$ of the Americans get their national and international news from the Internet. Similarly, Wurff and Lauf (2005) \cite{vanDerWurff2007} and Schifferes, Lusoli, and Ward (2009) \cite{Schifferes2009} have indicated a continuous growth in the online news readership in Europe, based on a study about online news consumption in the 2005 UK General Election". In \cite{Schifferes2009} the authors examine (1) the growth and patterns in online news consumption; (2) the ways in which website visitors' consume online news; (3) the potential links between online news consumption and political participation and voter turnout. This approach goes beyond the previously mentioned response theory and is typical in social science.


% Schifferes, Lusoli, and Ward
%
%Abstract 
%
%This article examines the role of online news consumption in the 2005 UK General Election. In particular, it uses public opinion survey data and log file data from the BBC News Election website to assess: (1) the growth and patterns in online news consumption; (2) the ways in which website visitors' consume online news; (3) the potential links between online news consumption and political participation and voter turnout. The data indicates that although online news consumption is still a minority interest, it is growing rapidly but as part of a wider news diet. In terms of the online news audience, a minority of politically interested and engaged citizens are using online sources to supplement and increase their political information. However the majority of online news surfers tend to either use it on a one-off occasion or for irregular monitoring of the campaign.

Segev raises the question regarding the biases which exist in such online news channels, because those could affect our perceptions of the world. He states, that together with better abilities to express local and national views, popular news Web sites may reinforce, for example, dominant American or western views over local views. In order to measure the representation of a country he introduces the salience index (Eg. \ref{GSI},. The local salience index (LSI, Eq. \ref{LSI}) and global salience index (GSI, Eq.\ref{GSI}) allow to identify how salient a country is (comparing to other countries) in its own news sites and how salient a country is in the news sites of other countries. Thus, the GSI of a country is defined as the percentage of news items that mentioned it (not including items from its own news sites) out of all news items that mentioned any country name. The well known TF-IDF measure, which is used to rank seach reaults in information retrieval procesdures has a comparable structure.

\begin{equation}
\label{GSI}
GSI
\end{equation}

\begin{equation}
\label{LSI}
LSI
\end{equation}

Since news articles appear dayly in a continuos flow on multiple channels it seems to be possible to track all news items which appear within a time period $t$, calculate $\rm {LSI}(t)$ and $\rm {GSI}(t)$ as a function of time. Because many people use Wikipedia read more - especially background information to trending topics - we consider Wikipedia as a source for a reference analysis. We can assume, that Wikipedia shows differences in user behavior, especially as a consequence of cultural differences. The report (CITE Report7) gives already an indication. 

Using Wikipedia as a source for a representation and relevance analysis requires a slightly different approach. Articles representing a topic, such as a country, exist in different languages and the content of the article differs much for all languages - because people in different countries have a different view on the same things or facts. Usually, Wikipedia articles are not changed every day - accept the articel is a controverse one and is part of a so called edit-war (\textbf{CITE YAZERI}). On the reader side one can expect increased activity in access-rates in the context of real world events. This relates the topic to the previously mentioned work from Crane  \textit{et al.} \cite{Crane2008}. Also the editorial activity can be stimulated as a consequence of blazing discussions (\textbf{SEE ARTICLE VON JAN 25.01.}). But in both cases it is not possible apply the LSI and GSI from Eq. \ref{GSI} and \ref{LSI} directly because the specific meaning of words as required for calculation of LSI and GSI for articles form more than two hundred different languages can not be done automatically. 

A different approach for news coverage analysis was published by Kutter and Kantner \cite{kutter2012corpus}. They describe a corpus-based content analysis, which is based on 'semantic fields', and thus allows semi-automatich analysis of texts in multiple languages. Semantic fields can be adopted as a heuristic tool for inference-making. They suggest to use it in order to relate smantic information to specific conceptual categories of a social science research project. 

This work builds on the two publications \cite{Segev2010} and \cite{kutter2012corpus}. We introduce a new approach to measure the local and global representation of topics in Wikipedia. We address the same problem as Segev but use a different source for information flow analyis. Instead of semantic fields, we use the local neighborhood networks and inter-wiki links. 

%ETOSHA.NOTE Need the printout of the SEGEV paper.
%Segev, E. (2010): 
%https://docs.google.com/file/d/0B-k-FXlNIDy-OUc0RzJPTXFqYjg/edit?usp=drive_web
%https://docs.google.com/document/d/1Lfmr-hoF39OIiM4bJrPQ73tgdyo40wEw_WBM-6aezlY/edit
%https://docs.google.com/presentation/d/1LFN79l95VMPctppz7aUv1YgXQSPUQ9j0ki1OSNBtUMQ/edit#slide=id.gc994cb227_0_15
%
% LAST LINK HAS FORMUALS AND IS STARTING POINT FOR SLIDES....
%

Beside content based analysis with a focus on representation of topics or reputation of individuals there are many other approaches to media analysis. Zammit-Mangion \textit{et al.} \cite{Zammit-Mangion2012} apply a dynamic spatitemporal modeling technique to identify the properties of an underlying process about which the articles report. They use the Afghan War Diary to describe the conflict and to train a latent Gaussian model. They state, that the nature of this specific conflict for itself seems to be the reason why the simple Poisson point process works so good in this case. They write, that this special conflict is characterized by insurgent movements and qualifies as a case of irregular warfare where activity is only loosely dependent and actioned by a myriad of disparate groups. 
This aspect is related to our work \textbf{(see results in chapter ... )} where we want to identify such a property. One very importatnt queston is: Can we show, that our assumption, that Wikipedia users act also as independent individuals or can we identify strong correlations cused by strong dependencies among users on short or longer time scales?  
  
\section{Scientific Contributions}
%
%ETOSHA.check HIER MÜSSEN ALLE MEINE PAPER zitierte SEIN
%

Research questions and hypothesis have to be defined carefully. Especially if data is already available before a question is asked or a hypothesis is defined it is possible to align the research question too much with existing data. 
This approach is dangerous and leads to wrong results, although dataexploration is an important part in the data science process \cite{ONeil2013}. According to \cite{Good2012} (p.16) "\textit{A well-formulated hypothesis will be both quantifiable and testable, that is, involve measurable quantities or refer to items that may be assigned to mutually exclusive categories}". This means, a good hypothesis is one which can be verified and validated by application of statistical tests. 

A quantitative analysis is not possible directly in many cases because the variables one is interested in are not accessible directly. This means, one can not measure interactions or relations between elements or subsystems directly. Indirect measurements have to be used instead. Such indirect measurement is based on well known or assumed relations between the accessible variable and the target variable. Correlation analysis reveals even more details than analyis of individual measurement results. One has to differentiate aspects which are simply not measurable and such effects, which do not exist in isolated systems. In general, not all hidden variables are also emerging phenomena.
%ETOSHA.todo define emerging phenomena

A potentially disturbing effect of measurement procedures and data collection techniques on the underlying system has to be evaluated. This ensures, that the process which is in the focus of a study is not manipulated in an unwanted way. The number of measurements and simulations - in general the number of conductable experiments - is limited. One has to select the right sampling methods and an appropriate definition of the study focus to eliminate the so
called selection bias best as possible. If an artificial bias is introduced, it is important to quantify it as already menitoned.

Since the advent of Big Data technology (including hardware and software) analytical methods have to be developed further. Especially new study types, based on a variety of larger, more diverse data sets allow integration of different currently isolated scientific disciplines. It is necessary to identify the right scope. An appropriate resolution for time series data, the right number of neighbor nodes within a network, and the right ranges for sliding window analysis have to be choosen. Especially when data is collected from individual but dependent - or at least related systems - with different measurement techniques, it is important to align the data series and to normalize the measured data appropriately. Therefore, normalization, filtering and (re)sampling become fundamental elements in study preparation and experiment design. Finally, terminology has to be integrated or translated between different scientific disciplines in order to have a benefit from existing best practices.

\subsection{Addressed Problems and contributions}
The following four specific problems were addresses in this work during several sub-projects:

\textbf{(P1) Uni-variate and multi-variate times series analysis}: Analysis of traffic data (measured and simulated), evacuation simulation results, and social media usage data was done using established methods, such as Dentrended Fluctuation Analysis (DFA\nomenclature{DFA}{Detrended Fluctuation Analysis}), return interval statistics (RIS\nomenclature{RIS}{Return Interval Statistics}), Cross-correlation analysis (CC\nomenclature{CC}{Cross-Correlation or Pearson-Correlation Analysis}) and event-synchronization (ES\nomenclature{ES}{Event-Synchronization}).

\textbf{(P2) Studie design for interdisciplinary computational science}: Common studies on social online media includes a strong selection bias. In many cases not much is known about the user's properties. In other cases, demographic, geographic, economic, cultural and even political issues influence online plattforms. Studies on massive online data can be improved and validated if comparable reference data is available. We consider Wikipedia as a potential source for context networks to identify trends and contextual bias.

\textbf{(P3) Detrending of raw media usage data}: Development of new normalization methods and measures for media usage analysis allows independent analysis of different communication channels and finally a comparison of
those. In general, one can analyze individual communication channels as done by many researchers, but the remaining challange is to understand how the importance, acceptance and reliability of different channels changes over time. 

\textbf{(P4) Integration of multiple facettes of a complex system}: Generalization and integration of existing network creation and analysis methods enables us to develop a unified framework for large scale simulations and data analysis as a tool for complex systems research.

%
% Review: Presence Measures
%
%ETOSHA.ref \cite{Biocca2003}
%http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6790590&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6790590
%At a time of increased social usage of net and collaborative applications, a robust and detailed theory of social presence could contribute to our understanding of social behavior in mediated environments, allow researchers to predict and measure differences among media interfaces, and guide the design of new social environments and interfaces. A broader theory of social presence can guide more valid and reliable measures. The article reviews, classifies, and critiques existing theories and measures of social presence. A set of criteria and scope conditions is proposed to help remedy limitations in past theories and measures and to provide a contribution to a more robust theory and measure of social presence.

%
% Presence Measure: Subjective vs. Objective
%
%ETOSHA.ref \cite{IJsselsteijn2000}
%http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=920235
%The concept of presence, i.e. the sensation of 'being there' in a mediated environment, has received substantial attention from the virtual reality community, and is becoming increasingly relevant both to broadcasters and display developers. Although research into presence is still at an early stage of development, there is a consensus that presence has multiple determinants. To identify and test which parameters affect presence, a reliable, robust and valid means of measuring presence is required. In this paper, we describe the categories of factors thought to have an impact on presence. Furthermore, we present an overview of various approaches taken to measuring presence, which can be divided into two general categories: subjective measures and objective corroborative measures. Since presence is a subjective experience, the most direct way of assessment is through users' subjective report. This approach has serious limitations however, and should be used judiciously. Objective measures, such as postural, physiological or social responses to media, can be used to corroborate subjective measures, thereby overcoming some of their limitations. At present, the most promising direction for presence measurement is to develop and use an aggregate measure of presence that is comprised of both subjective and objective components, tailored to the specific medium under study.


The following section provides a high level explanation and some background information to this problems and also, the specific contributions, provided by this work.
    
Initially we were interested in the properties of non stationary processes in socio-technical systems \textbf{(CITE SOCIONICAL)}, especially in Wikipedia. In (P1) we study creative and intellectual processes in which Wikipedia pages are created and edited by a group of individuals and the information consumption process by millions of anonymous readers. As a third potentially related process we consider changes in price and trading volume in financial markets. All those processes might be related to each other and thus they might also influence each other.  (\textbf{RELATION TO SORNETTE)} \cite{Crane2008}, and \textbf{KULAKOWSKY - SOCIONICAL-Traffic, EVACUATION})

We want to understand how those processes are coupled and if the coupling or the changes change over time. What properties do affect the structural properties of the underlying network? What is the right time scale which allows us to find correlations? For now we do not yet look into causation between the considered processes.

The definition of the right sample group is complicated in a multilingual global social network. E.g., lists of companies in a stock market (the component of an index) are well defined. Companies offer information in public web pages, but in different languages such pages vary in in content and of cause in user attraction. Also the Wikipedia pages about about those companies are not comparable to each other. Most importantly because the pages are written in different languages and speakers of different languages are interest in financial topics not evenly.
In many cases multiple pages exist for one company. Measurement of the level of representation of topics, such as companies, can be based on the definition page.
The available amount of information in Wikipedia differs dramatically, especially if data is aggregated from all available languages it is important to normalize this data series. 

In order to address such hidden bias, we developed the representation index of Wikipedia pages (P3) and study them in context networks (P2). This allows a context sensitive analysis and comparison of content extracted from arbitrary content networks, not only Wikipedia (P4). The representation index allows us to describe the data set in more details. Since interpretation of final results should always be embedded into a clear context which shows, if the findings are in line with the hypothesis or even caused by external hidden or artificial influences.

%
% Herausstellen der wichtigsten Ergebnisse und Betonen meiner Beitr\"age
% 
To model complex systems means to describe the properties of the system in the researchers focus in such a way which allows 
a quantification of several of it's sometimes even hidden aspects, or in simple words: we want to meassure properties of 
complex system, even if a direct measurement is not possible. 
Such indirect methods require the representation of complex system within a computational environment because one has to 
compute some not measurable properties or one wants to simulate the systems behavior to study the time evolution. Therefore one needs 
appropriate representations of complex systems which can be used in HPC \nomenclature{HPC}{High Performance Computing}
 and HPA \nomenclature{HPA}{High Performance Analysis} environments. 
For several years the network approach has been evolved and more and more research is done with a strong focus on complex networks.
A quite new trends is the research in networks of networks. Such coupled networks show special properties which are not
observed in isolated networks. Especially such effects which are results of interactions are responsible for the applicability 
of the name \textbf{complex systems}. 

Instead of simplification achieved by chopping a system into it's pieces new approaches are necessary. Beside some technical 
requirements, which are related to IT engineering and IT operations, also new analysis methods are necessary. Optimal data representation and access patterns are crucial aspects in this context. This is also comparable to many other disciplines in research.
The question drives the technology which is used to answer a certain question and with advanced technology one can answer many
question in more detail or even faster and more efficient than before. This shows, how concepts, theorie, technology and data, 
derived from real world systems are related to each other. If one wants to understand processes behind large social networks, one
has to work with data, derived from such networks. Preparation of raw data with repeatable and reliable procedures require special technology and
procedures. Large scale storage and distributed computing are not new. Network algorithms can also be applied to very large networks in HPC but with limited feature sets.
So far there is no general approach to handle large time dependent complex networks available. We need a system which is able to
connect different views. On one site, time dependent properties of the many constituents of the system are measured, stored, and analyzed. 

Multi-particle systems are often expressed as vectors in physics. The state of the system at any point in time can be expressed by a Matrix, which contains
a row for each element. But what about the connections and the interaction between the elements or between elements in several groups, which can be seen as
subsystems. And how to define such a subsystem? How to handle different types of interactions between elements? A typical problem
in physics is the motion of solid bodies or particles. Different types of interactions between particles can be, e.g. the gravitation and 
electromagnetic forces. Both together have an influence on the trajectory of the particle. In this situation one knows much about 
the way of interaction and a common law - the equation of motion - can be written down and solved. Although the presence of the gravitational field is unlimited one
skips the interaction between components which are far away from each other. This makes computation and models much easier.
But in social networks we can see totally different properties. There is no gravity or electro-magnetical force. What does it mean, two persons are far away or attract each other in the context of a social network? In terms of their location
it might be thousands of miles but in terms of communication, it is just one call or a digital message between them. In this situation one has not any more only a single value which determines the distance. The distance of nodes 
within in a social network usually depends on the selected properties to construct the connecting network. And as such a connecting network is usually not stationary nor is it limited to a fixed set of types of links. It is very important to track and to incorporate such information in new analysis algorithms.

Multiplex or Multi-layer algorithms can help to connect existing models which express special in isolation already well studied phenomena and properties on different 
scales. A large model, which covers multiple effects in one embedded environment is the result of our effort to address (P4). 

Such integrated research has to be done very carefully. Not all possible integrations are useful but some might be. E.g. in the context of socio-physics one can handle the individual behavior of each person and in the case of a group of persons, some group dynamics can change the average behavior of the group
in a different way which can not be determined by just looking on the measurable averages. Such emergent properties are critical since they are not known and not obvious. In which case do we have to consider
the properties and the influence of single elements and in which case are the group properties more relevant? 

This question can 
not be answered without a given context and without impact analysis - but therefore we need appropriate models.

\textbf{CITE SOCIONICAL INTEGRATION APPROACH and MODEL COMPARISON}

So far, we can find many rather specific models which are able to expain a certain aspect but without 
looking into the environment which means one has either a limitation in the size of the system or one disconnects the components of the system - but without it's original embedding into it's environment many system change their behaviour.  

Many socio-technical systems are related to human motion, not necessarily by vehicles, but also by walking in complex geometries like large buildings or even cities. One can work with the trajectory for a person which is walking around in a city \textbf{(MALTA)} or a stadium \textbf{(MUNICH)}. For a group of persons
one has to track each persons trajectory together with structural information, which describes, if the persons a related to each other
and if they form a group, which might be the source for "groupdynamics"\textbf{ (HELBING BOOK)}. But for thousands of persons it would be a quite large data set, which
has to be handled. And in this case it becomes important to have multiple models for different levels of details and different scales which are connected to each other. Furthermore it is important to be able to work in combined HPC and HPA environments.
With such large scale multilevel models it will be possible to study unknown effects and phenomena. It is also important to study how such model transitions influence the simulation results.

 
 
 
 
The contributions to the field of complex systems research can be organized in three groups: 

\textit{\textbf{I.} Creation and manipulation of functional networks based on cross-correlation and event-synchronization}

\textbf{Contribution 1 :} Vergleich beider Methoden auf Basis von Surrogat-Daten und echter Zeitreihen, besonders Beruecksichtigung der einzelnen Eigenschaften der Zeitreihen, die fuer Korrelationen eine Rolle spielen koennen. 

\textbf{Contribution 2 :} Untersuchung des Einflusses der verschiedenen Filter- oder Aggregats-Funktionen auf die damit herausgehobene Sicht auf das System.

\textit{\textbf{II.} Visualization and characterization of time dependent functional networks, in order to describe dynamic processes within complex systems}. 

A well known static network structure is used to quantify network properties but in some cases functional networks are not obviously visible. One has to compute correlation-links from data which was measured individually at the network nodes. The underlying structure 
might be hidden but it might be accessable through a dynamic correlation network. Such correlation networks are used as a tool to measure different types of hidden interactions between network nodes.

\textbf{Contribution 3 :} Wir nutzen dynamische Eigenschaften, die nicht in dem Element enthalten sind, sondern die sich aus der WW mit anderen Systemen ergeben.

\textbf{Contribution 4 :} Wir verbinden Zeitreihenanalyse mit der Erzeugung von Netzwerkrepräsentationen, deren Analyse wiederum zu Zeitreihen fuehrt, damit lassen sich Systeme auf verschiedenen Skalen modellieren und die Modelle miteinander koppeln.

\textit{\textbf{III.} Übergang von Eigenschaften einzelner Elemente zu Gruppen bis hin zum Complexen Systemen.}

Bei diesem Ansatz geht es um die kollektiven Eigenschaften von Gruppen und Subsystemen, die sich aus dem Individualverhalten ergeben und auf selbiges zurueckwirken und dadurch sichtbar werden.

a) Phasendefinition auf Systemebene und Phasenuebergaenge

\textbf{Contribution 5 :} Diskussion der Phaseneinteilung bei Verkehrsdatenanalysen auf Basis der Fluctuationseigenschaften und der Kreuzkorrelation

b) Beziehungen zwischen Subsystem beschreiben

\textbf{Contribution 6 :} Analysis and Characterization of information flow in the social content nework Wikipedia. 

\textbf{Relevance Measure}
\ref{chap.RELEVANZ}

This work contributes to the se problems by developing and evaluating 
novel techniques for a time-dependent characterization of the global and local 
relevance of WWW content based on cross-correlations in user-access time series, in 
edit-event time series, and in document length. 

We focus on content, user activity, 
and editor activity in selected groups of Wikipedia articles as a first application 
mainly because of data availability. 

Our goal is the assignment of a ranking value 
to a hypertext page (node), which covers static properties of the given node and its 
neighborhood (context) as well as dynamic properties derived form usage and edit 
processes. 

We show in several examples how this goal is achieved.


\subsection{Structure of this work}
\textbf{CHECK ALL CHAPTER NUMBERS}

This work consists of three main parts followed by a reference section. A fourth part can be seen as an appendix.  

\textbf{Part I}: Chapter 1 provides an introduction to the scientific discipline "Complex Systems Research". Relevant aspects from network theory are presented in chapter 3 and related topics from social network research are shown in chapter 4. In this chapter it will be shown, that Wikipedia can be considered to represent a complex system. Finally, all used mathematical methods - most importantly Detrended Fluctuation Analysis (DFA), Return Interval Statistics (RIS), Cross-Correlation (CC), and Event-Synchronisation (ES) for pairs of time series - are introduced in chapter \textbf{5}.

\textbf{Part II}: New analysis methods and data preparation methods are introduced in part II. Chapter 6 introduces the idea of neighborhood networks which define the context for a given network node. Such neighborhood graphs allow a contextual normalization or detrending of raw data. More details about study design and data preparation procedures are described in chapter 7. Especially in case of long term studies it is important to know the stability of structural properties of the system. In chapter \textbf{8} we discuss the problem of non-stationary real networks and how to overcome the limitations. As an example, we study and compare the life cycle of several Wikipedia sub-projects in different languages.  In order to connect physics, social science and research in economy we propose a generic framework for network creation from measured data. This procedure allows systematic operations on arbitrary time series data sets and gives the ability to compare results from and integrate data from different sources. Creation of functional networks is the central element, and thus, the selection of the right measure for link strength calculation is essential. Chapter 10 shows difficulties during link strength interpretation. How artificial links and real links can be separated will be shown and an auto-adaptive filter method is demonstrated. The last chapter in part II introdces two new measures, the representation index and the time resolved relevance index (TRI), which both can be used to classify network resources based on the local or global relevance. Since changing network structures can be found in functional networks we need a method to quantify the underlying dynamics. Based on the idea of force directed layout, applied to create natural and aesthetic representations of networks I us a new concept, called structure induced stress and explain it in chapter \textbf{12}.

\textbf{Part III}: Results from several sub-projects are combined in part III. Chapter 13 shows a characterization of the overall editorial and knowledge consumption processes in Wikipedia. A purely data driven market study, based on Wikipedia pages is presented in Chapter 14. Here, I compare the two approaches based on Wikipedia data (completely open) and Google trends data (free, but not open). The new methods developed in this work are finally applied to the special field of financial market studies. Chapter 15 gives a conclusion and provides an outlook into the next steps of my research. 

\textbf{Part IV}: The application of some fundamental time series analysis methods gave us some interesting inspiration. Two previously published documents are presented in part IV. Chapter 16 shows our approach to study stationarity of an agent based evacuation simulation. A comparison of an agent based traffic simulation and real traffic data shows a difference, especially regarding the inherent correlation properties. This work is presented in chapter 17. 
  
%ETOSHA.link \url{http://columbiadatascience.com/2012/09/24/reflections-after-jakes-lecture/}

%ETOSHA.link.label "Law and Ethics of Online Human Subject Experiments"
%ETOSHA.link %\url{http://www.datasciencecentral.com/profiles/blog/list?user=02bj314n4m3i2&page=2}


Frankleben, April 16th 2015
Mirko Kämpf









\newpage
\pagenumbering{arabic}

\setchapterpreamble[u]{%
\dictum[Albert-L\'{a}szl\'{o} Barabási, \textit{Nature Physics}, \textbf{8}, 2012]{Reductionism, as a paradigm, is expired, and complexity, as a field, is tired. Data-based mathematical models of complex systems are offering a fresh perspective, rapidly developing into a new discipline: network science.}
\vspace{1cm}
}
%ETOSAH.note http://judithcurry.com/2012/01/25/nature-physics-insight-complexity/

\part{Introduction to Complex Systems and Networks}

\setchapterpreamble[u]{%
\dictum[Albert-L\'{a}szl\'{o} Barabási, \textit{Nature Physics}, \textbf{8}, 2012]{Reductionism, as a paradigm, is expired, and complexity, as a field, is tired. Data-based mathematical models of complex systems are offering a fresh perspective, rapidly developing into a new discipline: network science.}
\vspace{1cm}
}

\chapter{Complex Systems}
Complex systems are in the focus of interest of many interdisciplinary research projects. Initially this branch of research was called \textit{Systems Theory}. Later, the field evolved into \textit{Cybernetics}. The current or modern form finally is called \textit{Complex Systems Research}. It is much more than a theory  and has applications in many different fields. According to Newman \cite{Newman2011} "\textit{A complex system is a system composed of many interacting parts, often called agents, which displays collective behavior that does not follow trivially from the behaviors of the individual parts}".

%ETOSHA.TODO
% The inspiration_node tag provides a classification of the node type of an inspiring node.
% (a) review
% (b) method
% (c) controversial
%
%ETOSHA.INSPIRATION_NODE.review Newman2011  

 They exist on several scales. Microscopic, mesoscopic , and also macroscopic systems can be complex. They have many properties in common, but therefore a certain abstraction is required. 
Complex systems are represented by networks. A network consists of nodes and links. Nodes represent the objects and links the relations between objects which can be either just conceptual links or real interactions caused by forces. The alternative term graph is preferably used in the mathematical context but in principle it has the same meaning or expresiveness like the term network.
The following section introduces several examples of complex systems and summarizes the typical properties. 

According to Pietschmann \cite{Pietschmann2011} a dominant philosophical principle or paradigm is the mechanistic mindset (original: "mechanistische Denken der Neuzeit") is based on four components:

(1) Everything which can be measured should be measured. The philosopher, mathematician, physisist and astronomer Galileo Galilei (1564-1642) influenced the evolution in science, especially in natural science during the first half of the 17-th century.

(2) Alles in kleinste Teile zerlegen. Dieses Prinzip gründet wesentlich auf den Theorien und Schriften von René Descartes (1596-1650).

(3) Entweder -- oder. Obwohl Aristoteles, einer der bedeutendsten und einflussreichsten Philosophen, Hunderte Jahre vor den zuvor genannten Philosophen gelebt hat (384 v.Chr. -- 322 v.Chr.), beeinflusste Aristoteles unsere Kultur erst ab ca. dem Jahr 1200. Der Grund hierfür liegt darin, dass seine Lehre erst zu dieser Zeit gemeinsam mit der arabischen Zahlenlehre nach Europa kam.

(4) Ursache -- Wirkung. Diese vierte "Säule des mechanistischen Denkens der Neuzeit" gründet laut Prof. Pietschmann wesentlich auf den Arbeiten von Isaac Newton (1643-1727). Pietschmann führt in dem Vortrag eindrücklich und mit viel Humor aus, wie stark diese Paradigmen unser tagtägliches Denken und Handeln beeinflussen. Und: Wer den Denkrahmen nicht einhält, wird in der Regel lächerlich gemacht.

Pietschamn says: "one should differentiate without separation". Complex systems analysis in general requires a comparable approach. One goal of this work is, to develop an analysis
procedure which does not requiere a complete isolation elements in the research focus. Embedding and contextualization of data combined with relative measurement procedure have been found to be reasonable methods which are in line with his recommendation.
\section{What are Complex Systems?} (Abgrenzung) 

%
% FRAGE WCS1
%
\textbf{Define Terms:} \textit{System, Complex System, Subsystem, Ensemble}

The term \textit{system} is used in several scientific disciplines. 

According to ... \citep{Roche199} %http://informatics.indiana.edu/rocha/complex/csm.html


What criteria can be used to classify a system as a complex system? The following section explains several features of complex systems. recent research results


\textbf{Cascading Failures}\\

\textit{Due to the strong coupling between components in complex systems, a failure in one or more components can lead to cascading failures which may have catastrophic consequences on the functioning of the system.[3]
}
\textbf{Complex systems may be open}\\

\textit{Complex systems are usually open systems — that is, they exist in a thermodynamic gradient and dissipate energy. In other words, complex systems are frequently far from energetic equilibrium: but despite this flux, there may be pattern stability, see synergetics.
}
\textbf{Complex systems may have a memory}\\

\textit{The history of a complex system may be important. Because complex systems are dynamical systems they change over time, and prior states may have an influence on present states. More formally, complex systems often exhibit hysteresis.
}
\textbf{Complex systems may be nested}\\

\textit{The components of a complex system may themselves be complex systems. For example, an economy is made up of organisations, which are made up of people, which are made up of cells - all of which are complex systems.
}
Multi scale models have been proposed to handle large scale systems, but
individual interactions are removed also in this case. 

MICRO MACRO MESO 

What is the meso-part in \cite{Jesus2009}?


\textbf{Dynamic network of multiplicity}\\

\textit{As well as coupling rules, the dynamic network of a complex system is important. Small-world or scale-free networks[4][5][6] which have many local interactions and a smaller number of inter-area connections are often employed. Natural complex systems often exhibit such topologies. In the human cortex for example, we see dense local connectivity and a few very long axon projections between regions inside the cortex and to other brain regions.
}
\textbf{May produce emergent phenomena}\\

\textit{Complex systems may exhibit behaviors that are emergent, which is to say that while the results may be sufficiently determined by the activity of the systems' basic constituents, they may have properties that can only be studied at a higher level. For example, the termites in a mound have physiology, biochemistry and biological development that are at one level of analysis, but their social behavior and mound building is a property that emerges from the collection of termites and needs to be analysed at a different level.
}

\textbf{Relationships are non-linear}\\

\textit{In practical terms, this means a small perturbation may cause a large effect (see butterfly effect), a proportional effect, or even no effect at all. In linear systems, effect is always directly proportional to cause. See nonlinearity.
}
Nonlinear interactions (e.g. .... EXAMPLE) between components of a large
system lead to a complex system in which the concept of super-position can not
be applied. One concequence are emergent phenomena which can not be
studied after a segregation of the whole system. One has to handle all
interacting elements of the system, which is usually a large number.

\textbf{Relationships contain feedback loops}\\

\textit{Both negative (damping) and positive (amplifying) feedback are always found in complex systems. The effects of an element's behaviour are fed back to in such a way that the element itself is altered.
}

\section{Examples} 

\subsection{Physiological Networks}

Cite paper from Jan (Nature Communications) ...

\subsection{International Stock Markets}
Financial markets are created by interaction people, representing different participants in the market with overlapping interest. In general, the interest of different actors is not in line, but competing. Such properties lead to very complex structures and dependencies between companies.
 
Information flow, e.g., via news channels, is the result of and also the input to human trading activity in financial markets. Here we do not consider automatic intra-day trading as in (\textbf{CITE INTRA DAY TRADING) - EXAMPLES in \cite{Zheng2004}}. \textit{Is the following correct? }The considered information flow model is not working in such automated trading systems because the time scales are very different. Private investors follow the news, nowadays in near real time, but even if a decision is done very impulsive, it is always slower than automatic trading, which happens on sub second scale, while a transaction based on a call with the bank or broker needs at least minutes. Automatic trading also replaces the central human elements, such as emotional and intelligent interacting people by deterministic machines and models, which can be deterministic as well, or even data driven. Preis \textit{et al.} demonstrate the impact of external information, provided, e.g., by Google Trends or Wikipedia (WHICH DATA and which AUTHORS?) on the result of trading activities, if decisions are based on external knowledge. 

\textit{CITE Kenett} and \cite{abc} demonstrate how international markets depend on each other. It is clear, that the interdependence of complex systems leads to more complex systems. Individual simplification of each system would lead to simpler models, but one has to expect information loss. In order to overcome this limitation it is important to develop methods, which take the coupling between such diverse systems into account. Information theory provides a robust framework for such analysis. Our network creation methods are partially based on information theory.

\cite{Smith2009}
%
%
\textit{The credit crisis roiling the world's financial markets will likely take years and entire careers to fully understand and analyze. A short empirical investigation of the current trends, however, demonstrates that the losses in certain markets, in this case the US equity markets, follow a cascade or epidemic flow like model along the correlations of various stocks. A few images and explanation here will suffice to show the phenomenon. Also, whether the idea of "epidemic" or a "cascade" is a metaphor or model for this crisis will be discussed.}


\subsection{Climate Networks}

\textbf{Cite the papers from Donges, the one which uses a fixed Threshold for link strength (see creation chapter), the one from ISRAEL which shows how the link strength can be weighted to illustrate how the creation methods evolve over time and why...} 

Further examples for application of correlation based network construction are: earthquake studies \cite{Tenenbaum2012}, ...
Mobility Networks (based on airline connections or daily comute traffic via train, bus, or car) \cite{Vespignani2009}.

\chapter{Network Theorie}

Properties of real world objects are represented as random variables, in case of time dependent properties time series are used. A relationship between objects can be defined implicit or explicit. An implicit relation is given as property of the one object. Such a property has an identifier of a target as value. To represent such a case only two data objects are required. They are called \textit{node}s. An explicit relation is represented by a third data object. This is called \textit{edge} and has as many properties as are required to describe a real world phenomenon. Usually this is a random variable, but it can even be a multivariate random variable or even a time series for each property. The source and target properties contain the identifier of nodes. 

\section{Types of Networks}
\label{network.models} 
Networks are also called graphs, preferably by mathematicians. If it is relevant which node is the beginning of an edge and which one is the end, a graph is directed. Such a graph describes, e.g. a "Dependency Network" (DN). Such DNs are directed and in case of directed acyclic graphs (DAG) they can used to build Bayesian Networks (BN). 

The probably most important property of networks is the degree distribution. It is very important because characteristic properties of networks can be shown in the link distribution plot \textbf{(see FIGURE ...)}.
This function or curve can easily be generated by a link count procedure. Depending on the used computational model one has to send a message to all neighbor node along each link and then one aggregation step is required before the binning, preferably on logarithmic scale, is done. If the edge list is available as a database table, one has to process the full node index. This can be an expensive operation, if the graph is stored as property graph with many attributes. Maintaining a single adjacency list with node and edge ids in an optimized encoding speeds up this procedure, especially if a time dependent analysis and observation of the evolution of the degree distribution is required. 

\cite{Krings2012} Network Growth and Time Window dependence
%ETOSHA.link http://semanpix.de/opendata/wiki/index.php?title=Krings2012


\cite{Kamps2009} 

\subsubsection*{Gradverteilung}

Zu diesem Abschnitt habe ich eine Rechnung von Jan in den Notizen. Diese sollte ich hier einbauen. 

Das Wort \textit{Gradverteilung} leitet sich von dem englischen Degree Distribution, ab. In einem Netzwerk haben nicht alle Knoten gleich viele Kanten und somit nicht den gleichen \textit{Knotengrad} \cite{Albert}. Die Knotengrade werden mit Hilfe der Verteilungsfunktion $P(k)$ charakterisiert, diese gibt die Wahrscheinlichkeit an, das ein zufällig gewählter Knoten genau $k$ Kanten enthält. Erstellt man nun ein Histogramm und trägt $P(k)$ über dem Grad $k$ auf, so gibt dieses Histogramm die Gradverteilung des Netzwerks \cite{Newman} an. Für einen zufälligen Graphen ergibt sich eine Poisson-Verteilung\footnote{Die Poisson-Verteilung ergibt sich nur für Netzwerke mit großem Umfang, ansonsten entspricht die Gradverteilung einer Binominalverteilung \cite{RandomGraphs}.} \cite{RandomGraphs}. Diese Verteilung hat einen Peak bei $P\left(\langle k\rangle\right)$ \cite{Albert}, da der Großteil der Knoten ungefähr den selben Grad hat, welcher nahe an dem Mittelwert der Grade $\langle k\rangle$ liegt. Um so erstaunlicher ist jedoch, dass für reale Netzwerke Gradverteilungen gefunden werden, welche gänzlich davon abweichen. Diese können für solche Netzwerke durch Potenzgesetze oder Exponentialgesetze beschrieben werden \cite{Newman} siehe Abbildung \ref{Gradverteilungen}. Netzwerke mit einem abklingenden Potenzgesetz werden skalenfrei genannt.
\begin{figure}[H]
\centering
	\begin{minipage}[t]{0.6\textwidth}
		\begin{figure}[H]
		\includegraphics[width=\textwidth]{tmp/DegreeDistribution1.png}
		\end{figure}
	\end{minipage}
	\begin{minipage}[t]{0.6\textwidth}
		\begin{figure}[H]
		\includegraphics[width=\textwidth]{tmp/DegreeDistribution2.png}
		\end{figure}
	\end{minipage}
	\begin{minipage}[t]{0.6\textwidth}
		\begin{figure}[H]
		\includegraphics[width=\textwidth]{tmp/DegreeDistribution3.png}
		\end{figure}
	\end{minipage}
	
	\caption{Verschiedene Gradverteilungen für diverse reale Netzwerke \cite{Newman}. Dabei wurde die Verteilungsfunktion $P(k)$ über dem Grad $k$ aufgetragen. Für einige Netzwerke ergibt sich ein exponentieller Zusammenhang, während für andere ein Potenzgesetz gilt, je nachdem ob die Achse für den Grad linear oder logarithmisch dargestellt wird. Die Abbildung \textit{(e)} stellt den einzigen Fall mit einer linearen Skala da, alle anderen sind logarithmisch.}
	\label{Gradverteilungen}
\end{figure}

Several categories and classes of networks exist. Depending on the characteristic topology and structural properties one can distinguish scale free networks, random networks. Figure \ref{Gradverteilungen} shows the degree distribution for several network types.

... show the small world phenomenon. ...

Depending on the node properties one or classes of nodes which are part of the network the number of partitions is an interesting aspect. The term 'bipartite' is used if exactly two types of nodes exist, '$k$-partite' networks consist of k different node types but in both cases only nodes of different types are connected.   

\subsection{Scale Free Networks}
%http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.86.3200
\textbf{Epidemic Spreading in Scale-Free Networks}
PRL 86 3200 2001 Pastor, Sattoras, Vespigiani

PRL 109 ... Aoki2012

=> Show some typical examples ...

\subsection{Bipartie and $k$-partite Graphs}

Modifying a common shared resource, e.g., a Wikipedia page or collaboration on a research article leads to implicit social networks among the collaborating persons.  Bercovitz \cite{Bercovitz} concludes that the information in the bipartite network structure of the original graph contains significant useful information on the implicit social network. Therfeore it can be used as an effective data structure for advanced analysis, such as recommender systems. Initially there is a bipartite network, which consists of two different types of nodes, people and ressources. Common activity on one resource is interpreted as an implicit link between the collaorating people. In opposite to this, a correlation network expresses an implicit relation between resources, based on common activity patterns caused by all user interactions. This way a second complementary view of the system emerges in both cases. 

An exploratory study on Wikipedia edit activity was done by Jesus \textit{et al.}  \cite{Jesus2009}. They report, that due to topics are aggregating editors. In general the efforts are highly coordinated which results in dense clusters. This work does not take the time scale into account. Our results show also correlations between edit events, but only on very short time scales. After removing all events with short inter-event times we found uncorrelated edit activity (in the time domain).

\textit{ A $k$-partite graph is a graph whose vertices can be partitioned ino $k$ disjoint sets so that no two vertices within the same set are adjacent.} Edges connect always two vertices of different types or from different sets. In case of a bipartite graph only two different types of vertices are available. 

=> Multilayer networks are a combination of these both. The previously mentioned networks can be interpreted as special cuts through a multilayer network.

In many cases networks a representations of a systems at one point in time focusing on one aspect of the system. This leads to a clear visualization. One can handle this particular aspect with mathematical tool, like network measures (see section ...). But in some cases, available information is lost, because related data from history or from the neighborhood is just ignored. Dynamics of networks is in the focus of several researchers, especially if process on top of networks are investigated, one collects data over a longer period in time. Beside this kind of data about the process, one needs a reliable representation of the environment in which the process is embedded. In this case, it is required to describe and characterize the networks structure as a function of time.

\subsection{Highly Dynamical Networks: Temporal Networks}


Review of TN ... by Petter Holme: 
\textbf{http://arxiv.org/pdf/1508.01303v1.pdf}

All the mentioned networks were modeled following a connectivity driven approach. According to 
%%ETOSHA.cite
%http://www.nature.com/srep/2012/120625/srep00469/full/srep00469.html
such 'Network modeling plays a critical role in identifying statistical regularities and structural principles common to many systems. The structural patterns of the network are at the basis of the mechanisms ruling the network formation.' Because this connectivity driven models necessarily provide a time-aggregated representation,  they may fail to describe the instantaneous and fluctuating dynamics of many networks. ... et al. address this challenge by defining the \textbf{activity potential}. An activity potential is a time invariant function characterizing the nodes' interactions and constructing an \textbf{activity driven model} capable of encoding the instantaneous time description of the network dynamics. Especially if network formation or transformation processes are of interest, this model has the advantage as it is able to explanation structural features such as the presence of hubs, which simply originate from the heterogeneous activity of agents. Highly dynamical networks can even be described analytically. This allows to overcome the limitations of time aggregated data.

'Walking and searching on time-varying-networks' (arxiv 1206.2858 (2012)) is another example which illustrates the importance of a new approaches in network science which go beyond static network layers and simple topology.\\

In this work we use the term 'functional networks' to define measures for pairwise properties which can be interpreted as connectivity between network nodes. Similarity measures and concepts from information theory are used to describe connections between network nodes even if such do not exist physically or if they are are not visible obviously.
\textbf{HOW IS THIS COMPARABLE WITH "activity potential"?}


\textbf{Duality between Time Series and Networks}
Andriana et al.  \cite{Andriana2011}

Studying the interaction between a system’s components and the temporal evolution of the system are two common ways
to uncover and characterize its internal workings. Recently, several maps from a time series to a network have been
proposed with the intent of using network metrics to characterize time series. Although these maps demonstrate that
different time series result in networks with distinct topological properties, it remains unclear how these topological
properties relate to the original time series. Here, we propose a map from a time series to a network with an approximate
inverse operation, making it possible to use network statistics to characterize time series and time series statistics to
characterize networks. As a proof of concept, we generate an ensemble of time series ranging from periodic to random and
confirm that application of the proposed map retains much of the information encoded in the original time series (or
networks) after application of the map (or its inverse). Our results suggest that network analysis can be used to distinguish
different dynamic regimes in time series and, perhaps more importantly, time series analysis can provide a powerful set of
tools that augment the traditional network analysis toolkit to quantify networks in new and useful ways.

Citation: Campanharo ASLO, Sirer MI, Malmgren RD, Ramos FM, Amaral LAN (2011) Duality between Time Series and Networks. PLoS ONE 6(8): e23378.
doi:10.1371/journal.pone.0023378



\section{Networks of Networks}


\cite{Gao2012a}



DEFINE WHAT NoNs really are, based on the paper from Dror and all the people around Havlin.

\cite{FutureICT-proposal} \textit{FutureICT-proposal}

Nodes from one network can also be linked to other networks within another network. Such links between networks introduce dependencies and if they exist in both directions inter dependencies, which can lead to cascades of failures are the result.

 How interdependent are infrastructures?

\begin{figure}[h!]
  \centering
      \includegraphics[width=0.85\textwidth]{semanpix/CommunicationProcessesAndCouplesNetworks/sketch.eps}
     \caption[\textbf{Interdependence between networks} leads to Networks of Networks.]{\textbf{Interdependence between networks} can lead coupled networks in which processes can interfere withe each other by forming positive or negative feedback loops on a large scale which might be invisible on smaller scales.}
     \label{fig.CPACN} 
\end{figure}


NoN are related to the studies of co-evolution of intelligent 


SOCIONICAL Book in general

\url{https://books.google.de/books?id=5Y65BQAAQBAJ&pg=PA11&lpg=PA11&dq=FireSim+SOCIONICAL&source=bl&ots=PxgmeHWRAr&sig=44NXehPCAdCFEqFaTvf7nXyWRX4&hl=de&sa=X&ved=0ahUKEwjWt8TC_MvKAhXFwA4KHYHRAooQ6AEILjAC#v=onepage&q=FireSim%20SOCIONICAL&f=false}

\url{http://www.springer.com/fr/book/9783642366130}

Co-evolution aspect (in general):

Consider the road-network as a first network layer. The people which move along the streets form two more networks, (a) the relation between people is defined by their social networks (friendship, family) and (b) there is a communication network, which introduces a kind of short-cut. Even if the distance between two persons is long, they can not see each other, but they can do a call to inform each other about their next steps. In some cases, this mobile communication network can cause unusual delays in information flow, because it is overloaded, or it can even collaps completely and disapear. In this case a complete layer of the complex system is lost and some propeties - such as small world propety - are lost. This leads to fundamental changes in the dynamic behaviour of the system .... WHAT IS THE IMPACT OF SMALL WORLD ON INFO FLOW?

CITE (3):  Wirts: Crowd density, Group Formation methods, Model von Jan und Kollegen ...
 




\cite{Gao2012} %Nature Physics 8, 40 (2012) - Netonets.org

\section{Mathematical and Technical Representation of Networks}

Graph models, as presented in the previous sections, describe characteristic properties of graphs and allow an interpretation of their behavior. The technical representation of the network is related to the selected model, if a graph is generated during simulation or to the type of network which is studied from measured data. The second case is complicated, since in general the properties of the network one works with may differ from expectations, so one should be prepared for the worst case. This means, the full network must be handled. In many cases it is easy to store a full matrix which represents the full graph. For some models such as ... it is known that the graph has much less links than the maximum number of links. Such a sparse graph can be represented as adjacency list, which is much more efficient since it requires much less memory. In comparison to a full matrix, which also contains values for links which do not exist, an adjacency list only contains the link values which are different from null. In case of multilayer graphs it is possible to handle each layer as individual adjacency list or each matrix element contains a vector, in which each component represents a layer. Such data sets can be obtained for discrete points in time. In this case, the system state at each time step is be represented by a network with a particular label representing the time. Alternatively, the matrix elements could contain vectors of time series to represent the time dependent multilayer graph. A simple way to represent such a property is by using semantic networks. First, each link is defined as an entity which is unique if source and target are known. Such a link can have arbitrary properties, which all represent a specific metric or measure, related to a layer. Finally, each of such link-metric pair can have be related with a set of value value-time stamp pairs. The same result is obtianed if a tuple, consisting of link, metric, and time stamp is related to a value. Storing such data in a triple store allows 
simple projections of individual layers using the semantic query language called SPARQL. For large scale operations on a graph it is very important to work with optimized compact representations of the graph. But for data exploration, e.g., to identify correlations and dependencies between several features in the raw data it is very helpful to have such a flexible representation. 

A graph $G$ consists of vertices $V$ and edges $E$ which both are finite sets. Usually a vertex is represented as a point and edges are lines, connecting the dots. DESCRIBE THE STANDARD FOR ARCS, how are they oriented? WHAT IS THE NAME? Beside those mathematical terms physicists prefer the terms node (which is the same as a vertex) and link instead of edge.




What is a Property-Graph?
\cite{Xin.2013}


\label{GraphX.PROPGRAPH.CUT}

Apache Spark uses a property graph and a technique, called edge cut, to represent large networks in a distributed system.

Node properties:
- page name\\
- language\\
- $k_{IWL}$\\
- $k_{AL}$\\
- ID\\
- $k_{redirect}$ \\
- sum( access )\\
- sum( edits )\\
- size \\
- $z_{PEAKS > 10}$\\
- $z_{PEAKS > 20}$\\
\\

Which properties can be used to predict link creation?

Which data set allows me to extract the link creation events?



Edge-Properites:
- Link strength and significance measure for each link-reconstruction approach.


%TODO
\textbf{Formulas ... .}\\


Nodes and links are arbitrary objects which have any unique identifier. This is all, one needs to define a network. This requires the existence of node and link objects in the mathematical sens, a physical representation is not necessary, and this allows the application of the network and graph theory in multiple disciplines even if no physical objects are available.     

In many cases, matrices are used to represent a graph. This allows simple notations of algorithms applied to a graph, e.g. to calculate typical or characteristic properties, like the degree \textit{k} which is total number of in- and outgoing links for each node. \\

%TODO
\textbf{Definition of Graphs ... and representation in algorithms.}\\

%TODO find reference for node encoding (MapEquation ???)
A much higher computational efficiency can be achieved with appropriate encoding \textbf{(see: Huffman code, [18] IN ROSVALL.2008)} of node and link data (see ... MapEquation), by using adjacency lists, and with distributed graphs. In distributed processing environments like Apache Hadoop one can find several approaches for graph processing. Apache Giraph \cite{Giraph} loads node- and edge-lists which are distributed across multiple computers (CSN) \footnote{CSN: compute and storage nodes which form a cluster for large scale data processing}. The procedure which decides how data should be distributed influences the efficiency of processing. 


% http://www.select.cs.cmu.edu/publications/paperdir/osdi2012-gonzalez-low-gu-bickson-guestrin.pdf   ... page 6, Abs. 5, Satz 2
According to Gonzales \textit{et al.} 2012 \cite{Gonzales.2012} \textit{the placement of the data-graph structure and data plays a central role in minimizing communication and ensuring work balance}. Based on this concept, a new graph abstraction which is called Resilient Distributed Graph (RDG) was introduced by Xin \textit{et al.} 2013 \cite{Xin.2013} with an implementation in the GraphX software package. An RDG is a tabular representation of the vertex-cut partitioning like explained in \cite{Gonzales.2012} combined with data parallel partitioning heuristics. This way the concept of a resilient distributed data set which is not bound to a network or graph was extended and allows working with large graphs, even if one computer has not enough memory. \\


%TODO
\textbf{RDG and vertex-cut compared to ... .}\\

Two effects are of interest: (a) one needs less memory to store the full graph in its list representation, and (b) one has faster access to neighbor nodes, because in an adjacency list, all linked nodes are accessible directly linked to the source node. In an adjacency matrix, an iteration over the full column or the full row would be required, even if only one link exists. Efficiency can be increased very well in the case of sparse networks. This means, one should find out if a sparse network exists early to save storage and computational resources. But in some cases this is not possible, e.g. in correlation networks the symmetric matrix of all links can be reduced to one triangle. \\

Adj. Matrix, Adj. List, Bipartite graphs, Hypergraphs, MultiGraph, \\





Matrix, Tensor, Concepts of multiplex and temporal networks ... \\

Metadata is required for complex systems and for tracking data set dependencies ... \\

\begin{figure}[h!]
  \centering
      \includegraphics[width=0.85\textwidth]{semanpix/DSMetadata/DAD.eps}
     \caption[\textbf{Data Access Descriptors (DAD)} connect meta information of distributed data sets for efficient handling of multi-layer and time dependent networks.]{\textbf{Data Access Descriptors (DAD)} decouple analysis workflows and specific implementations of particular algorithms from physical representation of network data. This allows efficient handling of multiple aspects and time dependent properties, which are relevant in complex systems analysis.}
     \label{fig.DAD} 
\end{figure}

\section{Obvious and Hidden Links in Networks}
The links between nodes represent relationships between them. Different link types are used for different types of relations. One has to distinguish between directed and non directed relations. A directed relation can be a property which is is only visible in the context of one node. Starting from one node, one can ask: "What are the nodes children?" This kind of relation implies another inverse relation such as: "What are the child's parents?" Directed links can be of a specific type or they can have a weight. Link types can be manifold. The more types or the more possible values one has to distinguish the more complicated and less expressive the analysis procedures become. For practical reasons, one can replace two of such directed links - which are the inverse of each other - by one non directed link. In other cases a link only exists if the weight exceeds a defined threshold. Such procedures have on drawback, some information gets lost and they can not be reversed. 

Links between web pages or in our special case between Wikipedia articles are easy to detect and to extract. Page networks can be created either manually or in case of larger networks by application of standard software. Not all structural links are obviously visible. A family tree is an example of a structural network but due to limitations of data availability and access limitations it is often hard to create a comprehensive family tree which spans several centuries. Friendship and follower networks are modern examples of structural networks. In many social networks, net structure is not actively maintained. This means, links are created over time and they can disappear as well. The network grows but as soon as some system properties change, the network is not updated immediately and is not in aligned to reality any more. In this case such a network is not a valid representation of reality any more. E.g., friendship emerges, it stays for certain time but friendship can also end. One can see a life cycle which spans a certain time range. Within this range, friendship networks are considered to be a structural networks. One has to distinguish such quasi-static structural networks, although they may change over time, from functional networks. Functional networks can overlay on top of structural networks or they can be caused by structural networks. The characteristic property is found on a different, usually shorter time scale. Detection of functional links requires a comparison of properties of node pairs or node tuples in case of hyperlinks. A link is not defined directly like in the case of friendship. It is more of an indirect relation like co-occurrence of or co-location of elements. In case of co-occurrence of extreme events in two time series, one can conclude that there might exist a relation between both elements which causes the extreme event. One has to be careful, there is no easy way to prove that the relation is not a random correlation. On the over hand, co-location can cause a structural link. A chemical reaction can be used to explain this. Only if the atoms or molecules are within a certain range and other conditions are appropriate a reaction can happen. Before the reaction, reactants are individual entities. During the reaction they are co-located and thus part of a functional network. After the reaction a now molecule exists, which means a structural link between two disconnected entities appeared or emerged. A functional network also can be reconstructed from social media data. If people work together over a certain period of time the collaboration can be interpreted as a functional network. This can cause friendship. On the other hand, friendship and the implicated trust between friends can be the reason for deep successful collaboration in research and business. 

Connectivity links are used to describe a symmetric relation between two nodes. In case of causation networks, dependency links are used to express the influence of one node on others.

%
% TODO DebtRank
% Stefano Battiston
% http://www.nature.com/srep/2012/120802/srep00541/full/srep00541.html
A recent example of such a dependency measure is called DebtRank. It was introduced by Battiston \textit{et al.} \cite{Battiston2012} with the goal to determine the systemically important nodes in a network. Analysis of systemic risk in financial systems is essential in order to understand critical situations or trends towards such critical destructive eruptions like the global financial crisis in 2008. They show, that not only the size of a system component but even more the centrality in the network matters. They suggest that the debate on too-big-to-fail institutions should include the even more serious issue of too-central-to-fail.

Links can exist for a long time or just instantly. What instantly means and for how long a static link has to exist depends on the use case. Chemical reactions and social interactions can be described by networks but the time scales are very different. Temporal links can even be special types of dependency links which are the result of process on top of a network, such as a communication process. The link only exists as long as communication lasts or as long entities are in a effective range of each other. Temporal links can even exist between nodes which are not connected by a static network.

\section{Construction and Reconstruction of Networks}

\textit{Reconstruction:} We start with data and extract information which let us describe node and link properties.

\textit{Construction:} We define rules to describe a growth process or a re-wiring procedure which changes properties of an already existing graph in a specific well known way. 
  
In general Similarity measures are used to compute structural or functional links. 

A structural link (SL) is typically an obvious link, which can be  ...

A functional link (FL) can be a result of hidden processes or collective properties of sub set of network nodes.  

The differentiation between structural and functional networks does not cover the causation. Structural networks can be directed or even non directed. Correlation networks are usually non directed as they are calculated from symmetric functions. Some special types of functional networks exist, such as correlation networks based on mutual information \textbf{see DIPLOMA thesis from POTSDAM} or dependency networks calculated from triples of nodes \textbf{see paper from DROR}.


\section{Growth Models}
\label{sec.networkgrowth}
\subsection{Linear, Algebraic, and Exponential Growth}
\subsection{Logistic Growth}
\subsection{Geometric Growth}
\subsection{Gombertz Model and Extended Growth Mode}





\section{Quantitative Measures for Complex Systems Represented as Networks}


Size of a network? How to split it into subgraphs? Partitions / Cuts etc ...

"brief communication" => Diameter of the web





What is correlation in networks?

- Relation between Structure and Correlation ...

\textbf{See:} Books.... 

\textbf{See:} \cite{Pernice2011} 

Several network measures allow advanced studies of graph properties, such as robustness or ability to adopt to new
environments. But individual measures are useful in some contexts, while they do not much support information gain in other situations. We distinguish (a) spatial measures, which describe the topology of a graph, and (b) properties of the graph in a local context (strongly related to the next neighborhood) or even in a global scope.


%ETOSHA.NOTE Check slide:
%http://cs.brynmawr.edu/Courses/cs380/spring2013/section02/slides/04_PlanarGraphsAndSoftware.pdf

RELEVANCE of NODES
- cite "Lev M. Influencing spreaders ..."
- cite "The H-index of a network node and its relation todegree and coreness"
- cite 3 other collected ...



\subsection{Extraction of Informative Subgraphs}

\subsubsection{Minimum Spanning Tree}
\cite{Kruskal1956}

%Kruskal, J. B. "On the Shortest Spanning Subtree of a Graph and the Traveling Salesman Problem." Proc. Amer. Math. Soc. 7, 48-50, 1956.

%Weisstein, Eric W. "Kruskal's Algorithm." From MathWorld--A Wolfram Web Resource. http://mathworld.wolfram.com/KruskalsAlgorithm.html

\subsubsection{Planar Maximum Filtered Graph}

\url{https://sites.google.com/site/bctnet/measures/list#TOC-Clustering-and-Community-Structure}

\url{https://people.hofstra.edu/geotrans/eng/methods/ch1m3en.html}

\subsubsection*{Kürzester Weg, Average path length and Diamater}
Häufig wird für ein Netzwerk der \textit{kürzeste Weg} angegeben, welcher teilweise auch als \textit{Geodätischer Weg} bezeichnet wird \cite{Newman}. Es gibt verschiedene Arten: Zum Einen können zwei Knoten fest vorgegeben sein \cite{ShortestPath}, oder der Startknoten wurde festgelegt und der kürzeste Weg zwischen allen Knoten ist gesucht. In beiden Fällen muss der kürzeste Weg, bedingt durch die Gewichtung, nicht unbedingt mit dem Weg der kürzesten Länge übereinstimmen. 



The goal of measurements in general is quantification. If the same measure can be obtained from two different things and if the type of the measure allows computation of a difference or at least ordering, one compare the different things with each other. 

Several measures are well established. Cite: SNAP network, and "Network book from Estrada". Mention Newman. 

1) Cite the paper: "Wikipedia-as-Complex-Network-DEGREE 0602149.htm" \cite{Zlatic2006}
 
2) Explain what the Stanford SNA tool offers.

Processes are studied by "Havlin / Staneley". What is the role of "Percolation" and what is fractal dimension? Why are both important.

What are the established standard measures which are often used in SNAP?


First we care about typical network measures and properties before we introduce a new approach, one which connects the systems properties with single element properties.

In complex networks like in other many body systems, one can distinguish group properties and group phenomenon from single element properties. The later are subject of time series analysis. New methods of multivariate time series analysis with more than two or three elements, as shown in a later section were developed by ... . Such multivariate analysis takes the underlying structure of the system only implicitly into account. This work uses primarily the evolution of the systems structure and connects this information to the node level properties. 

\subsection{k-Means Clustering}

\textbf{QUALITÄT:}  Paper über Vergeich von Clustering-Methoden und:

\textit{A clustering could be considered good if each data point were near to its closest centroid. So, we define a Euclidean distance function, and a function which returns the distance from a data point to its nearest cluster’s centroid:}

\subsection{Modularity Clustering}
Modularity Matrix:

\begin{equation}
B_{ij} = A_{ij} - \frac{k_ik_j}{2m}
\end{equation}
(Newmann 2006) 


Average Cluster coefficient

\ref{http://en.wikipedia.org/wiki/Cluster_analysis}


\subsubsection*{Clusterkoeffizient}
Der Clusterkoeffizient beschreibt die Cliquenbildung (bzw. Transitivity) eines Netzwerks. Eine Clique entsteht, wenn jeder Knoten mit jedem über Nachbarn verbunden ist. Wenn Knoten A mit Knoten B verbunden ist und ebenso Knoten B und C verbunden sind, dann folgt daraus, dass auch die Knoten A und C verbunden sein muessen \cite{Newman}. Dies wird Transitivitaet genannt. Bezogen auf soziale Netzwerke würde man sagen,\, der Freund meines Freundes ist auch mein Freund. Der Clusterkoeffizient waere dann die mittlere Wahrscheinlichkeit, dass\,   der Freund meines Freundes auch tatsächlich mein Freund ist . Im Sinne der Graphentheorie misst der Clusterkoeffizient die Dichte an Dreiecken in einem Netzwerk. Es gibt zwei verschiedene Definitionen eines Clusterkoeffizienten, diese werden als globaler bzw. lokaler Clusterkoeffizient bezeichnet. Der Begriff des \textit{globalen Clusterkoeffizienten} beruecksichtigt das Verhältnis von Dreiecken zu verbundenen Tripeln zueinander, wobei ein Dreieck eine Menge von drei Knoten beschreibt, bei dem jeder Knoten mit jedem verbunden ist \cite{Newman}, während ein verbundenes Tripel \, einen Knoten beschreibt, der mit zwei anderen Knoten verbunden ist, welche aber keine gemeinsame Kante haben \cite{Newman}. Der Clusterkoeffizient ist nach \cite{Boccaletti} wie folgt definiert
\begin{align}
	C = \dfrac{3 \cdot Anzahl \,\, der \,\, Dreiecke}{Anzahl\,\, der\,\, verbundenen\,\, Tripel}
	\label{C1}
\end{align}  
Der \textit{lokale Clusterkoeffizient} bezieht sich dagegen darauf, wie viele Kanten ein Knoten im Verhältnis zu seiner Anzahl an möglichen Kanten besitzt. Daraus lässt sich dann der Clusterkoeffizient für den Knoten $i$ berechnen\footnote{Diese Formel gilt nur für einen ungerichteten Graphen.} \cite{Watts,Newman}.
\begin{align}
C_{i} = \dfrac{2\, n}{k_{i}(k_{i} - 1)} \qquad bzw.\qquad C_{i} = \dfrac{Anzahl \,\,der \,\,mit\,\, Knoten\,\, i \,\, verbundenen \,\, Dreiecke}{Anzahl\,\, der\,\, in\,\,Knoten\,\, i\,\, zentrierten\,\, Tripel} 
\end{align}
\begin{table}[H]
\begin{tabular}{rccl}
mit  & $n$ & \ldots & tatsächlich vorhandene Kanten\\
	 & $k_{i}$ & \ldots & Nachbarn des Knoten $i$\\
\end{tabular}
\end{table}
Aus der Definition des lokalen Clusterkoeffizienten ergibt sich der Clusterkoeffizient für das gesamte Netzwerk mit Hilfe des Mittelwerts \cite{Newman}.
\begin{align}
C' = \dfrac{1}{N} \sum_{\substack{i}} C_{i}
\label{C2}
\end{align}
\begin{table}[H]
\begin{tabular}{rccl}
mit  & $N$ & \ldots & Anzahl der Knoten\\
\end{tabular}
\end{table}
Hierbei sei besonders darauf hingewiesen, dass die Gleichungen (\ref{C1}) und (\ref{C2}) durchaus sehr unterschiedliche Ergebnisse liefern können (siehe Abbildung \ref{Abb15}).
\begin{figure}[H]
\centering
	\begin{minipage}[t]{0.3\textwidth}
		\begin{figure}[H]
		\includegraphics[width=\textwidth]{tmp/clustercoefficient1.png}
		\caption*{$C_{1,2,3,4}=\lbrace 0,0,0,0\rbrace$\\ $C=0$ \\ $C'=0$}
		\end{figure}
	\end{minipage}
	\begin{minipage}[t]{0.3\textwidth}
		\begin{figure}[H]
		\includegraphics[width=\textwidth]{tmp/clustercoefficient2.png}
		\caption*{$C_{1,2,3,4}=\lbrace 1,1,1,1\rbrace$\\ $C=1$ \\ $C'=4$}
		\end{figure}
	\end{minipage}
	\begin{minipage}[t]{0.3\textwidth}
		\begin{figure}[H]
		\includegraphics[width=\textwidth]{tmp/clustercoefficient3.png}
		\caption*{$C_{1,2,3,4}=\lbrace 1,0,1,\frac{1}{3}\rbrace$\\ $C=\frac{7}{12}$ \\ $C'=\frac{2}{3}$}
		\end{figure}
	\end{minipage}
	\caption{Diverse Netzwerke mit dazugehörigem lokalen Clusterkoeffizient $C_{i}$, globalem Clusterkoeffizient $C$ und der Mittelwert der lokalen Clusterkoeffizienten $C'$ (, dieser Clusterkoeffizient gilt global). (\textit{Links}) Der minimale Wert des Clusterkoeffizienten wird dargestellt, das Netzwerk enthält keinerlei Dreiecke. (\textit{Mitte}) Dieses Netzwerk besitzt den maximalen Wert des Clusterkoeffizienten, es enthält die größtmögliche Anzahl an Kanten. (\textit{Rechts}) Ein Netzwerk, welches unterschiedliche Werte für die unterschiedlichen Definitionen des Clusterkoeffizienten liefert.}
\label{Abb15}
\end{figure}

\subsection{Centrality vs. Range}
% What is long range?

The range of a node is long, if it is connected the largest connected component.
% Infectious disease on Livestock trade networks (Poster, DPG 2010)

How can we describe the state (in the life cycle) of a system?

What is a network in equilibium or out of equilibrium?

\subsection{Multiplex PageRange}
% PLOS ONE : Arda Halu et al.


RANKING of documents ... not network is used.



In a very recent approach, Halu {\it et al.} \cite{10.1371/journal.pone.0078293} introduced the Multiplex 
PageRank algorithm to calculate a centrality measure for nodes in networks with multiple 
layers.  Here, a layer is an individual aspect of a complex system.  The category network, 
the network of page links, and also the dynamically calculated correlation network can be 
interpreted as such layers, which form a multiplex network.  Especially the combination of 
the static and dynamic network layers (see Fig. 4 of
\cite{10.1371/journal.pone.0078293})
allows a better understanding of the time evolution of some node properties like ranking or 
centrality.  

With latent semantic analysis it is possible to classify text documents based on
their contents' term statistics \cite{Gohr09topicevolution}. The term
frequency (TF)
for all documents of a text corpus is
used to define a semantic space, within which a distance between different
elements can be calculated. Typically, the ranking algorithm calculates a
distance between the search query, which requires a context as a term vector, and
the documents, expressed by normalized term vectors. The TF-IDF approach
\cite{Salton1988513} calculating TF and inverse document frequency (IDF) is used
to normalize the term vectors of the document.  However, the link structure of
the network is not considered in TF-IDF.  Combining the traditional PageRank 
algorithm with a TF-IDF based similarity index derived from a latent semantic 
model has thus been suggested \cite{5364637.LSPR}.
% Xiaoyun Chen, Baojun Gao, Ping Wen: An Improved PageRank Algorithm Based on Latent Semantic Model
% DOI:10.1109/ICIECS.2009.5364637 
% In: Proceeding of International Conference on Information Engineering and Computer Science, 2009. ICIECS 2009.  
Another limitation of TF-IDF is that its results are not related with the number 
of readers of a given webpage or document. Both limitations also hold for the 
approach of Segev {\it et al.} \cite{Segev2010}, who has introduced global and 
local salience indexes to characterize the relative importance or prominence of 
countries in online news articles.  But, since news articles are hardly connected 
and not maintained for a long time, these limitations are less relevant for them 
than for, for example, Wikipedia articles.  We thus suggest that the ideas of 
news-based salience indexes of Segev {\it et al.} should be extended towards 
quantities derived from network properties, text volumes, and user acceptance to 
allow reliable relevance measurements for longer persistent WWW pages.
%There are many differences between loose and disconnected news articles
%which are published very frequently and pages, with many links, which are 
%maintained for a long time. But how important is a topic, a country, or an
%idea within in a given lingual context? The answer to this question might be a different one
%if data is retrieved from Wikipedia. But this is plausible, as news channels have a different
%purpose. It might be interesting to see a comparison of the salience indexes of a topic, 
%its representation index in Wikipedia and its relevance index, derived from text volume and 
%user acceptance.

According to \cite{10.1371/journal.pone.0064841} article relevance measurements
derived from social webs are called altmetrics. Examples include the well-known
Impact Factor and the number of citations of articles in
Wikipedia or other SMAs. However, Lozano {\it et al.} \cite{LOZANO} have shown
that the relationship between Impact Factors and papers' citations get weaker
over time.  Thelwall {\it et al.} \cite{10.1371/journal.pone.0064841} conclude
that metric values for articles published at different times, even within the
same year, are often not comparable, and the coverage of all altmetrics - except
for Twitter - seems to be low. Furthermore, both approaches do not consider the
dynamic properties of the document network node life cycle and usage properties.



%ETOSHA.hide
%
%\subsection{Activity Potential}
%
%\subsection{Topology of Coupled Networks}
%\cite{Donges.2011} (Donges.2011)
%Show local and global measures for NON
%
%
%
%
%\begin{figure}[h]
%\includegraphics[width=0.9\textwidth]{semanpix/Fig.A/NoN.eps}
%\end{figure}





Especially for the minimum spanning tree the following metrics were proposed and applied in recent studies:

\textbf{REPHRASE AND CITE Gang-Jin
}
\textit{A simple measure of the normalized tree length (NTL) is proposed by Onnela et al. [24,25],
which is used to analyze the temporal state of the financial market, and is defined by}
\begin{equation}
abc
\end{equation}
\textit{where theta denotes the set of edges (or links) in the MST at the time scale s.
}

\textit{The average path length (APL), which can be used to measure the density of the MST network
structure, is defined as the average distance of the shortest path between any two currencies
i and j [32], i.e.,}

\begin{equation}
cdf
\end{equation}
\textit{where ls
i j is denoted as the number of links in the shortest path between two nodes (currencies) i and j at
the time scale s.
}

\textit{Onnela et al. [24,25] also introduced a measure of the mean occupation layer (MOL) to describe the spread of nodes on the MST, which also can be used to quantify the changes in the density of the MST. For the central vertex (or node) vc at the time scale s, the mean occupation layer is defined by}
\begin{equation}
abc
\end{equation}
\textit{the central vertex in a tree is defined as a vertex that has the maximum degree (or links); levs(vi)
is the level of vertex vi with respect to the central vertex vc at the time scale s; and the level of the central vertex vc is set to be zero [23].}




\textit{In order to detect the power-law or scale-free behavior of MSTs for the FX market at
different time scales, we introduce and employ a powerful toolbox proposed by Clauset et al. [61], which is based on the maximum likelihood estimator (MLE) and the Kolmogorov–Smirnov (KS) statistic. The probability distribution P(k) of the power-law model is defined by [61]}




\textit{Onnela et al. [24,25] defined two measures, the single-step survival ratio and the multistep survival
ratio, to study the robustness and long-term evolution of the MSTs respectively. Considering that the number of time scales is limited, it is not suitable to introduce the measure of multistep survival ratio in our study. Following Onnela et al. [24,25], we hereby only introduce the single-step survival ratio (SSR), which is defined as the ratio between edges of MST and edges found in common in two consecutive MSTs at time scales si and si-1, i.e., (means the number of) [39],}
\begin{equation}
def
\end{equation}
\textit{where E(si) and E(si-1) stand for the set of edges of MST at time scale si and si-1 respectively, $INTERSECTION SYMBOL$ is the intersection operator [25], and N-1 denotes the number of edges in the MST.
We calculate the single-step survival ratio of MSTs at different time scales for the FX market by
Equation (14) and show the graphical representation of LSSR in Figure 8. The average value of LSSR is equal to 0.8212, which is similar to the result reached by Onnela et al. [25] who studied the dynamic asset trees for the U.S. stock market.}


\section{A new Approach: Hadoop Distributed Graph Space}
\subsection{The Meta-Graph Model}
\subsection{Scalable Graph Processing in Hybrid Environments}


%
% Here I need the image from the old MacBook (OmniGraffle) 
%

\cite{Hadoop.TS} 

\include{semanpix/ScalableGraphAnalysis/imageLS}
\label{ext.fig.sga} 

label{fig.sga}
Other non standard examples for usage of Hadoop as the fundamental storage and processing platform in related scientific topics are automatic image processing \cite{Almeer2012} and fast event detection on PMU data as reported by Khan \textit{et al} in \cite{Khan2015}. 

\chapter{Social Networks}

Social Force Model by Helbing ...

Social Temperature by Kullakowsky.

Social Gravity is defined in Bannister et al. \cite{Bannister2013} ...


During the last decades, a large variety of network studies appeared. Networks represent the global climate (\textbf{CITE 3}), software systems (\textbf{CITE LINUX KERNEL and ETH study}) and of cause social networks (CITE MILGRAM, KARATE CLUB,FACEBOOK). How connected are people? Which organizational short cuts or leaks exist in huge organizations? Which networks improve and which block information flows? Such and many more questions are in the focus of research.

\textbf{Embedded networks}\\
Analysis of information flows and decision processes is more challenging. Information can flow between a source and a destination in many alternative ways. The direct path (see (a) in figure ... ) is usually included in models, and because of model limitations the indirect path (b) is ignored. Such indirect paths cross the system or model boundaries - this is why they can not be included. As long a the propagation time $t_b$ is in the same regime like $t_a$. For $t_a << t_b$ when the indirect propagation takes much more time than the direct, we can ignore this problem. It is important to know the time horizon to find out, if system properties depend on the embedding or not. 

A second approach is based on the structural properties of a system, which can depend on the creation, construction or preparation method. No matter if a path is direct or indirect, all paths, until a certain depth are used to define the system. This leads to different sizes (network diameter) One should ask now, if the system structure or a structural property changes as a consequence of the extraction or removal from original embedding and as a function of size. In this work we focus on time dependent structural properties. Our networks, extracted from Wikipedia use first and second neighbors only. 

\textbf{REPHRASE THIS SECTION} (see red section)\\

Backstöm \textit{et al.} \cite{Backstrom2011} \\

According to the Milgram experiment, we would need three more steps to reach all nodes in the network, but recent results from Facebook show, that the world is indeed smaller, and with 4 steps, all nodes can be reached (in average).\\

HOW LARGE IS A THIRD / 4-th Neighbor network? \\

Can I crawl such networks during Christmas?\\

The embedding can also bring a variety of different node types and different interaction types are usually represented by different link types which form network layers. If the same type of links is used, but with different realizations a weighted network can be used. Bipartite networks represent interactions between two different types of nodes and n-partite networks generalize this concept. 
Multi-layer networks represent different interaction types and finally, the combination of both leads to less simplified more realistic models. This way, highly detailed graphs can represent a system, but also the structural properties of this graph are characteristic properties of the same system. Such structural properties are not included in meanfield models since local averaging does not take the network structure into account. 

FIGURE ... SKETCH THE DIRECT AND INDIRECT PATH\\

Cachia et al. \cite{Cachia2007} discuss the relevance of online social networks (OSN) in general and with focus on future research - which means, prediction of social trends based on properties obtained from social networks. OSNs enhance creativity as a result of efficient and easy communication. They also foster collective intelligence by aligning individual thinking towards future goals. Using the inherent information, OSNs can be seen also as expert tools to measure and describe changes in social trends (see my work about Emails...	 )  %https://docs.google.com/document/d/1xRT1Pbru7TTVjDubpLu6rDPWhPjteN-IKmdPhqNwYDo/edit

The Social-Graph, provided by Facebook attracts lot of attention. Traud \textit{et al.} \cite{Traud2011} analyse the structure of the Facebook graph and ... conducted a real life experiment with users based on AB testing (IS THIS CORRECT).

\textbf{FIND THIS PAPER !!! }\\

A deeper relation between social science, which is focused on the elements which form social networks and physics is based on a social temperature. In general, temperature is defined in statistical physics ... 
Floria2008 et al. \cite{Floria2008} use the social temperature of a network and also Kulakowsky (... which paper?) in a social system in general. This way, they apply the formal framework of Equilibrium Statistical Mechanics to the new field of social network analysis. It is possible to describe the asymptotic behavior of strategic evolution of a social system. According to \cite{Floria2008} the term temperature of a network is more than a metapher.

Zhao \textit{et al.}   \cite{Zaho2011} define the entropy of a social network and find variable entropy depending on the time of the day during a typical week day. The adaptability of social behavior was analyzed by dynamical real networks with existing models of social interactions. 

Since networks are very common and can be found in many places in nature it is surprising, how different social networks can be, compared to biological and technological networks (cite Newman and Park). NP find in "B", that social networks have non trivial clustering with relation to transitivity - WHAT DOES THIS MEAN??? They also find positive correlation in the node degree, known as assortative mixing.  \textbf{Explain hyptothesis on page 8 ....}

Another interesting feature of social networks is caused by a feedback loop - a typical property of a complex system. Kawamoto and Hatono study Twitter communication networks and found a correlation in the re-tweet rates. As a consequence, an explosive diffusion occurs. The transition point changes with the changed average retweet rate but also the increased fluctuations are seen as reason for this behavior (EXPLAIN A BIT MORE WHAT THIS MEANS. 


\cite{Traud2011}

Define: Social Networks

Person to person:

Person to Ressource: Bipartie 

%Viewing Implicit Social Networks As Bipartite Graphs


\section{Content Based vs. Communication Based Social Networks}



\section{Information Flow, Shift of Interest, and Response to Excitation}
Online services can be classified by multiple criteria according to dominating properties core functionality. Such functionality can be: (a) e-commerce, (b) content publishing, (c) content sharing, (d) social interaction, (e) personal trading, and (f) messaging services. 

In all cases, at least a personal profile or a descriptor of any type of item, such as products, services, or books form a content base. This content can be interlinked. Content sharing requires also a network of interacting people. Some users offer content, others search for it and request it - no matter if the legal or illegal sharing of copyrighted material. A new level of social interaction in content publishing platforms allows early communication between readers and authors. Publishers like OReily offer early access programs. This lessons could be learned form social networks offered in the WWW. Personal activities, such as trading on online market places and messaging on Facebook and Twitter would also not work without the two dimensions, content and social connections. We can not isolate this aspects any more. 

'What is Twitter, a Social Network or a News Media?' is the title of an article published by Kawk et al. \cite{Kwak2010}. They collected 41.7 million user profiles, 1.47 billion social relations, 4262 trending topics from 106 million tweets. In comparison to other social networks, they found a deviation in the degree distribution. The follower network, has a non-power-law distribution. Measuring the reciprocity of the network reveals, that most of the users just consume twitter messages. They use Twitter as a source for messages rather than being active in social interactions. An analysis of three different node ranking measures, shows, that static and dynamic aspects differ. 
PageRank and a ranking based on the in-degree show comparable results, but the node ranking based on re-tweet activity differs significantly. Especially because of the special relation between followers or fans of famous Twitter accounts, the likelihood of re-tweeting seems to be higher. They also found, that user participation is different for different topics. There exist core members which produce content for special topics, e.g., political topics, In other topics, the number of participating users grows over time, this indicates a real increase in interest in this topic. 
They compare the freshness of topics in Google Trends and Twitter. $95\%$ of trending topics are new on Google each day, but only $72\%$ are new on Twitter. This means, persistence is higher in Twitter, probably becaus.e of the internal structure of the social community. Currently, this is only speculation, because a comparison of both systems on a structural level is not yet done. This work can not solve this problem, but we contribute techniques which allow creation of hybrid analysis procedures to study the time dependent properties of such multi-faceted systems. 

\textbf{Identifiction of influencial spreaders ... Kitsak2010}

\section{Wikipedia: A Complex System of Connected Networks}

Kwak et al. \cite{Kwak2010} ask: "What is Twitter, a Social Network or a News Media?". They found a non-power-law follower distribution in a topological analysis of the entire Twitter site. The follower graph has a short effective diameter, and low reciprocity. This all mark a deviation from known characteristics of human social networks. Furthermore they found, that the majority (over 85\%) of topics are headline news or persistent news in nature.

Inspired by this question and results we analyzed available publications about Wikipedia. Is Wikipedia a social network? Yes, users, especially editors are related to each other during collaboration on an article. This can be a positive, inspiring relation or even disruptive. Kaltenbrunner et al. \cite{Kaltenbrunner2012} study the editorial process of Wikipedia articles, which is considered to be never ending - Wikipedia articles are never comlete. Social interaction happens on the accompaining talk pages. The comment activity per page is in general higher than edit activity but both show comparable properties. A power law is reported for the number of activity peaks per article, peak-lengths, and time between two consecutive peaks. A negative interaction between Wikipedia users is analyzed by Yasseri et al. in \cite{EDIT.WARS}. They studied a phenomenon called "edit-wars". \textbf{WHAT IS THE TAKE AWAY MESSAGE???} Vi\'{e}gas et al. introduce the "history flow" visualization mechanism. This tool highlights collaboration patterns and allows a visual analysis of article histories in a convinient data exploration procedure, e.g., edit-wars are visible as 'zigzag' patterns (see figure 6 in \cite{Viegas2004}).  

%ETOSHA.open.question
Is Wikipedia a source for news?

\subsection{Wikipedia and it's User Community}
%ETOSHA.note WCS1
\cite{Zlatic2006}

In March 2014 Wikipedia consists of ... wiki projects where each one contains pages in an individual which might be linked to a page which describes the same semantic concept in a different language. Such links are called inter-wiki links. They are the key to multilingual context sensitive data analysis without in a way, which does not require text translation and multilingual text processing. 

%stats.wikimedia.org/DE/TablesWikipediaDE.hml
Table ... shows the number of new pages $N_P^{l}$ and new links $N_L^{l}$ per month. 

\textit{See also charts in the sweden report ... }

\begin{figure}[h!]
  \centering
      \includegraphics[width=0.85\textwidth]{semanpix/CommunicationProcessInWikipedia/WikipediaAndSocialGroupsOfReadersAndEditors.eps}
     \caption[\textbf{Wikipedia: is a network of networks (NoN)} which consists of interlinked content and the social network of users.]{\textbf{Wikipedia: A network of networks} consists of interlinked content - the Wikipages - and the social network of users, which contribute and create more and even better content. Majority of users just consumes the public content without a direct contribution. Such interconnected systems can be seen as a global knowledgebase, as a stub for public interest, and as a free source for multiple facetes of reality, provided in multiple languages which works in general like an associative memory.}
     \label{fig.WikiNON} 
\end{figure}

Isolated information flow in the context of human trading activity in financial markets. This information flow model is not working in automated intra day trading systems. In such cases the central elements, emotional and intellignet people are replaced by deterministic machines. 

Content, Social, and Bi-Partite coupled networks which leads to correlation networks within the social and content network. Those might be the driving forces for changes within the "homogeneous" networks.  


\subsection{Wikipedia's Structural Properties}
Kamps and Koolen report \cite{Kamps2009}, that the Wikipedia link structure is comparable with the WWW, but Wikipedia pages are more densly linked. For Wikipedia, theoutlinks and inlinks behave the same and both are good indicators for relevance of an article. Interesting is the difference in their local link structures. Because of the higher density of local links in Wikipedia - probably as a consequence of categorization of content - such a local context is more applicable as a good indicator for relevance ranking of pages. This also motivates our approach of local neighborhood networks - which also ignores the global link structure of the entire system. 

\newpage

\chapter{Mathematical and Computational Methods}
\label{chap.Math}

HIER ZITIERE ich Campanharo \textit{et al.} \cite{Campanharo2011} "Duality between Time Series and Networks".

Their major difference is: they do an univariate approach to represent one time series as network. We use a multivariate approach to create a network from time series obtained from many different elements.

\textit{Studying the interaction between a system's components and the temporal evolution of the system are two common ways to uncover and characterize its internal workings. Recently, several maps from a time series to a network have been proposed with the intent of using network metrics to characterize time series. Although these maps demonstrate that different time series result in networks with distinct topological properties, it remains unclear how these topological properties relate to the original time series. Here, we propose a map from a time series to a network with an approximate inverse operation, making it possible to use network statistics to characterize time series and time series statistics to characterize networks. As a proof of concept, we generate an ensemble of time series ranging from periodic to random and confirm that application of the proposed map retains much of the information encoded in the original time series (or networks) after application of the map (or its inverse). Our results suggest that network analysis can be used to distinguish different dynamic regimes in time series and, perhaps more importantly, time series analysis can provide a powerful set of tools that augment the traditional network analysis toolkit to quantify networks in new and useful ways.}

\nomenclature[R$X$]{$X(t), Y(t)$}{Time series X and Y.}


\textit{This section describes the details of the selected time series algorithms for complexity research, which have already been implemented in 
Hadoop.TS libraries. There are two major types, although in both cases many time series are used to calculate 
properties characterizing the complex system.  Firstly, individual time series are used to calculate 
data later used for averages like the average fluctuation function of an ensemble of 
time series.  With such results, one can find out if the underlying processes have long-term memory or feedback 
loops.  Secondly, time series pairs are used to calculate cross
correlations, and then cross-correlation networks are reconstructed.  With such functional networks, the evolution 
of dynamical properties can be studied. 
}

A combination of several computational methods is required for our network creation procedures, as explained in Part II. Since we develop methods to create network representations from measured time series or data sets of collected events we combine method from different domains, such as time series analysis, regression, and statistical testing. For some methods, such as the Event-Synchronisation and the Kolmogorow-Smirnow test, some additional experiments provide illustrations of typical properties, which are useful for interpretation of the final results. 

This chapter describes methods for time series analysis used for network reconstruction. Several properties of a time series are used to classify or to quantify the elements of a complex system from which the time series was obtained by measurements or simple data collection processes. We apply this analysis procedures to our raw data sets in the context of network creation. The created network represents relations between those elements. Often such relations are hidden and can only be found by statistical inference methods. An example for this is a synchronous increase in, e.g., the usage of or the attention to resources, which exist in different scopes. Such a scope can be a particular web application, a web-portal, or even an individual Wikipedia sub project which is dedicated to one language. 	

If for a pair of pages no link exists, one can find out more details about the context of the page by reading the page content. What do they describe? Is there something both have in common? Maybe they are about the same concept. This would give us already information which can be handled as a hidden link. But, one has to understand both languages in order to be able to identify such a relation. Since Wikipedia has more than two hundred different languages this is not a practical approach, even for the most dominant 10 languages, a manual context analysis is not practical.

Our generic approach is based on a structural comparison of content or alternatively on similarity measures for pairs of time series, derived from the real world objects. Depending on the measure it is possible now, to differentiate different interaction types, which represent different processes.

Such networks can be created and analyzed over time. The results of such an analysis, e.g. the size of the strongest connected component or the average node degree describes global properties of all the elements represented by this network. A simple investigation of the properties of individual elements would not allow us to reveal such global properties. Because of this we call such networks complex networks and the underlying system complex systems. Starting with individual node time series we can create a time dependent description of the entire system, using structural properties. Finally, we apply time series analysis methods again to such group properties. 

Thus, both, the network analysis and the time series analysis methods are combined. During the phase of network creation, time dependent properties of individual nodes are used to calculate the properties of links between the nodes and to create functional links as described in Part II of this work. 

This chapter describes several aspects of time series analysis procedures. Starting with practical considerations, related to the process of data acquisition and data cleaning procedures, some uni-variate and bi-variate time series analysis procedures are described. Naming conventions and symbols are taken from the book \textit{"The Analysis of Time"} \cite{Chatfiled2004} if nothing different is explicitly mentioned.


%Such links only exist as a concept and can be represented by a discrete link strength value which is 0 or 1. Only links with with a strength of 1 really exist. For more sensitive analysis one can define scalar or vectorial link strengths where all elements are real numbers. This values can even be secrete or continuous time dependent functions.

\section{Describing Real World Phenomenon with Time Series}
In order to study the behavior of things one needs a representation of them. Such a representation can be just an idea which allows at least to think about it. In many cases a verbal description using richness of language and drawings were the best way to represent and share knowledge. E.g., when Alexander Humboldt explored Central and South America in the 18-th century no camera existed. Nowadays, as a consequence of the technical revolutions, it is much simpler and common to create a digital representation of the real world which can be obtained by a manifold of measurement procedures. Furthermore, many properties of individual entities can be recorded over time. If a continuous recording is not possible we us a discrete approach based on snapshots or series of individual data points. In order to study the dynamics we must care about the right order of all observations. This means, the time of the specific measurement must be known. This is already an example for working with metadata. A series of chronologically ordered data points (measured values) has to be combined with all information required for contextualization. A series of values for which a time $t_0$ and a time interval $\delta t$ is known is called a time series.

\subsection{Modeling Time Series}

The amount of available data which can be efficiently be handled defines natural limits for data analysis and information retrieval procedures. A simple theoretical model of a time series is described by:
\begin{equation}
\label{tsmodel}
y(t) = A(t) + B(t) + C(t) + D(t)
\end{equation}
where $A(t)$ is a seasonal variation of a specific period length, $B(t)$ describes trend variations, $C(t)$ describes cyclical variations which correspond to recurring factors, and $D(t)$ covers all random variations, such as random extreme events which are not already covered by one of the three previous terms or noise of a particular type. Practical applications are based on scalar data, measured at discrete times. Especially the representation of data in computer systems requires discretization, even if the the raw signal is a available as a continuous signal, such as temperature and pressure of a gas. If the distance between to time steps $\Delta t$ for consequent data points in a time series goes to zero ($\Delta t \rightarrow 0$) one can describe the phenomenon with a continuous time dependent function. For practical reasons we use the discrete notation in this work.  

A time series model is useful to represent a process which is discovered in nature, in a way, which allows calculation of values $y(t)$ for any given $t$. Future values of a variable can be calculated more or less precisely, depending on the type of the underlying process and the knowledge about the system. In deterministic systems, the future position of an object can be calculated by Newtons equations of motion. But as soon as an unknown event occurs or a previously not considered factor influences the system as well in addition to all already covered influences, the results of such a prediction are not precise or even useless. 

In general, applications of models derived from data, are only valid in the context of several, often limiting, assumptions, e.g., existence of a stationary closed system. Terms $A(t), B(t),$ and $C(t)$ describe such deterministic processes. The third term, $D(t)$ is part of a second class of processes. Random processes are found in nature very often. The "Brownian motion", and "Random Walk" are important examples of random processes and have been studied extensively \textbf{\textcolor{green}{(CITE BRWONIAN MOTION, RANDOM WALK)}}. 

An alternative to the representation of a phenomenon in the time domain, is the frequency domain. This fransformation between both domains is given by the Fourrie-Transformation and inverse Fourier-Transformation. Many applications use this transformation combined with filters for a separation of a system into different subsystems. The length of a time series - time range between the first and the last data point - and also the resolution, which is the distance between consecutive data points influence the minimum and maximum frequencies measurable in a time series. 
Beside eq. \ref{tsmodel} or its Fourier transformation - which still would contain all available information - it is also possible to translate the time series data into arbitrary representations based on approximations - but this leads to loss of information. Information about the underlying process is already lost if the bandwidth of the Fourier spectrum is limited. 

Spectral filters are applied in order to isolate or separate individual coexisting aspects of processes. Such aspects can be decomposed by band filters. An example is separation of individual bands of EEG time series, particularly named $\delta$ (0.5-3.5 Hz), $\theta$ (4-7.5 Hz), $\alpha$ (8-11.5 Hz), $\sigma$ (12-15.5 Hz), and $\beta$ (16-19.5 Hz) (see section \textit{Method Summary} in Bashan \textit{et al.} \cite{Bashan2012}).

According to \cite{Keogh93segmentingtime} the Piecewise Linear Representation (PLR) is perhaps the most frequently used representation of time series for mining time series databases. The procedure which creates such a piecewise linear representation is called \textit{segmentation}. Especially in the context of similarity analysis it is a good practice to represent a time series by a set of motives. Such motives can be simply linear approximations of the time series or even polynoms of a higher order. 
%https://www.cs.rutgers.edu/~pazzani/Publications/survey.pdf
%Keogh, E., Chu, S., Hart, D., Pazzani, M. Segmenting Time Series: A Survey and Novel Approach. Data Mining in Time Series Databases. World Scientific Publishing Company

\subsection{Deterministic and Stochastic Components}
In Eq. \ref{tsmodel} we have deterministic and stochastic processes combined. A purely stochastic approach is the decomposition of the time series into a permanent and a transitory component, which both are stochastic. This methods was introduced by Beveridge and Nelson \cite{BEVERIDGE1981151} in order to study business life cycles based on economic time series. They state, \textit{that a large number of studies has shown that many economic time series are well represented by the class of homogeneous non-stationary 'ARIMA' process. In such a process, the first differences are a stationary process of autoregressive-moving average form.} In many cases, a transformation to natural logs is required, before the first differences are stationary \cite{BEVERIDGE1981151}. The benefit of this model is is that at any given time only values from the past are required. This avoids extrapolation problems associated with two-sided filtering methods such as centered moving averages \cite{BEVERIDGE1981151}. Since the decomposition depends only on the past, the calculation can be done in real time. 

\subsection{Continuous vs Discrete Time Series}
Properties of real world systems in nature are described by analytic functions. A mathematical representation is helpful primarily for theoretical investigation and furthermore for analytic models. In general, an analytic model is a set formulas, which describe the behavior of a system as a function of time or any other parameter. Since many different influencing parameters can be included, even if they can not be known exactly upfront, they can be estimated based on measured data. Finally, this allows predictions about the future behavior of a system. 

For practical reasons, time series are discretized. Discretization of continuos values happens automatically during the measurement process and is predominantly influenced by the measurement device or procedure, which can include post processing steps, such as re-sampling. In case of simulations, one has to discretise the data because digital devices such as computers can not handle continuous values. Although analog computers have several advantages \cite{UlmanBOOK}, e.g., they can be used to solve ordinary differential equations, they are not very common in recent data analysis and large scale simulation applications.   

In general, a time dependent property exists in a continuous time. It can be described as an analytical function. Because the function is evaluated only at discrete times we loose information during the discretesation procedures. Evaluating the function means here, that either a value $y_c(t)$ is calculated for time $t$ or a measurement $y_m(t)$ is available at time $t$. During the discretization procedure the indefinite number of possible times is replaced by a definite number of discrete time values. The discretazation is usually done in time or space, or both. Usually, a time series consists of a list of (measured) values, the time series data, and metadata. Metadata enables interpretation of the values in the right context. If a time series is presented as set of time-stamp value pairs, only a unique identifier for the whole unevenly spaced time series is required. Aris \textit{et al.} \cite{Aris2005} investigated advantages and disadvantages of several representaions of event time series.This work is relevant especially for large time series data sets, since the representation has a major impact on the overall performance of analysis procedures. 

\subsection{Event Time Series}
An event time series can have a natural origin, e.g., different things happen in distinct times, or it can be obtained by transforming a continuous time series as shown in fig.\ref{fig.ContinuosAndEventTimeSeries}. Peak detection algorithms are applied in order to find existing events based on patterns or certain rules. Such an event is characterized by a specific pattern (peak form, or motive) and a time stamp. The time can be the beginning of this pattern or even the time of a characteristic property such as a minimum or maximum value in the pattern. 

Based on simple thresholds it is also possible to define event time series. At the time, when a continuous value crosses the threshold an event is defined. The direction can also be used to provide further information, such as, categories of events. For an ensemble of time series we can use a mathematical expression such as $e(t)  \exists : y_i(t) >  n \cdot \sigma( y )$ to define individual events based on the groups properties. Figure \ref{fig.ContinuosAndEventTimeSeries} shows several time series examples. 

\label{ext.fig.ContinuosAndEventTimeSeries} 
\input{semanpix/ContinuosAndEventTimeSeries/imageLS}

For this work, we distinguish discrete time series (evenly spaced) and event time series (unevenly spaced) based on data availability. For an equidistant discrete time series the distance between two consequent values is a well known constant. We call the inverse of this value sampling rate which describes the number of values within a time interval of a second. If no data is available, usually zero is recorded, which is a problem if zero is also a valid value one could measure. It is not possible to distinguish, if the value exists and is zero or if a value is not available at this time. As long as the events are not really discrete, a lower sampling rate can be used, but according to the Nyquist–Shannon sampling theorem information is lost if expected duration, and sampling rate do not match. Due to sampling it is also possible to loose the precise time of the event. 

Using event time series solves this problem. Each value is stored in a list with an exact time stamp. This also saves storage space especially if sporadic events have to be handled. But event time series are not efficient for continuous values. A more detailed investigation of efficiency of different event time series is presented by Aris \textit{et. al.} in their article \textit{Representing Unevenly-Spaced Time Series Data for Visualization and Interactive Exploration} \cite{Aris2005} %\textbf{http://hcil2.cs.umd.edu/trs/2005-01/2005-01.pdf}. 
They discuss the challenges for unevenly-spaced time series data and compare the following four methods: (a) sampled events, (a) aggregated sampled events, (a) event index, and (d) interleaved event index. They describe the advantages, disadvantages,
choices for algorithms and parameters, and compare the different methods regarding performance. We consider this document as a guideline for further improvements of the Hadoop.TS software package.

\section{Data Cleaning and Preparation}
\label{DCLP}
Data sets are not always as clean as expected. Due to technical problems some values can be wrong or absent. Domain specific effects can lead to variations in the data, which do not reflect the real properties of the underlying process, one wants to study. E.g., if we are interested in the dynamical properties of attention payed to web ressources it is not enough tomeasure click rates only. It is alos relevant to contextualize appropriately. A jump in activity can be caused by be a real sporadic increase of interest in a topic, but also based on additional access channels which just connects more users which were already interested in that topic - the average increase might be zero in this case. 

\textit{Time series pre processing aims on providing the right data - which means the right time series data, grouped according to a particular research question - in an appropriate representation for further analysis procedures.}
 
If data is missing, one can try to fill the gaps based on simple assumptions, such as using average values based on a comparable time interval, or simply interpolation on existing data. Machine learning algorithms can be applied, if categorical data for contextualization is missing. But it is important to verify results of such procedures carefully. In general, such operations are not part of the analysis procedure, rather they belong to the measurement process. And if the measurement procedure can not be controled, the measured results are not reliable and additional steps can not lead trustworthy results. It is very important to use only reproducable operations during the data preprocessing phase. 

If data comes from different domains it is important to identify the right transformations which allow a comparison. Two common approaches are "zero-transformation" and "first differences". For time series modeling algorithms such the ARIMA process it is also important to provide stationary time series. 

\subsection{Standardization}
\label{sec.standardization} 
% http://www.mathworks.com/matlabcentral/fx_files/27561/1/content/MATS/helpfiles/TransformTimeSeries.htm
Time series $y_1(t)$ and $y_2(t)$ can only be compared to each other if properties like mean value or standard deviation, or even both are in the same range. In order to achieve this, a processing step called normalization, also known as "zero-transformation", is applied. This leads to normalized or standardized time series. Via a linear transformation of all samples of the time series a normalized time series $y_{norm}(t)$ is calculated with: $y_{norm}(t) = (y(t) - \mu) / \sigma$. The result $y_{norm}(t)$ has mean zero ($\mu=0$) and standard deviation one ($\sigma=1$).

Other relevant standardization approaches are Gaussian, Uniform and Linear standardization. 
For the Gaussian and the Uniform standardization a rank ordering function from the sample marginal distribution to marginal Gaussian distribution (Uniform distribution respectively) is applied. In case of a linear standardization the result is computed via a linear transformation so that the minimum is transformed to 0 and the maximum to 1.

\subsection{Detrending}
In order to acchieve this, trends are removed from the original data. Knowledge about such trends is valuable \textbf{(see chapter WEEKLY PATTERNS FOR NODE CLASSIFICATION)} After such trends have been removed one can analyze a mix of signals and noise. Figure \ref{sampleseries} shows three access-rate time series for Wikipedia pages (a,c,e) and the weekly trends (b,d,f). Since all pages are from the same language we can not see a a time delay for the nightly access minimum, but the influence of extreme events is visible also in the weekly trend patterns.  

\begin{figure}[b!]
	\centering
	\includegraphics[width=0.75\textwidth]{tmp/Wochengang.png}
	\caption{Beispiele der Wikipedia Zugriffsstatistik aus \cite{Burst} für drei ausgewählte Artikel mit \textit{(a\& b)} eher stationärer Zugriffsrate (Thema \,  Illuminati (book)\,), \textit{(c\& d)} mit plötzlicher endogener Zunahme an Aktivität (Peak am Sonntag, 7. Mai 2009, Thema \,  Heidelberg\,), und \textit{(e\& f)} eine exogene Zunahme an Aktivität (Thema \,  Amoklauf Erfurt\, hat einen Peak am Mittwoch, 11. März 2009, als ein anderer Amoklauf in Winneden passierte). Die linke Seite zeigt die kompletten stündlichen Zugriffszeitreihen (vom 1. Januar 2009 bis zum 21. Oktober 2009; z.B. für 42 Wochen = 294 Tage = 7056 Stunden) mit Nummern im Diagramm, welche die Höhe der Peaks angeben, welche abgeschnitten wurden, um die Fluktuationen in der Basislinie zu zeigen. Der rechte Teil zeigt die durchschnittliche Zugriffsrate für jede Stunde während eines Wochenzyklus von Montag bis Sonntag. Zu beachten ist der Effekt der Peaks auf den durchschnittlichen Zyklus am Sonntag in \textit{(d)} und am Mittwoch und Donnerstag in \textit{(f)}.}

\label{sampleseries}	
\end{figure}

Such cyclic patterns were already found by \textbf{AUTHOR} \textit{et al.} \cite{CircadianPatterns} in Wikipedia edit-event time series. We find two typical variations: (a) weekly patternsm, and (b) daily patterns, which are superposed. The dominating pattern is caused by human sleep patterns. Seasonal effects, content specific usage patterns, and cultural influneces might be the reasons for the patterns which overlapp the curve of activity trends even on a daily scale.

A procedure called \textit{"Whitening"} \textbf{(CITE: WHITENING)} detects trends in time series. The same parameters - used to describe the trend - are used to clean (detrend) all time series. This approach can only be used if all time series are expected to show the same fundamental properties which are represented by this trend. In case of Wikipedia we found, that trends are very different between pages from different topics and languages. Time zones and usage context lead to different weekly trends. Because of this we have to process trends on a per node level. The second approach we introduced in this work is a contextual detrending \textbf{(see chapter TRI)}. This approach is a kind of normalization based on the local neighborhood. 

\subsection{Time Shifts and Gaps}
Data sets from different domains can contain different features and side effects. In chapter ... \textbf{(REF TO STOCK MARKET ANALYSIS)} we use data form Wikipedia and from stock markets. While the Wikipedia access and edit history is always available - except during failures, the stock market activity is limited to trading days. Periodically, during the weekends there is no activity, but bank holidays also interrupt the activity.ignoring such days would lead to information loss. One can expect a different behavior of people before or after such special days. If such days are simple ignored, we would have to ignore also the available data from the second domain. Here we can think about the following scenario: persons use Wikipedia and other online systems to collect information about stock markets before they buy or sell. During the weekend or an official holiday, they probably have more time available for such information consumption activities. If this is true, we would miss the increased activity which is related to changes in the trading data during the next time period. This indicates how important the right alignment of time series from different domains is. Our approach is, to fill the gap with the average value of the last and next values before and after the gap. 

%\subsection{Aggregation and Resampling}
%\label{sec.dataaggregation}
%
%CITE THIS WORK AND EXPLAIN QUICKLY THE EFFECT OF Aggregation and Resampling.
% http://www.qucosa.de/fileadmin/data/qucosa/documents/8010/BA_Anne_Schmiedel.pdf
%\cite{Schmiedel2011}


\section{Univariate Time Series Analysis}
Our approach for describing and analyzing complex systems is primarily based on time series data which describe individual properties of individual single elements as a function of time (see red boxes in fig. \ref{fig.StructureVsFunction}.b) regardless their correlation or dependencies (see blue arrows in fig. \ref{fig.StructureVsFunction}.b). Such data is not a direct source for network creation. A time series contains data, collected for or from one individual thing, e.g. a node of a network. If links between such entities  physically exist, one can even measure the properties of those links as a function of time in some cases. Examples of such properties which can describe links are the density of traffic on roads in urban road networks 
%ETSOHA.cite 
%(CITE TRAFFIC NETWORKS), 
or the delay probability for flights in air traffic networks 
%ETSOHA.cite 
%(CITE AIRTRAFFIC NETWORKS). 
In some cases, the link properties are not direct accessable, this means they have to be calculated from pairs or triples of nodes as described in chapter \ref{NetReconstructionFramework}. 
Since the structural elements of complex systems are fundamental, they are in the focus of recent research activities. Weighted network measures address variablility and dynamics of system structure. Wiedermann \textit{et al.} \cite{Wiedermann2013} provide a detailed discussion about their new weighted network measures in the context of climate network studies.
%ETSOHA.cite 
%(CITE WEIGHTED NETWORK MEASURES)).
In order to study the dependency between function and structure it is important, first to create the appropriate time series representation and than to select the right metric or measure for depedency analysis. Depending on the kind of network one works with, it is also helpful to investigate the properties of individual elements by univariate analysis methods. This allows either to identify categories of elements based on such properties or it is possible to characterize the process which leads to such time series. 

Some analysis methods can only be applied to, e.g., stationary time series. If such specific properties are required, additional transformations have to be applied to the raw data. If a process is realy a stationary process or not can be found out by analyzing the average values, variances, auto-correlation, and higher order moments. Time series with inherent trends can be detrended or analysed via methods which include detrending. A common approach is simply to fit a linear or polynomial function which is then subtracted from raw data series to prepare the residual analysis. If no simple model can be found it is very handy to detrend by periodical averages, such as daily, weekly or monthly averages.

\subsection{Stationarity of Time Series}

A time series is callaed stationary if (a) there is nosystematic change in the mean value (no trend), no systematic change in variance, and all periodicity has been removed (see \cite{Chatfield2004} .p13). All individual sections of a time series show comparable properties if the time series is statonary. This strongly depends on the length of a window. Short time ranges can be interpreted as quasi stationary, since no trends and no periodicity exist on the selected scale. Especially for short windows the statistical significance can be the limiting factor, while the nature of the process has a strong impact on stationarity on longer time ranges.    

Furthermore, the noise level and overlayed periodicity have an influence on analysis results. For simple calibration experiments, reference models are used to create comparable time series for statistical tests, e.g., random noise is added to well known functions parametrized by fixed or even random parameters. Analysis algorithms are applied to such semi-random data in order to measure the influence of patterns on specific features measured by certain algorithms. Common patterns are: (a) sine-waves with a given phase $\phi$, amplitude $a$, and frequency $f$, single spikes of a given height $a$, plateaus and increasing or decreasing trends between fixed levels $[a_{min}, a_{max}]$ a of given width $w$. 

Often such pattern also exist in real data. Because they have an impact on the metrics calculated from the raw time series, one has to study this impact in more detail. In chapter \ref{impact.of.peaks} we investigate the impact of single spikes on the correlation properties, calculated by Pearson-Correlation. This approach can be generalized further, which allows us also to calibrate the methods. Based on such a calibration we can define error or significance levels depending on the flavor of data being analyzed. 

Wenn ein Prozess nicht stationär ist, dann lassen sich bestimmte Analysen,
wie z.B. Kreuzkorellation nicht einfach anwenden. Für Granger Causality
trifft das auch zu. Um die Daten nun in eine stationäre Form zu bringen,
kann man Trends ermitteln und abziehen (Differenz) oder mit relativen Daten
weiterarbeiten (wir haben dazu durch den "Wochentrend" dividiert) oder die
Inkremente das Signals betrachten, wenn diese stationär sind.

Repräsentiert nun diese Reihe noch den originalen Prozess?
Ja, sie repräsentiert den Prozess, aber nicht vollständig.  Das ist bei
Zeitreihen immer so -- sie sind nie eine vollständige, sondern immer eine
dimensionsreduzierte Repräsentation der Prozesse.

Man entfernt ja Information und müsste dann ja sagen, nach Entfernung der
Trends, wie auch immer man diese nun interpretiert - erhalten wird eine
bestimmte Korrelation. Man äussert sich in dem Moment nur zu den
"zufälligen" Anteilen, also nur zu dem nicht vom Trend bestimmten
Verhalten. Ist diese Argumentation korrekt?
Ja, schon korrekt.  Aber es bleiben nicht nur zufällige Anteile übrig,
sondern meist noch mehr.  EIN deterministischer Anteil wird abgezogen --
das macht den Prozess stationär, wenn nur dieser deterministische Anteil
nicht-stationär war. 

The average and standard deviation of a time series allows identification of purely random processes. In this case, both are constant. In case of a random walk, which is a non stationary process with time dependent average value and standard deviation, the series of first differences is stationary. 

Aber bei einem Random Walk ist beispielsweise der
stochstische Anteil nichtstationär -- da kann man so keinen stationären
Prozess draus machen, sondern man müsste die Inkremente betrachten.

\label{ext.fig.Average} 
%\input{semanpix/7DISTR/imageLS}
\input{semanpix/UVADistributions/imageLS}

%https://quantivity.wordpress.com/2011/02/21/why-log-returns/
Financial time series, especially daily stock prices $p_t$ are available but for data analysis the logarithm of returns $log( r_i )$ are used, rather than price or raw returns. The return $r_i$ at time $i$ is defined as the relative price change $\frac{p_i-p_j}{p_j}$ since the the last time $j=i-1$. This leads to an implicit normalization and allows a direct comparison of all components of a stock index, even if their price level, trading volume and contribution to the index differ or unknown. Figure \ref{.fig.Average}.a shows distrubution of raw prices in comparison with a values obtained from a simple standardization (figure \ref{.fig.Average}.b) , and calculated log returns (figure \ref{.fig.Average}.c).


\textbf{CITE}


First, log-normality: if we assume that prices are distributed log normally (which, in practice, may or may not be true for any given price series), then $log(1 + r_i)$ is conveniently normally distributed, because:

 \begin{equation}
    1 + r_i = \frac{p_i}{p_j} = \exp^{\log(\frac{p_i}{p_j})} 
 \end{equation}

This is handy given much of classic statistics presumes normality.

Second, approximate raw-log equality: when returns are very small (common for trades with short holding durations), the following approximation ensures they are close in value to raw returns:

\begin{equation}
    \log(1 + r) \approx r , r \ll 1 
\end{equation}

Third, time-additivity: consider an ordered sequence of n trades. A statistic frequently calculated from this sequence is the compounding return, which is the running return of this sequence of trades over time:

 \begin{equation}
 (1 + r_1)(1 + r_2)  \cdots (1 + r_n) = \prod_i (1+r_i)
 \end{equation}
 
This formula is fairly unpleasant, as probability theory reminds us the product of normally-distributed variables is not normal. Instead, the sum of normally-distributed variables is normal (important technicality: only when all variables are uncorrelated), which is useful when we recall the following logarithmic identity:

    \begin{equation}
    \log(1 + r_i) = log(\frac{p_i}{p_j}) = \log(p_i) - log(p_j) 
    \end{equation}

Thus, compounding returns are normally distributed. Finally, this identity leads us to a pleasant algorithmic benefit; a simple formula for calculating compound returns:

    \begin{equation}
    \sum_i \log(1+r_i) = \log(1 + r_1) + \log(1 + r_2)  + \cdots + \log(1 + r_n) = \log(p_n) - \log(p_0)
    \end{equation}

Thus, the compound return over n periods is merely the difference in log between initial and final periods. In terms of algorithmic complexity, this simplification reduces O(n) multiplications to O(1) additions. This is a huge win for moderate to large n. Further, this sum is useful for cases in which returns diverge from normal, as the central limit theorem reminds us that the sample average of this sum will converge to normality (presuming finite first and second moments).

Fourth, mathematical ease: from calculus, we are reminded (ignoring the constant of integration):

\begin{equation}
    e^x = \int e^x dx = \frac{d}{dx} e^x = e^x 
\end{equation}

This identity is tremendously useful, as much of financial mathematics is built upon continuous time stochastic processes which rely heavily upon integration and differentiation.

Fifth, numerical stability: addition of small numbers is numerically safe, while multiplying small numbers is not as it is subject to arithmetic underflow. For many interesting problems, this is a serious potential problem. To solve this, either the algorithm must be modified to be numerically robust or it can be transformed into a numerically safe summation via logs.

As suggested by John Hall, there are downsides to using log returns. Here are two recent papers to consider (along with their references):

Comparing Security Returns is Harder than You Think: Problems with Logarithmic Returns, by Hudson (2010)
Quant Nugget 2: Linear vs. Compounded Returns – Common Pitfalls in Portfolio Management, by Meucci (2010)

\textbf{CITE}

\subsection{Autocorrelation}

The analysis begins with the subtraction of the average value $\Delta x_j(t) = x_j(t) - \bar x_j$ with 
$\bar x_j = \langle x_j(t) \rangle = {1 \over L} \sum_{t=1}^L x_j(t)$.  Here, $L$ is the lengths of the 
considered $j$th time series $(x_j(t))$.  Then the (auto-) correlation function is calculated for various 
time delays $s$ (see, e.~g. \cite{Kantelhardt.2009}),
\begin{equation}
 C(s) = {\frac{ 1 }{ \langle \Delta x_j(t)^2 \rangle (L-s)}} \sum_{t=1}^{L-s} \Delta x_j(t) \Delta x_j(t+s)
\end{equation} 
If the $\Delta x^j(t)$ are uncorrelated, $C(s)$ is fluctuating around zero for $s>0$. For the relevant 
case of long-term correlations, $C(s)$ decays as a power law characterized by a correlation exponent $\gamma$,
%
\begin{equation}
C(s) \sim s^{-\gamma}, \quad 0<\gamma<1.  \label{gamma}
\end{equation}
%
A direct calculation of $C(s)$ is often hindered by unreliable behavior of $C(s)$ for large $s$ due to finite-size 
effects (finite $L$) and non-stationarities in the data (i.~e. a time-dependent, not well-defined average $\langle 
x_j(t) \rangle$ that changes with the considered length $L$).



The auto-correlation function is calculated for a range of time lags $k$ by calculating the pearson correlation for the original time series with a shifted time series. For stationary time series with no short term correlations the correlogram shows a curve with a value of 1 for $k=0$. Trends in the time series cause misleading results, e.g., a very slow decay as shown in figure ... .a is an indicator for non stationary time series. For comparison we show the autocorrelation function for log returns of daily stock market prices during the period 2003 to 2014 for companies from the German stock market index DAX.

Wann divergiert $s_x$?


\label{ext.fig.AutoCorrelation} 
\input{semanpix/EXP0/ac}
 

This method works only for short time lags and thus it can not be used to analyse long-term correlations.



\subsection{Detrended Fluctuation Analysis (DFA)}

The DFA method was introduced in order to overcome these obstacles \cite{Peng.1994}.  It has become a widely
used technique for the detection of long-term correlations in noisy, non-stationary time series with more than 
750 publications using the approach up to now (2013); see \cite{Kantelhardt.2001,Kantelhardt.2009,Bashan2008}
for more detailed discussions of the method and its properties. In general, the DFA procedure consists of the 
following four steps:
\begin{enumerate}
\item calculate $Y_{j}(i) = \sum_{t=1}^{i} [x_{j}(t)-\langle x_{j}(t)\rangle]$, $i=1,\ldots ,L$, the so-called 'profile',
\item divide $Y_{j}(i)$ into $L_{s} = \mathrm{int}(L/s)$
  non-overlapping segments of equal length $s$, 
\item calculate the local trend for each segment by a least-square fit to the data, where linear,
  quadratic, cubic, or higher order polynomials \cite{Bunde.2000} are used in the fitting procedure,
\item determine the variance $F_{s}^{2}(\nu)$ of the differences between
  profile and fit in each time segment $\nu$ of $s$ data points,  
\item calculate the average of $F_{s}^{2}(\nu)$ over all segments $\nu$ and 
  take the square root to obtain the fluctuation function $F(s)$. 
\end{enumerate}

Multiple iterations with segments of different $s$ are necessary to determine the dependency 
of $F(s)$ on the time scale $s$.  For long time series this is a time consuming procedure which fits well to the 
distributed approach supported by Hadoop.TS.  Usually, $F(s)$ increases with increasing 
$s$.  If data $x_{j}(t)$ are long-term power-law correlated according to Eq.~(\ref{gamma}), $F(s)$ increases, 
for large values of $s$, as a power-law, \cite{Peng.1994,Kantelhardt.2001,Kantelhardt.2009}
%
\begin{equation}
F(s)\sim s^{\alpha },\quad \alpha =1-\gamma /2.  \label{alpha}
\end{equation}%
%
The fluctuation exponent $\alpha$ is calculated by a linear fit applied to a plot of $F(s)$ as a function of $s$ 
on double logarithmic scales.  For long-term correlated time series one find $\alpha > 0.5$, and in the case of 
$\alpha = 0.5$ the data is uncorrelated. 




\subsection{Multifractal Detrended Fluctuation Analysis}
How is MFDFA related to DFA?

Several moments scale differently. 

Generalised Hurst-Exponent is used to quantify this - how?

What are the MOMENTS of a higher order?

Show results for STOCK DATA 


\subsection{Return Interval Statistics (RIS)}
\label{sec.RIS}

Power Law in Youtube: Sornette, \cite{G}

Here we talk about event time series ... How do we define those?

Use the threshold picture from a paper from Jan.

See our poster to explain how RIS works.

RIS is an alternative to continuous time series. Esepcially for sparse event time series it is possible to identify long range correlations by RIS.



Long-term memory effects in dynamic systems can also be identified based on the 
analysis of return intervals between extreme events that exceed a given threshold. 
Depending on the properties of the underlying system the distribution of inter-event times
can follow a power-law distribution, a Poisson distribution, an stretched exponential distribution or even a bimodal 
distribution like it was shown recently by an analysis of telecommunication data \cite{Wu.2010} of human interaction events.
To describe the recurrence of events exceeding a certain threshold $q$,
i.~e., $x_{j}(t)>q$, one investigates the statistics of the return time intervals 
$r = t_2 - t_1 \vert x_{j}(t_1)>q \wedge x_{j}(t_2)>q \wedge x_{j}(t)\le q \vert t_1 < t < t_2$ 
between such events at times $t_1$ and $t_2$. 
In an uncorrelated time series ('white noise'), the return intervals are also uncorrelated and distributed according 
to the Poisson distribution, 
%
\begin{equation}
P_{q}(r)=(1/R_{q})\exp (-r/R_{q}),  \label{simple}
\end{equation}%
%
where $R_{q}$ is the mean return interval $\langle r \rangle$ for the given threshold $q$. For long-term correlated data, 
on the other hand, a stretched exponential distribution 
%
\begin{equation}
P_{q}(r)={\frac{a_{\gamma }}{R_{q}}}\exp [-b_{\gamma }(r/R_{q})^{\gamma }]
\label{stretched}
\end{equation}%
%
has been observed \cite{Bunde.2003,Bunde.2005,Altmann.2005,Eichner.2007,Ivanov.2004} 
where the exponent $\gamma$ is the correlation exponent from Eq.~(\ref{gamma}), and 
the parameters $a_{\gamma }$ and $b_{\gamma }$ are independent of $q$ \cite{Altmann.2005,Eichner.2007}.
In order to compare time series with different average inter-event times $R_q$ the 
normalized distributions $P_q(r) R_q$ of return intervals $r$
between events exceeding the different thresholds $q$ have to be used.






\subsection{Detection of Long Term Correlations with DFA and RIS}

Why is DFA better than Autocorrelation analysis?\\

How are DFA and RIS connected with each other?\\


\begin{figure}[htp]
 \centering
 \includegraphics[width=2.9in]{semanpix/FFMandPR/Image5b.eps}
 \includegraphics[width=2.9in]{semanpix/FFMandPR/Image5c.eps}
 \caption{For two surrogate data sets (uncorrelated: red circles, correlated with $\gamma=0.2$ according to Eq.~(1): 
olive diamonds) results of both methods of long-term correlation analysis are shown: 
(a) DFA of order two and (b) RIS.  The lines indicate the theoretical behaviors according to Eqs.~(3)-(5) -- red line: 
$\alpha=0.5$ in (a) and simple exponential in (b), olive line: $\alpha=0.9$ in (a) and stretched exponential in (b).} 
\end{figure}


\subsection{Entropy}
Entropy is another very important and even more generic property, especially in the context of information theory. Even if required conditions like normal distribution of values or stationaryty are not given, it is possible to calculate and compare the entropy. Active information storage and within a time series and influence from one series to another, represented as transfer entropy are important measures and can be integrated into network reconstruction methods. Figure \ref{fig.JIDTConcept} shows an illustration of the information theoretical concept as presented be Lizer et al. and was inspired by figure 1 in \cite{Lizier2014}.

Entropy depends only on distribution of values. Shuffling the values will not change the entropy. In case of sliding windows we can expect different values. In order to measure the difference of Entropy between two consequent time windows, the Kullbach-Leibler Konvergenz is calculated.


%\label{ext.fig.JIDTConcept} 
%\input{semanpix/JIDTConcept/imageLS}

\label{ext.fig.StructureVsFunction} 
\input{semanpix/StructureVsFunction/imageLS}


WHY DO WE NEED THIS IMAGE?

WHAT CAN WE EXPLAIN USING THIS IMAGE?

If an long term memory exist or not can be analyzed by DFA, MFDFA and RIS as shown before. A simple comparison of entropy could even be a fast - but not so reliable - alternative. The higher, or stronger the memory effect is, e.g. in long-term correlated time series, one can expect a smaller entropy. Purely random data, such as white noise has the highest entropy.


Long term memory of a system can be analyzed by the Hurst exponent. The Hurst exponent is related to the decay of the auto-correlation function. Furthermore, the generalized Hurst exponent is used in fractal geometry. The Hurst exponent can be estimated by DFA which is an advantage especially in case of non stationary time series. 

The difference of two probability distributions is calculated by Kulback-Leibler Divergenz.

\subsection{Multifractality}

\cite{Morales2012a} introduced a specific variant of a generalized Hurst exponent to measure the time dependent fractality of financial time series during the years 1995 to 2009. They calculate the weighted generalized Hurst exponent from time series using a sliding window technique combined with exponential smoothing. This has the consequence, that more recent events contribute to or influence presence more than older events. They suggest, that the scaling behavior of such time series can be used as an indicator for stability of companies or markets in general because they found that the multifractality of companies which did not survive the crises showed an increased multifractality during the crisis. They also study the scaling behavior of the log returns for and discuss the relation between both types of scaling.
\textbf{TODO:}Make clear, what the measures are and how the scaling types are related.


\section{Multivariate Time Series Analysis}

This section explains analysis procedures for time series pairs and tuples of time series. All mentioned approaches are not new. But, although they are widely accepted and often used, I think it is important to start with a warning: Cross-Correlation or Pearson-Correlation should not be considered to be universal approaches for reliable link creation for functional networks. 



First, I review the positive and negative properties of the mathematical algorithms in general. Beside Perason-, Spearman-, and Kendall-Correlation, a promising approach, the Event-Synchronization is investigated in more detail. Than, the theoretical concepts Mutual Information (MI), and Granger-Causality (GC) are presented. Both are highly relevant, the first in inforamtion theorie (CITE RELEVANCE OF MI) and the second even in economic studies (CITE RELEVANCE OF GC). The variety of available measures allows even a generalization of our analysis framework to other related and even new fields of application. A more specific discussion about link strength calculation and significance tests is provided in chapter \ref{IdentifySignificantLinks}.
%%ETOSHA.LINK.FROM \ref{IdentifySignificantLinks}.

It is important to note, that statistical methods, e.g. Pearson Correlation and Mutual Information analysis, in general reveal inter-dependencies. The correlation value obtained from Pearson-Correlation has a sign, but this tells nothing about the direction of the relationship between two time series. Event-Synchronization gives a synchronization strength $Q$ and direction information $q$ from which directed networks can be created. Finally, methods like Granger Causality, and Partial Correlation where developed to investigate causal dependencies directly. Instead of causal dependency it is also called \textbf{"causal predictability"} (CHECK CITEATION AND CORRECT). 

\cite{Pereda2005}
\textit{Multivariate time series analysis is extensively used in neurophysiology with the aim of studying the relationship between simultaneously recorded signals. Recently, advances on information theory and nonlinear dynamical systems theory have allowed the study of various types of synchronization from time series. In this work, we first describe the multivariate linear methods most commonly used in neurophysiology and show that they can be extended to assess the existence of nonlinear interdependence between signals. We then review the concepts of entropy and mutual information followed by a detailed description of nonlinear methods based on the concepts of phase synchronization, generalized synchronization and event synchronization. In all cases, we show how to apply these methods to study different kinds of neurophysiological data. Finally, we illustrate the use of multivariate surrogate data test for the assessment of the strength (strong or weak) and the type (linear or nonlinear) of interdependence between neurophysiological signals.}

\subsection{Pearson Correlation}
\label{crosscorrelation.con})

In case of a linear system, the input and output variables, represented as time series are related to each other by a linear model $y(t) = m X(t) + n$. If such a linear relation exist can be analyzed by correlation analysis. The Pearson-Correlation coefficient was introduced by \textsc{Pearson} in \textsc{1846} \cite{Pearson1846}. 
Some reasons exist, which prevent us from using this approach. 

Later in chapter ... we use Pearson Correlation in order to measure the similarity of two time series. A linear dependency with no time delay would lead to a high correlation value. 
Since we do not know any detail about the internal structure of the system we can not expect linear dependencies between the elements for which the correlation strength will be calculated. Radebach \cite{Radebach2010} uses Mutual Information instead of Pearson Correlation to define the association between nodes in climate networks especially beacause PC is limited to linear relations. Nonlinear dependencies could not be detected by PC. 


\cite{Kruskal1958}
See details in Wikipage: \textbf{http://semanpix.de/opendata/wiki/index.php?title=Kruskal1958}

The cross-correlation function $F_{CC}(\tau)$ is defined as a convolution, which is a mathematical operation on two functions $f(t)$ and $g(t)$. For each value of $\tau$ one obtains a number which expresses the overlap of both functions $f(t)$ and $g(t)$. The value of $\tau$ which corresponds to the maximum value of $F_{CC}(\tau)$ is a typical measure for a time delay between two signals. 
\begin{equation}
 F_{CC}(\tau) = (f \ast g)(\tau) = \int_{-\infty}^{\infty} f(t) \cdot g(t+\tau) dt
\end{equation}.
For comparison of two time series, and if no time delay is assumed, one simply calculates  the cross-correlation value $F_{CC}(\tau=0)$. This allows a quantitative interpretation of equality of the shapes of both time series. In case of periodic signals, especially if the frequency are equal, it would be possible to measure a phase difference. It is important to note, that a high or low cross-correlation value does not allow a conclusion about causal relations. Figure \ref{fig:example_cc} shows typical examples for cross-correlation functions.

\begin{figure}[htp]
\centering
\label{fig:example_cc}
\end{figure}

Furthermore, Pearson correlation is used for correlation analysis to identify correlations between two signals, functions or variables not only a comparison of signal shapes. 

Ihre Verwendung ist daher vielfältig, ein Gebiet der Anwendung stellt z.B. die Materialwissenschaften und die Werkstofftechnik \cite{Ultraschall} dar, sowie die Astrophysik \cite{DarkEnergy}. Desweiteren gibt es Anwendungsbeispiele außerhalb der Physik, wie z.B. medizinische Themen \cite{Psychosozial} und seismografische \cite{RandomFields}.\\
Die Kreuzkorrelationsfunktion ist nach \cite{Signaluebertragung} wie folgt definiert
\begin{align}
R_{x,y}(\tau) = \int_{- \infty}^{+ \infty} \, x(t) \cdot y(t-\tau) \, dt
\end{align}
\begin{table}[H]
\begin{tabular}{rccl}
mit  & $R_{x,y}$ & = & Korrelationsfunktion\\
	 & $t$ & = & Zeit\\
	 & $\tau$ & = & Zeitverschiebung\\
\end{tabular}
\end{table}

Für den Fall zweier identischer Zeitreihen ergibt sich die Autokorrelationsfunktion, für verschiedene Zeitreihen die Kreuzkorrelationsfunktion \cite{Ultraschall}. \\
Auf Grund der Variation der Zugriffsvolumen wird für eine quantitative Aussage zur normierten Kreuzkorrelationsfunktion \cite{Ultraschall} übergegangen
\begin{align}
\varphi(\tau) = \dfrac{R_{x,y}}{\sigma_{x}\,\sigma_{y}}
\label{KK}
\end{align}
\begin{table}[H]
\begin{tabular}{rccl}
mit  & $\sigma_{x} \, , \sigma_{y}$ & = & Standardabweichung der Signale $x(t)$ bzw. $y(t)$\\
\end{tabular}
\end{table}
Für diskrete Funktionen, wie die hier vorliegenden Zeitreihen, wird die Summenschreibweise verwendet. Da sich die zu untersuchenden Auswirkungen von der ersten Zeitreihe auf die zweite innerhalb eines Messintervalls von einer Stunde liegen, wird die zeitliche Verschiebung $\tau = 0$ gesetzt\footnote{D.h. es wird ohne Zeitverschiebung gerechnet, die Signale werden zu gleichen Zeitpunkten miteinander verglichen.}. Auf diese Weise kann nun der Tageswert der Kreuzkorrelation berechnet werden:
\begin{align}
\varphi_{T} (0) = \dfrac{\sum_{\substack{t=1h+24h\cdot T}}^{24h(1+T)} \bigl( x(t) - \overline{x} \bigr) \cdot \bigl( y(t) - \overline{y} \bigr)}{\sqrt{\sum_{\substack{t=1h+24h\cdot T}}^{24h(1+T)} \bigl( x(t) - \overline{x} \bigr)^{2}} \,\, \sqrt{\sum_{\substack{t=1h+24h\cdot T}}^{24h(1+T)} \bigl( y(t) - \overline{y} \bigr)^{2}}}
\label{KreuzKorrelation}
\end{align}

\begin{table}[H]
\begin{tabular}{rccl}
mit  & $\overline{x} \, , \overline{y}$ & = & Mittelwerte der Signale $x(t)$ bzw. $y(t)$\\
\end{tabular}
\end{table}
Zur Berechnung des Mittelwertes wird die Definition des arithmetischen Mittel \cite{TaschenbuchMathematik} verwendet:
\begin{align}
\overline{x}_{arithm} = \frac{1}{24} \sum_{\substack{t=1h}}^{24h} x(t) = \dfrac{x(1h)+x(2h)+ \ldots +x(24h)}{24}
\label{Mittelwert}
\end{align}


 Zur statistisch zuverlässigen Charakterisierung des Zusammenhangs zwischen den beiden zu untersuchenden Zeitreihen geht man zu größeren Zeitskalen über. Für den Monatswert wird der Median (nach \cite{TaschenbuchMathematik})der Tageswerte gebildet. Zuvor wurden die Tageswerte $\varphi_{T}$ der Größe nach sortiert.
Diese Methode wird verwendet, damit einzelne, sehr stark abweichende Tage, den Gesamtwert nicht verfälschen. Es ergibt sich für den Monatswert:
\begin{align}
x_{M} = \begin{cases} \varphi_{T+1}& \text{falls $n'=2T+1$},\\
							\nonumber\\
							 \dfrac{\varphi_{T+1}+\varphi_{T}}{2}& \text{falls $n'=2T$}. \end{cases}
\end{align}
\begin{table}[H]
\begin{tabular}{rccl}
mit  & $n'$ & = & $n - f_{T}$\\
	 & $n$ & = & 28 Tage $\ldots$ Gesamtanzahl der Werte pro Monat\\
	 & $f_{T}$ & $\ldots$ & fehlerhafte Tage pro Monat\\
	 & $T$ & \ldots & Tag\\
\end{tabular}
\end{table}
Die Gesamtanzahl der Werte pro Monat wurde auf 28 Tage festgesetzt. Die vorliegenden Datenreihen umfassen 42 Wochen. Ein Monat entspricht einem Zeitraum von 4 Wochen. Dieses Zeitfenster wird nun um eine Woche verschoben, der 2. Monat umfasst somit die Wochen 2 bis 5. Mit dieser Definition ergeben sich 38 mögliche Verschiebungen des Zeitfensters und daher enthält der vorliegende Jahres -Zeitraum 38 Episoden der Dauer eines Monats.\\ 

Für den Jahreswert werden Monate mit $f_{M} \geqslant 19$ ausgeschlossen, auf Grund der zugeringen Statistik. Anschließend wird der Mittelwert (nach \cite{TaschenbuchMathematik}) gebildet, das Ergebnis stellt den Jahreswert dar. 
\begin{align}
\overline{x}_{J,arithm} = \frac{1}{m'} \sum_{\substack{i=1}}^{m'} x_{M,i} = \dfrac{x_{M,1}+x_{M,2}+ \ldots +x_{M,m'}}{m'}
\end{align}
\begin{table}[H]
\begin{tabular}{rccl}
mit  & $m'$ & = & $m - f_{M}$\\
	 & $m$ & = & 38 Monate $\ldots$ Gesamtanzahl der Werte pro Jahr\\
	 & $f_{M}$ & $\ldots$ & fehlerhafte Monate pro Jahr\\
\end{tabular}
\end{table}
Die Standardabweichung (nach \cite{TaschenbuchStatistik}) für ein Jahr ist ein Maß für die Heterogenität bzw. Homogenität der Tage innerhalb des Jahres. Ein hoher Wert für die Standardabweichung stellt hierbei eine große Streuung der Werte dar. Dementsprechend bedeutet ein kleiner Wert, dass viele Werte kaum vom Mittelwert abweichen. Fehlerhafte Tage heißt in diesem Fall, dass mindestens eine der Zeitreihen keinen Zugriff in diesem Zeitfenster hatte. Der Nenner (siehe Gleichung \ref{KreuzKorrelation}) würde Null werden und damit wäre der Ausdruck mathematisch nicht definiert. Die fehlerhaften Berechnungen, welche 'Not a Number' ergeben würden - d.h. mathematisch nicht definierte Operationen werden ausgeführt - werden ausgeschlossen.
\begin{align}
s_{J} = \sqrt{\dfrac{1}{m'}\,\sum_{\substack{i=1}}^{m'} \bigl( \varphi_{M,i}-\overline{\varphi}_{M}\bigr)^{2}}
\label{Standardabweichung}
\end{align}
\begin{table}[H]
\begin{tabular}{rccl}
mit  & $m'$ & = & $m - f_{M}$\\
\end{tabular}
\end{table}

\textbf{Why not good for sparse data?}
\label{crosscorrelation.con})

A bi-variate normal distribution and a reasonable large number of value pairs are required to identify significant correlation between processes. Only linear dependencies can be identified. 

SEE EXAMPLE OF DEPENDENT function, for which no PEARSON CORRELATION BUT A HIGHER ORDER CORRELATION exist.

SHOW an image with such a nonlinear dependency and demonstrate the "MI-Advantage" using the MULTIVARIATE Toolbox. 




\subsection{Rank Correlation}
In many cases, especially if not enough data is available and if the value pairs are not taken from a bi-variate normal distribution it is not possible to identify correlation between two time series in a reliable meaningful way. Two alternatives are Spearman-Rank-Correlation and Kandall Rank Correlation.

EXPLAIN THE APPROACH FOR SPEARMAN ....

\begin{equation}
\label{spearman} 
\end{equation}

EXPLAIN THE APPROACH FOR KENDALL ....

\begin{equation}
\label{kendall} 
\end{equation}

We use the Pearson-Correlation primarily as a similarity measure for calculation of functional links from time series data on several time scales. Rank correlation is used for statistical correlation analysis for dynamic processes in Wikipedia.

\newpage
\clearpage
\subsection{Event Synchronisation}
\label{ES} 
Synchronization analysis is used to quantify how well two processes are aligned to each other in the time domain. Depending on the nature of the processes one has to compare each individual pair of events or one can use simplified representations, such as periodical functions. This allows to say, that synchronization can be quantified if for two processes an exact coincidence in time or phase can  be measured.


Event series are usually not used to describe periodic processes. \textit{Event Synchronization} (ES) compares the time delay between pairs of events from two different processes, especially if no periodicity exists in both. ES is a simple and efficient algorithm which operates on pairs of sparse event time series \footnote{Aris \textit{et al.} (\cite{Aris2005}) discuss the advantages and disadvantages of four different representations of unevenly-spaced time series, especially in the context of visualization. Sampling and aggregation reduce the required memory but introduce errors. Time differences are not represented in case of time indexes, and the time axis is partially stretched by interleaved event indexes.}. Quiroga \textit{et al.} introduced  ES in 2002 to study neural spike trains in EEG signals \cite{Quiroga2002}. Malik \textit{et al.} applied this method to data about monsoonal rainfall \cite{Malik2010,Malik2012}. So far, this both seem to be the only applications of ES. 

One of our sub projects was done in order to investigate properties and features of this promising method before we use it to create correlation networks. Results of our experiments will be discussed in this section. The first experiment simulates a transition between identical and independent time series, the second one covers the case of different event densities. 

The number of events divided by total time called event density. This allows a synonymous usage of "density" and "event count". The ratio of both series densities is another interesting value, used on the x-axis in figure \ref{fig.ESIdenticalToIndependentEx2}.b and figure \ref{fig.ESIdenticalToIndependentEx3}. One has to be careful, if different data sets with different aggregation properties and different time series length should be combined. Re-sampling, as explained in section \ref{sec.dataaggregation}, 
%%ETOSHA.TODO%% EXPLAIN THE RESAMPLING
has to be applied to all series before event synchronization is calculated for time series pairs from multiple sources. Furthermore it is recommended to verify the calibration functions with new data sets, as our initial experiments may not 
reflect particular properties in measured data, such as long term correlations.
 
The following section concludes our preliminary findings and describes practical aspects of ES implementation and usage. Furthermore, results from \cite{Boeker2012} are summarized. 
With our implementation of the ES algorithm in the Hadoop.TS software package \cite{Hadoop.TS} it is possible to calculate the synchronicity of time series pairs in large scale data sets using Apache Hadoop or Apache Spark on clusters of distributed computers. How the software library can be integrated into Matlab is explained in \cite{Hadoop.TS.Matlab.Integration}.
%%ETOSHA.TODO%% FINISH the tutorial https://github.com/kamir/Hadoop.TS/wiki/How-To-use-Hadoop.TS-in-Matlab

\subsubsection*{Motivation to use Event Synchronization}
Collaborative processes like the Wikipedia edit process are the results of self organization. The content creation process has a different nature compared to the crowd based information consumption process, such as usage of Wikipedia pages by users and online or mobile applications. One reason is, that both have a different audience. Changes to pages are not done so often, in many cases less than once a month. Also the number of Wikipedia editors is not as big as the number of readers or indirect users like automatic tools which have Wikipedia clients build in. Because of this, the edit event time series are rather sparse and methods for dense or continuous time series like cross-correlation or rank-correlation can not be applied. 

To track the evolution and to compare the properties of Wikipedia pages at several discrete times, we implemented a data structure called \emph{event time series} (ETS). Such an ETS object contains the index or the time stamp as a numerical value of data type long. Even if multiple events occur at the same time or during the same period (which has the length of the selected time window and expresses the time resolution) we do not store the number of individual events in binary mode (see figure \ref{fig.ETS_spikes_and_ETS_PAIR_COMPARISON}.a). Only the time stamp or the index of periods for which events have been registered are stored. The default time resolution is milliseconds as this is also the default resolution of the timer in common computer systems. We use hourly resolution for practical reasons. The incremental mode allows tracking of event counters or values like page size or nr of links on a page. Figure \ref{fig.ETS_spikes_and_ETS_PAIR_COMPARISON}.b and \ref{fig.ETS_spikes_and_ETS_PAIR_COMPARISON}.c show examples for low and very high event densities.   


Both, Wikipedia edit-event time series and Wikipedia access-rate time series are the results of aggregation procedures. ETS can be visualized in different ways, e.g., as shown in figure \ref{fig.ETS_spikes_and_ETS_PAIR_COMPARISON}. The number of detected events during each measurement interval or at each time step is shown in the top row. Because a useful data set inspection is not possible with this representation a second plot is provided. The bottom row shows the event number or event index as a function of time at which it was detected. The total number of events can be much higher than the shown maximum values. We plot does no show the absolute number of counted events but the event index. Aris \textit{et al.} \cite{Aris2005} use a comparable approach called \textit{event index} and \textit{interleaved event index}. During creation of ETS from complete event collections some detail information is lost. Note, one can not reconstruct the original event collections from ETS data structures.  

\label{ext.fig.ETS_spikes_and_ETS_PAIR_COMPARISON} 
\input{semanpix/ESIllustration/imageLS}

In case of high event density or high access-rates the series look more like discrete continuous time series. A threshold based transformation, as illustrated in figure \ref{fig.ESAlgorithmExplain}.a, can be applied to any time dependent function to create sparse event time series for faster processing. 

The values of such dense series can be filtered with simple filter role sets, e.g., if for a given pair of two consequent values the first one is below and the second one above a certain threshold $t_s$ one can interpret this as an event which happened at time $ t_1 + ( t_2 - t_1) / 2$. This filter approach is also used to define the inter-event times dependent on the threshold $q=t_s$ for RIS (see section \ref{sec.RIS}).  Instead of using a threshold one can interpret the change of the sign as an event as well. ES analysis can easily be applied to data series if time stamps are already given in form of indexes. Otherwise interpolation is required to define the time stamp of an event. A convenient format to represent a time is a numerical representation rather than a text representation of time and date, especially if computation on the time index or time stamp is required. The disadvantage of this approach is obvious. Validation of results always requires a transformation back into a human readable time representation. We found, that a reasonable way to handle the time stamps is working with a time offset, defined as the time since a well defined time $t_0$. An example is shown in figure \ref{fig.GoogleTrends1}. There we present the measured data with a numerical equidistant index, which represents the day since January the 1-st, 2004.

\subsubsection*{The Algorithm}
\label{sssec:ES_algorithm}

According to Quiroga \textit{et al.} $i$ and $j$ are two event time series with events occurring at times $t_l^i$ and $t_m^j$. $s^i$ and $s^j$ are the total event numbers in these series. The indexes $l$ and $m$ range from $1 ... s^i$ and 
$1 ... s^j$. In order to find a quantitative measure which describes, if two series are synchronous based on co-occurrence of events we calculate how often an event in time series $i$ precedes an event in time series $j$, or how often an event in time series $j$ precedes an event in time series $i$.

\label{ext.fig.ESAlgorithmExplain} 
\input{semanpix/ESAlgorithmExplain/imageLS}

A time lag $\tau_{lm}^{ij}$ is used to quantify closeness or co-occurrence of two events. Figure \ref{fig.ESAlgorithmExplain} illustrates the definition of $\tau_{lm}^{ij}$ which is calculated as:


\begin{equation}
\tau_{lm}^{ij}=\frac{1}{2}\min\left( t_{l+1}^i - t_l^i, t_l^i - t_{l-1}^i, t_{m+1}^j - t_m^j, t_m^j - t_{m-1}^j \right)
\label{eqn:time_lag}
\end{equation}

For different application contexts one can consider different definitions of $\tau$. Using a global time lag $\tau_g$ independent from both series is and alternative, suggested by Quiroga \textit{et al.} It can be defined as a minimum or average of all $\tau_{lm}^{ij}$, keeping in mind that the parameter is meant to avoid double-counting and thus should be sufficiently small.

Depending on the calculated difference between $t_l^i$ and $t_m^j$, especially if it is less than $\tau_{lm}^{ij}$, both events are considered synchronous. We calculate the contributions for all event pairs $J_{lm}^{ij}$ as: 

\begin{equation}
J_{lm}^{ij}= \left\lbrace
\begin{matrix}
1 & \text{if} & 0 < t_l^i-t_m^j < \tau_{lm}^{ij}\\
\frac{1}{2} & \text{if} & t_l^i-t_m^j = \tau_{lm}^{ij}\\
0 & \text{else}
\end{matrix}
\right.
\label{eqn}
\end{equation}

Because $t_l^i$ must occur before $t_m^j$ for $J_{lm}^{ij}>0$ we can conclude that $J_{lm}^{ij}+J_{lm}^{ji}$ must be 0 or 1. This takes into account that all event pairs can be either asynchronous or synchronous.\\

For all pairs their individual contribution to synchronicity or asynchronicity $J_{lm}^{ij}$ is aggregated as $c(i|j)$:

\begin{equation}
c(i|j) = \sum_{l=1}^{s^i}\sum_{m=1}^{s^j} J_{lm}^{ij}
\label{eqn}
\end{equation}

We repeat this procedure to calculate also all contributions $c(j|i)$ to cover both possible directions. 
The first result is $Q_{ij}$. It represents the strength of event synchronization and is calculated as the sum of $c(i|j) + c(j|i)$, while subtracting $c(i|j) - c(j|i)$ gives us the value $q_{ij}$, which represents the strength of the delay between both series:

\begin{subequations}
\label{eqn:ESresult}
\begin{align}
Q_{ij} = \frac{c(i|j)+c(j|i)}{\sqrt{s^i s^j}}\label{eqn:ES_str}\\
q_{ij} = \frac{c(i|j)-c(j|i)}{\sqrt{s^i s^j}}\label{eqn:ES_del}
\end{align}
\end{subequations}

To normalize the values we divide both by $\sqrt{s^i s^j}$, then the event synchronization strength $Q_{ij}$ ranges from $0$ to $1$ where the maximum can only be reached if $s^i=s^j$. The delay value ranges from $q_{ij}=-Q_{ij}$ to $q_{ij}=+Q_{ij}$. If $Q_{ij}=1$ both series are completely synchronized because for every event in time series $i$, there is a synchronous event in $j$. $Q_{ij}=0$ means both series are completely desynchronized. The delay describes which of the both series is leading, e.g., if $q_{ij}=-Q_{ij}$ for all pairs of synchronous events the events in the first time series ($i$) precede the events in series two ($j$). If $q_{ij}=+Q_{ij}$ the second series is leading, and in case of $q_{ij}=0$, none of the both can be considered to be the leading or influencing time series.

\subsubsection*{Identify Significant Event Synchronization}
In order to interpret calculated $Q$ value as a representation of a hidden functional link in a correlation network one has to distinguish functional synchronization from random synchronization. Two computational experiments helped us to study ES properties systematically. Both experiments are implemented in Hadoop.TS. 

% a) ESPropertiesChart1Tool
% b) ESPropertiesChart2Tool
%\label{ext.fig.ESIdenticalToIndependentEx1} 
%\input{semanpix/ESIdenticalToIndependentEx1/imageLS}

\label{ext.fig.ESIdenticalToIndependentEx2} 
\input{semanpix/ESIdenticalToIndependentEx2/imageLS}

As shown in figure \ref{fig.ESIdenticalToIndependentEx2}.a, even for random event series the event synchronization strength  $Q_{ij}$ (black crosses and black curve) is not equal to zero. A minimum synchronization strength, which depends on the relative shift and on the ratio of the event densities of both series was found. In case of a pair of ETS with equal density the synchronization strength is $Q_{ij} \approx 0.25$ even for really large relative shifts. Figure \ref{fig.ESIdenticalToIndependentEx2} shows simulation results and the exponential fit for $Q$ and $q$. The delay value disappears for higher shifts. In order to identify the direction of the delay value the relative shift has to be less than $0.5$. Furthermore we found the polynomial function $Q_{rand}$ to describe the dependency between random synchronization and the density ratio:

\begin{equation}
Q_{rand}=\frac{0.7}{\left(5+\max \left(x,\frac{1}{x}\right)\right)^{0.6}}
\label{eqn:cal_fit}
\end{equation}

\subsubsection*{Experiment 1 - Transition from identical to independent event time series}

First, event synchronization is calculated for pairs of identical time series with random events there one is shifted against the other by $\Delta t$. Figure \ref{fig.ESIdenticalToIndependentEx2}.a shows event synchronization strength $Q$ (black crosses) and delay $q$ for ETS pairs with equal density. The number of events per 8760 time steps (which corresponds to an hourly time resolution for one year) are in the range $z \rm \epsilon \lbrace 100, 200, 300, 400, 500 \rbrace $. $q$ values for positive relative shifts (green triangles) and negative relative shifts (blue circles) are monotonously functions. Both series have the same density. $Q$ reaches a maximum for $\Delta t = 0$ and a minimum of $Q_{min} = 0.25$ which seems to be independent from relative shift and density at higher relative shifts. We subtract this empirical saturation value and calculate the parameters for an exponential fit function as shown in figure \ref{fig.ESIdenticalToIndependentEx2}.a as a black line:
\begin{equation}
Q(t) - Q_{min} = a \cdot e^{- b \cdot \Delta t/T}  
\label{eqn:ES_shift_fit}
\end{equation}
where $a=0.75$, $b=4$, and $\Delta t/T$ is the relative time shift between the time series.

The exponential part of the fit-function can be explained as follows: We generate event time series of independent events. This means at each time an event can be registered with the same probability. Such a process is a Poisson process and the inter event times, which are distances between two events follow an decreasing exponential distribution. 
\begin{equation}
p_{poison}( t ) = \frac{1}{T}e^{- t /T}
\label{eq:expdist}
\end{equation}
where $T$ is the mean inter-event time and $t$ a particular inter-event time between consequent events.

Two events $(t_l^i,\;t_m^j)$ are synchronous with a resolution $\delta$ if delta is smaller than all four inter event times between two consequent events from within the time series for which an event was registered. Thus, four distances contribute to the value of $\tau$ according to Eq. (\ref{eqn:time_lag}). Because all four distances are independent from each other and from the time shift we can calculate the probability for a pair of events to be synchronized as the joint probability of having all inter event distances smaller than $\delta$. We define $\delta = |t_l^i-t_m^j|$ and write the probability $p_{sync}(\delta)$ for finding a synchronized event pair at distance $\delta$:
\begin{equation}
p_{sync}(\delta) = p_{poison}(\delta) ^ 4 = \left( \frac{1}{T}e^{-\delta/T} \right)^4
\end{equation}

Using this simplification we are able to explain the exponential part of the fit function Eq.  (\ref{eqn:ES_shift_fit}), but not the vertical offset. For large time shifts the synchronization of two initially synchronized events disappears but there can be a synchronization with a following event which causes a measurable synchronization strength even if initial synchronization is destroyed. 

The delay value for time shifts larger than the average inter-event time (in this case the relative shift is one) fluctuates around zero as shown in figure \ref{fig.ESIdenticalToIndependentEx2}.a. The randomly generated event time series are supposed to be statistically independent. 
%The inset in figure \ref{fig.ESIdenticalToIndependent}.a shows the simulation result in a log-log plot.
The green and blue curves in \ref{fig.ESIdenticalToIndependentEx2}.a represent the exponential fit function: $q(t)=e^{-b \cdot t/T}$ with $b=5$. This fit only describes the dependency between shift parameter and delay value for small shifts. The value for $b=5$ exponential function can result from overlapping the decay function for $Q$ and the relation between $q$ and $Q$. According to \cite{Boeker2012} for very low shifts, all pairs of events have the same direction of delay as long as they are still synchronized, thus $|q|$ is close to its maximum: $|q| \approx Q$. Then some pairs of events start to newly synchronize. These newly synchronized pairs' delay direction is opposed to the general direction, which is given by the shifting direction between the time series, so they decrease the result instead of increasing it like in the case of $Q$, leading to a fifth $e^{-t/T}$ factor which is related to $b=5$. Shift direction influences the value of $q$. With increasing relative shift $q$ increases  monotonously and for negative relative shifts $q$ decreases monotonously. Both can be distinguished from each other as long as the relative shift is less than $0.5$, indicated by the highlighted range in \ref{fig.ESIdenticalToIndependentEx2}.a. 

This experiment provides a first empirical calibration value, a threshold to distinguish non random event synchronization from non significant synchronization which is also detectable in pure random event series.
The second experiment taking into account that also different densities and density ratios have an influence on $Q_{rand}$ but not on $q_{rand}$.

\subsubsection*{Experiment 2 - Influence of density differences on ES}
In the second experiment we calculated ES for two independent time series with variable densities. We expect a symmetry around the density ratio 1. Time series have a length of 17520 (which is related to two years with hourly resolution). Event numbers for series $i$ are in the range $[10 ... 1000]$ for series $j$ the number of random events was in the range  $z \rm \epsilon \lbrace 10, 260, 510, 760, 1010 \rbrace $. Figure \ref{fig.ESIdenticalToIndependentEx3} shows the results for 1000 computations in a log-log plot together with the polynomial fit function for comparison. The smaller image inside figure \ref{fig.ESIdenticalToIndependentEx3} highlights minor deviations between the simulation results and the expected theoretical value, based on our empirical calibration function Eq. (\ref{eqn:cal_fit}).  

\label{ext.fig.ESIdenticalToIndependentEx3} 
\input{semanpix/ESIdenticalToIndependentEx3/imageLS}

Results from both experiments allow implementation of an automatic calibration method. We know the influence of density differences on synchronization strength. So we calculate a relative event synchronization for measured data, as the quotient of the calculated synchronization strength and the theoretically expected value which is calculated from density ration and the empirical calibration function Eq. (\ref{eqn:cal_fit}). We apply a threshold filter to identify relevant or significant links between randomly chosen pairs of Wikipedia pages. During network construction procedure we use only those pairs for which the calibrated result $Q_{cal}=\frac{Q}{Q_{rand}}\geq t_f$ where $t_f$ is a variable threshold parameter. 

Our procedure to reconstruct networks from event time series is based on work from Malik \textit{et al.} \cite{Malik2012} and will be explained in chapter \ref{NGFTS} in detail.

\clearpage
\newpage

\subsection{Mutual Information}
Nonlinear dependencies between time series $X(t)$ and $Y(t)$ - if such exist - can not be be identified by Pearson Correlation analysis. Such a dependency can be found in the example illustrated in figure ... .

Kraskov formula: 

The concept of Mutual Information (MI) is based on the Entropy $H$ of a joint probability density $P(x,y)$ function and the individual distribution functions of the values $p(x)$ and $p(y)$. In case of existing non linear dependencies (see figure \ref{fig.NONLINEARDEP}) the Pearson Correlation would indicate no correlation or a strongly decreased correlation value, especially for long time series.

The strong prerequisites of Pearson Correlation (bi-variate normal distributed values) is not required for Mutual Information. Instead, for each pair of time series the values are binned into $B$ ranges.

Mutual Information can be applied as a sliding  window technique. It gives value between 0, for independent time series and $log B$ for time series which completely share all information (or between 0 and 1 in case of normalized Mutual Information). MI for a particular time window contains no information about a direction or a delay. 

A different approach is called Transfer Entropy. In this case, the history of a signal is taken into account. 

Conditional Mutual Information allows to measure the influence of third element on the relation between two elements. This is related to the procedure of ... as published by Kenett et al. (...) and described in chapter \label{adopt.ICF}.  

\label{ext.fig.NONLINEARDEP} 
\input{semanpix/NONLINEARDEP/imageLS}



%\subsection{Multivariate Granger Causality}
%\url{http://www.sussex.ac.uk/Users/lionelb/MVGC/docs/mvgc_preprint.pdf}
%\textbf{Explain what it is ...}
%\textbf{Compare GGC with MI according to JIDT paper ...}
%
%Granger Causality (GC) was introduced to analyze causal relations between groups of elements from complex system for which time series data is available. Barnett and Seth \cite{Barnett2014} developed a multivariate variant of GC. Our reconstruction methods of functional networks use bi-variate measures as well as multivariate measures. 
%
%Based on the concept of local neighborhood (see chapter \ref{chap.DEFINETHESCOPE}) we can define groups of nodes from structural information about the system. Even in arbitrary networks we can measure modularity and assign cluster ids to nodes to group the nodes time series data. This kind of pre aggregation either reduce the number of time series or generates multivariate Process which can, according to \cite{Barnett2014}, be seen as the multivariate source, target and conditioning variables X, Y, Z. This variables represent groups of elements now where group definition is based on existing structural data. Using “multivariate” G-causality one can measure causal relations between groups of elements for comparison structural relations, defined by the underlying structural information for which the clustering was applied. G-causality is thus able to account for group interactions. Especially in the context of coupled processes the overall function has to be cooperatively but it can also be seen competitively. It is essential for systems which consist of interacting subsystems. This kind of complex behavior can usually not be understood by traditional bivariate analysis \textbf{(Ladroue et al., 2009; Barrett et al., 2010)}.
 
\subsection{Comparison of Methods}

% use the EntropyTest class in Hadoop.TS2

What comes in the toolbox? \textit{JIDT: An information-theoretic toolkit for studying the dynamics of complex systems} \cite{Lizier2014} ... show simple application of the three measures.


The following figure shows some artificial time series pairs which illustrate typical patterns found in our data set. A shifted daily cycle which represents a typical day night cycle is shown in (a). The same pattern is shown in (b) but some noise of a small amplitude was added. The same series as in (a) but overlaid by increasing linear trends are shown in (c). Finally, the influence of pairs of strong peaks on top of rather random data is shown in (c). 

\url{http://www.sussex.ac.uk/Users/lionelb/MVGC/docs/mvgc_preprint.pdf}
The MVGC Multivariate Granger Causality Toolbox:
A New Approach to Granger-causal Inference

\textbf{BENCHMARK for some example series.}


\textbf{\textit{TODO:}} Create the series and plot all four pairs.


\textbf{\textit{TODO:}} Calculate CC, TE, MI and GC for all 4 pairs and show a table with results. 

USE THE: ",EntropieTest"







\textbf{\textit{TODO:}} Show the measures and discuss the differences.
 
\begin{figure}[ht!]
\centering
	\begin{minipage}[t]{0.45\textwidth}
		\begin{figure}[H]
		\includegraphics[angle=270,width=\textwidth]{tmp/sinus_009_001.eps}
		\caption*{$\varphi(\tau=0) = 1.000$}
		\end{figure}
	\end{minipage}
	\begin{minipage}[t]{0.45\textwidth}
		\begin{figure}[H]
		\includegraphics[angle=270,width=\textwidth]{tmp/sinus_009_007.eps}
		\caption*{$\varphi(\tau=0) = 0.946$}
		\end{figure}
	\end{minipage}	


	\begin{minipage}[t]{0.45\textwidth}
		\begin{figure}[H]
		\includegraphics[angle=270,width=\textwidth]{tmp/sinus_009_006.eps}
		\caption*{$\varphi(\tau=0) = 0.320$}
		\end{figure}
	\end{minipage}	
	\begin{minipage}[t]{0.45\textwidth}
		\begin{figure}[H]
		\includegraphics[angle=270,width=\textwidth]{tmp/sinus_009_002.eps}
		\caption*{$\varphi(\tau=0) = -1.000$}
		\end{figure}
	\end{minipage}	
	
\caption{\textbf{Comparison of uni-variate Measures using selected time series pairs.} Pearson-Correlation, Event-Synchronization, Mutual-Information, and Granger-Causality analysis are common techniques for information flow and dependency analysis. Typical examples are presented to illustrate how the methods can be used complementary. Strong peaks and long tailed distributions of the raw values influence the Pearson-Correlation. A simple transformation allows usage of the far more efficient Event-Synchronization procedure. Mutual-Information and Granger-Causality anaylsis provide also information about the direction of the dependency between two time series.}

\end{figure}

\clearpage

\section{Statistical Tests}
% https://explorable.com/significance-test
Using statistical tests we determine whether a null hypothesis (H0) can be rejected, in favor of the alternative hypothesis (H1). By comparing observed values with theoretically expected values such test statistic is calculated and used for a conclusion based on known statistical properties. Such a test can tell us, if a relationship between observed variable and a theoretical system (one sided test) or even between two observed variables (two sample test) exist, or whether pure chance could have produced the same observed results. Such tests can have strict prerequisites. According to Rochon \textit{et al.} \cite{Rochon} the t test assumes that the two samples arise from the same normally distributed population with unknown variance. In order to assure that the normality assumption holds, a preliminary Shapiro-Wilk test is applied often. 

%To test or not to test: Preliminary assessment of normality when comparing two independent samples
%Justine Rochon*, Matthias Gondan and Meinhard Kieser

Typical acceptance levels or significance levels are the 95$\%$ level, and the 99$\%$ significance level, depending on the domain. The significance level is related to the accuracy level. In general a hypothesis is never directly accepted, but rather the Null-Hypothesis is rejected. 

\subsection{Shapiro-Wilk Test}
%http://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test

A prerequisite for application of Pearson-Correlation analysis is, that values come from a Gaussian distribution. One way to verify this property for measured data is to apply the Shapiro-Wilk (SW) test. The SW test works well also for small samples but is limited to normal or Gaussian distributions. 

Figure \ref{fig.ShapiroWilkResults} shows the distribution of the p-values for financial time series. We used time series from two different stock indices with variable window lengths in for cross-domain analysis. The procedure is described in chapter \ref{CASEIII}. The top row is for data from the German stock index DAX and the bottom row for S\& P500. The labels are trading volume (tv), log return of prices (lrp), and absolute log return of prices (abs lrp). Different colors represent different window lengths. As already mentioned earlier, if the logarithm of the distances values is used, the distribution of the resulting values is in the most cases a Gaussian distribution with a confidence level higher than $80\%$.

\label{ext.fig.ShapiroWilkResults} 
\input{semanpix/ShapiroWilkResults/imageLS}

\label{ext.fig.ShapiroWilkResults} 
\input{semanpix/SWExperiment/imageLS}

In section \ref{SWTestInfluence} (see fig. \ref{fig.SWTestInfluence}) we analyse the impact of a preselection filter based on the p-value from SW test on obtained results.

The book "Time Series Analysis" by Henrik Madsen \cite{bookMadsen} suggests the following approach (see p. 152) :
\begin{enumerate}
\item Determine a model $\phi( B )X_t=\theta( B )\epsilon_t$ for the $\lbrace X_t \rbrace$ time series so that $\lbrace \epsilon_t \rbrace$ is white noise.
\item Based on the model perform a "pre-whitening" on $\lbrace X_t \rbrace$ given by the sequence of residuals: $\epsilon_t = \theta^{-1}(B)\phi(B)Y_t$.
\item Perform a similar "pre-whitening" on $\lbrace Y_t \rbrace$ given by: $\omega_t = \theta^{-1}(B)\phi(B)Y_t$.
\item Now, analyze the cross-correlation $\rho_{\epsilon \omega}(k)$ instead of $\rho_{XY}(k)$
\end{enumerate}
Since both time series X and Y can lead to different model which are used for pre-whitening, the operation is not symmetric any more. We used a slightly different approach and did a per node specific pre-whitening by removing the weekly trend obtained individually for each time series. Even if the weekly effects are different, we can expect that the resulting residual are more of a white noise. 

 
%Now we investigate the influence of individual peaks on the test statistic.
%
%DESCRIBE THE NUMERICAL TEST a bit more clearly ...
%
%\label{ext.fig.ShapiroWilkTestForPeaks} 
%\input{semanpix/ShapiroWilkTestForPeaks/imageLS}



%%%
%\label{ext.fig.ShapiroWikiTimeSeries} 
%\input{semanpix/ShapiroWikiTimeSeries/imageLS}

%%%
%\label{ext.fig.FinanceDataExamples} 
%\input{semanpix/FinanceDataExamples/imageLS}


 



\subsection{Kolmogorov-Smirnov Test}
\label{KSTEST}

% Journal of Histochemistry & Cytochemistry
% Proof without prejudice: use of KS test ...

\cite{Young77}

Comparison of probability distributions is an essential method to proof statistical significance. In our context, we use the test to identify if a system stays in stable state or if the system properties changed over time. In case of stable values, where no systematic drift or sporadic jumps but some small fluctuations exist, we expect that the distributions of the measured properties are also stable over time. If the measured samples for two time steps come not from the same distribution we can conclude, that the system has changed during this time. Even if a sequence of such pairwise comparisons shows no differences for individual steps, it is required to verify the properties on different time scales. With this hierarchical test it is possible to identify an appropriate time scale. As long as the distributions do not change for two consecutive steps, we can say, that the process is a kind of a \textit{quasistatic} process. Although, such a a quasistatic process doesn't exist - we can see such a system as an example in which the changes are slow enough. In thermodynamics such a process has to be "infinitely slowly". Based on the series of KS-test we can identify the appropriate time scale for which the approximation is valid.

In order to identify a stationary or stable behavior of a system during a period $T$ of length $dt$ we compare the distribution of values of a measured property such as the correlation link strength between pairs of elements per time interval. Values can be measured directly or calculated from other measures via a link creation functions (LCF) as explained in chapter \ref{LCF} (see table \ref{tab11.3}). 

Since we don't know the real distribution of the values of this property upfront and the shape of the distribution may vary in time we need a non-parametric test, to identify if the values obtained in two consequent time windows belong to the same or a different population. The Kologorow-Smirnow test is thus an essential component of our methodology. 

The test is based on the null hypothesis:
\begin{equation}
H_0 \colon F_{X_{t}}(x) = F_{X_{t + dt}}(x)
\end{equation}
which means, the distribution of the measured property during both consequent measurements is equal and the alternative hypothesis:
\begin{equation}
H_0 \colon F_{X_{t}}(x) \neq F_{X_{t + dt}}(x)
\end{equation}
which means, that the distribution function for measured values at two different times differs significantly. The KS-test compares two empirical distribution functions $F_{X_{t}}(x)$ with $F_{X_{t + dt}}(x)$, based on a test statistic:
\begin{equation}
d_n=\| F_n-F_0 \| = \sup_x | F_n(x) - F_0(x) | 
\end{equation}.

In practice, the statistic requires a relatively large number of data points to properly reject the null hypothesis.

\subsubsection*{Comparison of Three Implementations of a Two-Sample Kolmogorov-Smirnov Test}

Statistical data analysis is done with a variety of commercial tools such as SAS, SPSS, Matlab, ORIGIN among many others. No matter if such a commercial tool, a self made implementation, or existing open source software is used, one should always cross-validate results from different programs.

In this work we used an implementation of the two-sample Kolmogorow-Smirnow test as published on page 620 in the book “\textit{Numerical Recipies in C}” \cite{NRC} denoted as $a_3$. As reference implementation we use the \textit{ks.test} routine from the \textit{R statistics package} \cite{RstatisticalSW} denoted as $a_2$, and the open source Java library \textit{Apache Commons Math} (version 3) \cite{ACM3} denoted as $a_1$. 

The null hypothesis of the KS-test is that both groups were sampled from populations with identical distributions. In case we reject that null hypothesis the two gaussian distributions have different mean value, different variances, or entirely different distributions. 

To conduct the numerical simulations we generate random data sets. Each data set consists of two series of random numbers with a sample size $z=1000$ from a Gaussian distribution $P_G$ with $\mu=0.0$ and $\sigma=2.5$. 

\subsubsection*{Interpretation of Results Obtained From Parameter Variation}
Since the KS-test is a non-parametric test and does not provide a confidence interval for we have to find identify boundaries by numerical simulations. We vary the parameters of one of the two samples systematically and plot the resulting p-values over the the variation value.

\label{ext.fig.KSVariation} 
\input{semanpix/KSVariation/imageLS}

By parameter variation as shown in figure \ref{fig.KSVariation} we can illustrate that small deviations (for $\Delta \mu < 0.5$ and $\Delta \sigma < 0.9$) can not be identified by the KS-Test. In other words, if the KS-test indicates no significant difference between the two distributions, the mean can still differ by 0.5 and sigma by 0.9 respectively for distributions with our selected parameters. This interpretation is specific to the selected reference distribution, $P_G$ in this case. 

In order to estimate such boundaries for an arbitrary use case with real distributions obtained from a real data set, an individual calibration based on parameter variation should be considered. If the real distribution function can not be fitted easily, it is still possible to conduct such a calibration. First, one adds some random noise to one of the both data series for comparison. This means, the reference series is taken as obtained from the real data set and the second sample is generated by a step wise randomization. To each real value a random value from a Gaussian distribution for which $\mu$ and $\sigma$ are varied is added. This allows to identify how much noise would still be accepted even if the KS-test results indicates, that both series are from the same distribution.   

We can not reject H0, if the p-value $p > 0.001$, which means, we can conclude that  $X_1 = X_2$ with a confidence of 1-p $\%$.

According to the book "Nonparametrics: Statistical methods based on ranks" by Lehmann \cite{bookLehmann} the KS-test tests for more deviations from the null hypothesis than the Mann-Whitney test. It has more power to detect changes in the shape of the distributions (see Lehmann, page 39) but less power to detect a shift in the median. Since the distributions we usually found in our link strength data set are not Gaussian distributions, we are primarily not interested in variation of mean values, but rather more interested in identifying different shapes of distributions. Because of this we prefer the KS-test over the Mann-Whitney test. 



\newpage

\section{Surrogate Data for Functional Tests and Significance Tests}

Surrogate data can be entirely artificial or it can be derived from real data. Usually, randomized data series are created based on specific assumptions by transformations which change specific properties of the data only in a way, which allows a systematic comparison of analysis methods applied to the surrogate data and the real data. Thus, surrogate data is typically used for cross-validation and plausibility tests in addition to analysis of measured real world data. 

In this work we use two different types of surrogate data. The first type is based on random numbers or semi random number generators (RNG). The RNG reproduces properties of the time series obtained from a real world system. In order to be compatible with the real data, one has to assure, that surrogate data shows the same properties as the real data. For this purpose one has to measure characteristic properties using univariate analysis or calibration procedures. Relevant properties are mean value, variance, distribution of values, auto-correlation and long-term correlations, as well as fractality. In general a model of the required time series is useful but since time series modeling is not trivial we prefer a simpler approach, based on measured data.

A second type of surrogate data is generated directly from real data by randomization, either by shuffling the values or even by modification of the Fourier Transformation of the original time series. Simple shuffling is usually repeated in multiple rounds. The assumption is hereby, that shuffling destroys all kinds of correlations but the distribution of values, mean, and variance are conserved. This way, a comparison of analysis results from real data with results from shuffled surrogate data allows identification of non-random effects in measured data sets which are caused by auto-correlation within the time series. 

\subsection{Influencing Properties of Sets of Random Numbers}

%
% Section gives an overview on properties, creation methods, and influence
% of shuffeling
%

A set of random numbers can be used as surrogate data for statistical significance tests. It must have well defined and well known properties. Important properties include
the parameters which describe the distribution of the values, and the correlation properties. This  means, short term correlation measured by the auto-correlation function as well as long term
correlations, which are detected via Detrended Fluctuation Analysis,  have to be produced by the random series generator.

Random numbers are produced or generated differently for several applications. Sometimes one wants real randomness, e.g., for application in cryptography. Pseudo random numbers are useful for engineering since they can be reproduced if the same conditions exist, e.g., if the same seed value is used together with the exact same implementation on the same platform.

First, a random number generator has to create a large number of uncorrelated
uniformly distributed scalar random values which is not trivial. How random the numbers really are, depends on several properties. In order to reflect fairness in a game with fair dices, one needs evenly distributed integer values, in the range 1 to 6. This means, the probability of each of the six outcomes is equal and $\frac{1}{6}$. Auto-correlations should not exist, because in the presence of auto-correlation a higher probability for a particular number exists, depending on the history. Uncorrelated values are are not influenced by the history.  

Especially if large systems should be simulated over a long time period one has to use a random number generator with a long period instead of traditional random number generator which is built in into every computer system or programming language. In 1998 Matsumoto and Nashimura \cite{Matsumoto1998} published the first random generator with a period ($2^{19937}-1$) which is longer than the estimated number of electron spin changes since the beginning of the Universe ($10^{6000}$ vs $10^{120}$) \cite{Rrandtoolbox}.  

Since surrogate data has to be representative for a specific process it has to reproduce typical properties although the series should be random and not determinisitc. This allows variation of the property in the 
surrogate data series which is assumed to be the influencing factor regarding some special property in the research focus.

Uniformly distributed random numbers are used to generate new samples of uncorrelated random
numbers with any possible distribution (see fig. \ref{fig:rand_num_dists}) using the inverse transform sampling method. Shaping the distribution of the data set influences the average value and
the number of values within a given range but this procedure has no influence on
correlation properties. Therefore the frequency spectrum has to be manipulated.

A manipulation of the Fourier coefficients introduces auto-correlations. Details about the Fourier Filtering Method (FFM) are explained in section \ref{FFM}.

The following methods are typical means to modify properties of random number series:
(1) Random shuffling destroys correlations while the distribution of values (the shape of the PDF), mean and variance are not changed. 
(2) Phase-randomization creates auto-correlation and changes the distribution, mean and variance slightly. 

%Podobnik, B., Fu, D. F., Stanley, H. E., & Ivanov, P. C. (2007). Power-law autocorrelated stochastic processes with long-range cross-correlations. The European Physical Journal B, 56(1), 47-52.:
%\textbf{ABSTRACT}
%We develop a stochastic process with two coupled variables where the absolute values of each variable exhibit long-range power-law autocorrelations and are also long-range cross-correlated. We investigate how the scaling exponents characterizing power-law autocorrelation and long-range cross-correlation behavior in the absolute values of the generated variables depend on the two parameters in our model. In particular, if the autocorrelation is stronger, the cross-correlation is also stronger. We test the utility of our approach by comparing the autocorrelation and cross-correlation properties of the time series generated by our model with data on daily returns over ten years for two major financial indices, the Dow Jones and the S&P500, and on daily returns of two well-known company stocks, IBM and Microsoft, over five years.


% Theiler, J., Eubank, S., Longtin, A., Galdrikian, B., & Doyne Farmer, J. (1992). Testing for nonlinearity in time series: the method of surrogate data. Physica D: Nonlinear Phenomena, 58(1), 77-94. 
Theiler et al. \cite{Teiler1992} tested for nonlinearity in time series using surrogate data. They specify a linear process as null hypothesis first. 
Using the phase-randomization method they generate surrogate data series. Since this series are consistent with the null hypothesis
they calculate a discriminating statistic for the original series and for all surrogate data series. They rejected the null hypothesis - and detect nonlinearity in the data series - if the calculated values for the original and the surrogate data set are significantly different.

Figure \ref{fig:rand_num_dists}.a shows PDF for four random number series (a,b,c,d) generated by ... \cite{ACM3}. Series e was modified by our implementation of the Fourier filter method \cite{Hadoop.TS}. The distribution of values has also changed (compared to a). The important difference is the fluctuation coefficient which indicates long range correlations if $\alpha > 0.5$ as shown in figure ... . 

Random long-term correlated random numbers are used in this work for functional testing and validation of the DFA implementation. 

%
% TODO Zeige verschiedene Verteilungen und dazu jeweils die DFA Resultate
% die FFT und ggf. die MFDFA-Resultate
%
%  Vorlage fuer das BILD: Hadoop.TS paper fig.5
%
\begin{figure}[htp]
 \centering
 \centering
\includegraphics[width=2.8in]{semanpix/FFMandPR/Image5aM.eps}
\includegraphics[width=2.8in]{semanpix/RandomNumbers/Exp5PDF.png}
\caption{(a) Probability density functions (PDF) for sample data sets from random number generators /RNG) implemented in the open source software Apache Commons Math \cite{ACM3} and Hadoop.TS \cite{Hadoop.TS}. (b) Using FFM also the distribution of values is influenced (red and orange, see also green and red in (a) ) but shuffling has no influence on the distribution - but in case a sliding window technique is applied afterwards the distribution of values per time window is influenced also by simple shuffling, especially if time series are non stationary.} 
 \label{fig:rand_num_dists}
\end{figure}


%http://www.macalester.edu/~kaplan/knoxville/PD.pdf
%Physica D 142 (2000) 346–382
%Review Paper
%Surrogate time series
%Thomas Schreiber∗, Andreas Schmitz


%OK [31] T. Schreiber, A. Schmitz, Improved surrogate data for nonlinearity tests, Phys. Rev. Lett. 77 (1996) 635.
\textbf{Add some word to SCHREIBER SCHMITZ APPROACH ...}

\url{http://www.macalester.edu/~kaplan/knoxville/PD.pdf}

In \cite{Schreiber1996} Schreiber and Schmitz proposed a new iterative  method to correct deviations in spectrum and distribution, compared to the goal set by the measured data. The surrogate data is filtered towards the correct Fourier amplitudes
and rank-ordered afterwards to the correct distribution in an alternating iterative approach which stops after a finite number io steps. The remaining difference can be interpreted as the accurcy of the method.




%
% TODO Zeige verschiedene Verteilungen und dazu jeweils die DFA Resultate
% die FFT und ggf. die MFDFA-Resultate
%
%  Vorlage fuer das BILD: Hadoop.TS paper fig.5
%
\begin{figure}[htp]
 \centering
 \centering
\includegraphics[width=2.8in]{semanpix/RandomNumbers/Exp5NOISE.png}
\includegraphics[width=2.8in]{semanpix/RandomNumbers/Exp5DFA.png}
\caption{Referenz time series (black) with ... in comparison with randomized surrogat data derived from reference series (shuffled - blue, FFM.a - red, FFM.b - yellow} 

 \label{fig:rand_num_dists}
\end{figure}



 
 


\subsection{Generation of Long-Term Correlated Random Numbers}
\label{FFM} 

%---------------------------------------------------------------------------------------
% " Hadoop.TS 2 " (MacBOOK PRO)
%
% statphys.detrending.LongTermCorrelationSeriesGenerator
%---------------------------------------------------------------------------------------

The Fourier filtering method (FFM) is a very common method for generation of sequences of random numbers with power-law correlations ( \cite{PhysRevA.46.R1724} \textbf{CITE 2 , 4}  - CITATION INCOMPLETE).

%@2 D. Saupe, in The Science of Fractal Images, edited by H.-O. Peitgen and D. Saupe ~Springer, New York, 1988!; J. Feder, Fractals ~Plen	um Press, New York, 1988!.
%@4# C.-K. Peng et al., Phys. Rev. A 44, 2239 ~1991!.
%@5# S. Prakash et al., Phys. Rev. A 46, R1724 ~1992!. (PhysRevA.46.R1724)

According to Makse \textit{et al.} \cite{Makse1995} this method has the disadvantage of
presenting a finite cutoff in the range over which the variables
are actually correlated. Thus the FFM is not suitable for the study of scaling properties in the limit of
large systems.

A modified Fourier filtering method was published by Makse \textit{et al.} in 1995. Due to the modification, the cutoff in
the range of correlations is removed and the actual correlations extend to the whole system in the modified method \cite{Makse1995}. We implemented the FFM approach in the HadoopTS software package. 
% The tool to create the charts is "EntropyTest".

%TODO Beschreibe die Zufallszahlen-Entstehung

% q ist in meinem Programm f
% gamma-1 = -beta



\begin{lstlisting}

        FastFourierTransformer fft = new FastFourierTransformer();

        Complex[] c = fft.transform(data);
        Complex[] mod = new Complex[c.length];

        double N = c.length;

        double c0_R = c[0].getReal();
        double c0_I = c[0].getImaginary();

        double cf_R = c[(data.length - 1)].getReal();
        double cf_I = c[(data.length - 1)].getImaginary();

//        double fmin = 0;
//        double fmax = 0.5D * samplingRate;
//
//        double df = fmax / ( N * 0.5 );
        
        
        double f0 = c0_R;
        double df = N / 6283.1853071795858D;
        
        for (int i = 1; i < c.length; i++) {
            double f = i * df;
            double faktor = Math.pow(f, -0.5D * beta);
            mod[i] = c[i].multiply(faktor);
        }

        mod[0] = new Complex(0.0D, 0.0D);
        
    \end{lstlisting}



\begin{figure}[htp]
 \centering
 \includegraphics[width=0.485\textwidth]{semanpix/RandomNumbers/DFAforFFMseries.png}
 \includegraphics[width=0.485\textwidth]{semanpix/RandomNumbers/alphaVsBeta.png}
 \caption[Fluctuation functions $F(s)$ for surrogate data generated by phase randomization (FFM) and relation between fluctuation coefficient $\alpha$ and control parameter $\beta$]{Fluctuation functions for several series of random numbers, created with the Fourier-Filter-Method (FFM) are shown in a). The fluctuation coefficient alpha was calculated by a linear fit in the log-log plot of the fluctuation function in the range 0.8 < log(s) < 2.5 and shown in b) as the blue curve. The black curve shows the theoretical relation between $\alpha=\frac{1+\beta}{2}$. The blue curve shows the average from 100 iterations and the dashed curves illustrate the $\pm \sigma$ band.}
 \label{fig:beta_alpha_random_gen}
\end{figure}

%For very small and larger values of $\beta$ the calculated value differs from the theoretical assumed value, so $\beta$ should be in the range of $ 0 < \beta < 1$. 
% SEEMS to be wrong now


%TODO ref 2,4,5 in PRE 53 5, Mai 1996 






\textbf{TODO }
\begin{itemize}
\item Rebuild (a) and (b) in Origin.
\item nur relevante Funktionen (für in b) gezeigten Bereich
\item einige auswählen und hervorheben
\item Fit-Bereich hervorheben
\end{itemize}


\begin{equation}
 S(q) = \langle \eta_q \eta_{-q} \rangle \sim q^{\gamma-1} 
\end{equation}  



\nomenclature[R$Q$]{$Q$}{Der Q-Wert}
\nomenclature[R$Q$]{$q$}{der q-Wert}
%
% This was the test for working with the command:
%
%     \nomenclature{etc.}{et cetera}
%
% Die Abkürzung etc.\nomenclature{etc.}{et cetera} steht
% im Abkürzungsverzeichnis.

%\subsection{Creation of Surrogate Data for Pearson Correlation and Event Synchronization }

%To judge the relevance and significance of ES results, we introduce a surrogate data test. This is an additional calculation designed to provide a value to compare the real results to.

%To perform the surrogate data test for a given pair of time series, we shift one of the time series by half of the total time (i.e. 20 weeks, as the dataset comprises 40 weeks), then restore time congruence by adding the `free' first half of the other time series to its end (Fig. \ref{img:sdt_explain}). This way every point has a distance of 20 weeks to its former position relative to the respective other time series. The surrogate data test result is the strength of ES of these shifted time series. The shift is supposed to eliminate dependencies between the time series, so the test results can be expected to be equal to the calibration values because in both cases we calculate ES for a pair of independent time series with defined densities.

% Figure \ref{img:sdt_vs_cal} negates this expectation. The surrogate data test results are not equal but actually much lower than the calibration values. This is the consequence of a misconception: Assuming equal results also means assuming similar time series. Remembering the generating of random time series, every point of time had the same probability of featuring an event, which led to a Poisson process and exponential distribution of the event distances (page \pageref{eq:expdist}). In reality, events are not equally likely to happen at any time. Our time series are Wikipedia edit times. Explanations for temporarily changing distributions are weekly trends [{\sc Yasseri}2012a] or bursts of many events during short time intervals due to external events as investigated in [{\sc Kämpf}2012] or editing responses (\emph{reverts}) up to `edit wars' [{\sc Sumi}2011, {\sc Yasseri}2012b].

%We have seen that the surrogate data test does not give the same results as the calculation with shuffled data (calibration). Though it was not expected this way, it gives us additional information, if interpreted accordingly. Also note that the dependence on density ratio is very similar for both curves in Figure \ref{img:sdt_vs_cal}. At this point we are able to calculate ES, calibrate it using random data, and add an interpretation using the surrogate data test, which is enough to proceed to looking at real data.

%\begin{figure}[!ht]
%\centering
%\includegraphics[scale=1.0]{tmp/sdt_vs_thr_binned}
%\caption{Surrogate data test in comparison to the calibration curve after logarithmic binning. The surrogate data test unexpectedly gives significantly lower results, uncovering aberrations from a time-independent distribution used in time series generation for the calibration.}
%\label{img:sdt_vs_cal}
%\end{figure}

\newpage
\clearpage

\part{Method Development and Enhancement}
Improvements on existent methods and the description of new methods are explained in this part. Data sets are primarily selected if they are useful for illustrative purposes. Application of the new methods to other scientific problems are presented in the next part and in Appendix \textbf{( ...???... )} 
%ETOSHA.LINK  Which APENDIX IS IT? SET LINK ... 
\chapter*{Overview}

Graphs and networks are common representations of complex systems. Statistical properties of an ensemble of nodes are calculated from individual properties of many network nodes. Structural graph properties are a special kind of properties of the ensemble and can not be measured or calculated from individual elements. Such structural properties can be presented as as individual nodes properties, such as node degree or  membership in a clique or cluster - but in order to get values for it a graph analysis procedure has to be applied. A graph can be an individual entity or object, consisting of nodes with time dependent properties which for itself define time dependent link properties. Individual link groups describe special aspects or real world phenomenon. 

An important task in complex systems research is modeling of dynamic properties. 
Our approach is based on the correlation between node properties, network structure and link properties. Not all properties can be measured at once nor directly. Technical and economical limitations are addressed by recent technological improvements. Inexpensive distributed storage, linked data sets, and massive parallel processing provide the technical frameworks for next generation network analysis.  

When this work was started in 2009, there was no software product available for automatic extraction and detection of structural links and functional links between entities from segregated data sets collected from several domains. Several analysis techniques had been developed and proposed and several routines were available for commercial software packages like Comsol, Matlab, and Mathematica, or for open source software such as Python, R, and OpenDX. 

Properties of network nodes, especially time resolved properties captured in time series are used to calculate correlations between such nodes. The purpose of such correlation analysis is creation or detection of hidden links between nodes. Such links can be both, cause and result of interactions between nodes. One way to model such processes is to form layers, one layer per process. During this step, a decomposition is done. One may loose information during this step, but for simplification it is necessary. Later on, after, the correlation networks are known, a combination of several layers is used to draw a complex network again. One has to decide if a horizontal or a vertical cut should be applied. As horizontal cut we consider creation of layers which consist of links of the same type. Vertical cuts contain different link types around a well defined, usually small number of nodes. A comparison of both obtained networks shows, if the decomposition has a strong influence or if it could even be neglected. Structure-induces stress (SIS) is a new approach to quantify the impact of functional networks on the underlying network structure or the over way around.  

Network links can directly be extracted from existing data sets, if such links already exist. Implicit links are often described as hyperlinks in HTML documents, citations in research articles or books, or even semantic annotations, given in any of the many new formats such as Micro-formats \cite{MICROFORMATS} or JSON-LD \cite{JSON.LD}. Furthermore, co-authorship or co-occurrence of terms define network links between documents, books, or articles. Even grammatical structures of human language are useful sources for extraction of structural information \cite{Amanico2012} and similarity of texts.

A second type of links has to be reconstructed from node properties, if such are available as time series data. Because the connection is not implicit one has to start with an assumption or a hypothesis which describes why a correlation between both entities exists and how this dependency can be expressed by a function and thus how data analysis can reveal this hidden link. An explicit connection between two points can be defined by an external system which has an influence on both, either instantaneous or even with a given time delay. 

A very simple approach is the calculation of a correlation matrix for all possible pairs of network nodes within an ensemble. The '\textit{Pearson correlation}' is a common technique, but it can not be used in the case of sparse time series or if the values in a time series are not from a Gaussian distribution. In such cases, the event-synchronisation is an alternative approach to describe, how well the behavior of two different elements can be compared or how similar they are.

In many cases it is very important to find an unknown time delay which is usually a result of damping in the coupling process. The elements of the correlation matrix will be correlation functions. Such a correlation function $\textrm F_{\rm C}(t_s,t_e,\tau)$ describes a property of a pair of elements at a given point in time as a function of a third parameter, the time delay $\tau$.
To find such an unknown time delay one tries to find a maximum in the average correlation function of all correlation functions in the correlation matrix. If such a maximum exists it might be interpreted as characteristic property of the underlying process.      

In many cases, one is interested in a comparison of the underlying structural network (the explicit network) and the functional network (the implicit one). In this case one has to solve another problem. The full system, e.g. all messages from all people using a particular communication service, such as E-Mail, Twitter, Facebook, or others is not available publicly. Even if accessing all that data would be allowed, the amount of data is to much and it is not possible to handle all that data with traditional techniques. 

In case of our Wikipedia studies, we would have to process the time series of about $8 \cdot 10^6$ pages. A time series bucket with access rate data on hourly resolution for one year would require about 261 GB. It is not required to have all that data in memory at once. Network links are calculated from time series pairs. But how much memory would be required to store the full correlation matrix with a delay of $\tau = \pm 14$, which is two weeks? In this case each link would require 116 byte. The un-directed network has around $32 \cdot 10^{12}$ links and requires $\approx 172$ PB to store the network dynamics on weekly resolution for a whole year.

Our approach aims on reducing the amount of data during individual analysis steps while comparability is always given. As long as the impact of a simplification can be estimated, one can decide if the introduced error is acceptable or not.

\chapter{Embedded Context Graphs}
\label{chap.DEFINETHESCOPE}
Decomposition of interconnected systems helps to simplify but at the same time it causes information loss. Researchers have to analyze carefully, how the system in the research focus is related to other systems and how internal components are related to or connected with each other. In the case of the World Wide Web, which is formed by hyper-linked pages, it was easy in the beginning. The number of such pages was still countable and maintainable, but soon the amount of content was too large. Nowadays several online systems compete. They offer comparable functionality to the same still growing audience. Studies on large social media applications reflect segregation effects as observed in social communities. Depending on age, location and the number of friends, which use a certain system already, the probability of becoming a member of an online community is influenced. The book \textit{Networks, Crowds, and Markets} by David Easley and Jon Kleinberg \cite{Easley2010} describes in chapter 4 the \textit{interplay between selection and social influence}. This concepts are applied to interacting social entities - to human beings, but they can be generalized to social content networks and hybrid systems as well. This way, the content, created by social communities reflects properties of the community and can be used as a stub for analysis. This is one of the main reasons for using Wikipedia data in this work.

\section{Define Subgraphs in Interconnected Networks}
How can we define the focus and the neighborhood of a complex social network? A simple approach is presented by Dorogovtsev et al. \cite{dorogovtsev2013evolution}.
Figure \ref{fig1.NeighborhoodNetworks}.a is an illustration taken from \cite{dorogovtsev2013evolution} (figure 6). It shows the embedding of nodes in a directed graph. In general, if edges are undirected, such a network consists of a giant weakly connected component (GWCC), which is also called \textit{percolating cluster} and several disconnected components (DC). In case of directed networks the GWCC consists of: (1) a giant strongly connected component (GSCC); (2) the giant out-component (GOUT); (3) the giant in-component (GIN); and (4) the tendrils (TE).

\begin{figure}[th!]
\includegraphics[width=0.52\textwidth]{semanpix/DefinitionOfLocalNetworks/sketch2.png}
\includegraphics[width=0.35\textwidth]{semanpix/DefinitionOfLocalNetworks/sketch3.png}
 %      \llap{  \parbox[b]{0.0in}{\Large \textsf a)\\\rule{0ex}{2.9in}  }}
 %     \llap{  \parbox[b]{0.0in}{\Large \textsf b)\\\rule{0ex}{1.8in}  }}
 %     \llap{  \parbox[b]{-5.0in}{\Large \textsf c)\\\rule{0ex}{2.9in}  }}
\caption{\textbf{Context and Neighborhood of a Semantic Concept.} A general description of components in a complete network (taken from \cite{dorogovtsev2013evolution}) is shown in (a). Because not all resources of large systems can be accessed at once (because of limitations or parallelized algorithms), a localized approach as presented in (b) is required. In (a) GSCC contains all nodes which are reachable from every node by a directed path. GOUT contains all nodes approachable from the GSCC. GIN contains all nodes from which the GSCC is approachable. GIN and GOUT include GSCC, and TE contains the rest of the GSCC, especially all the disconnected nodes which have no access to the GSCC and are not reachable from it. (b) Shows a systematic approach to define local networks which define an analysis scope. The central node CN is a single page about a company, or a project, or it is a “list-page” which bundles links to groups of pages, e.g. the page about a stock market index. CN has links to and from the local neighborhood (straight arrows) defined by pages in the same language (group LN). Inter-wiki links usually connect CN to many IWL pages in different languages (dash-dot arrows) covering the same topic or semantic concept. Those pages contain links to their local neighborhood in the same language (here not shown as arrows) which all contribute to the group GN. This way, all pages around all IWL pages define the global neighborhood GN. Some concepts are linked via redirect links or from so called disambiguation pages by specific links. The sketch shows those as CN' because of their strong relation to semantic core.}
     \label{fig1.NeighborhoodNetworks} 
     \label{fig.LocalNetworksDefineAnalysisScope} 
% https://docs.google.com/a/cloudera.com/document/d/1X6Ni3fREo-P_vxkSjpV6Bt7iyxWBDi-2hoE6RZVOtus/edit#
%\begin{figure}[h!]
%  \centering
%    \includegraphics[scale=0.5]{semanpix/DefinitionOfLocalNetworks/sketch.eps}     
\end{figure}

The GSCC is the set of nodes which is reachable from every node by a directed path.
GOUT is the set of nodes approachable from the GSCC by a directed path and includes
GSCC. GIN contains all nodes from which the GSCC is approachable and includes GSCC. 
TE form the rest of the GSCC. This are all the disconnected nodes which have no access to the GSCC and are not reachable from it. In this definition GSCC is the interception of GIN and GOUT. This allows a formal notation of the system $N$ as in\cite{dorogovtsev2013evolution}. We write:
\begin{equation}
N = GWCC + DC
\end{equation}
and
\begin{equation}
GWCC = GIN + GOUT - GSCC + TE.
\end{equation}

As of May 1999, the entire Web, containing $203 \cdot 10^6$ pages, consisted of the GWCC, $186 \cdot 10^6$ pages (91$\%$ of the total number of pages), and the DC, $17 \cdot 10^6$ pages. In turn, the GWCC included: the GSCC, $56 \cdot 10^6$ pages, the GIN, $99 \cdot 10^6$ pages, the GOUT, $99 \cdot 10^6$ pages, and the TE, $44 \cdot 10^6$ pages.

The size of the Internet, ten years later, in 2009, was estimated by \cite{Zillman2008} in  \textit{'The Deep Web'} report. It covers about 1 trillion pages of information located through the World Wide Web in various files and formats that the current search engines on the Internet either cannot find or have difficulty accessing. According to \cite{Zillman2008} search engines can find only about 20 billion pages at the this time (in 2008).

Nowadays, a definition of the size of the internet would even be harder. What is the "Internet"? Is it the set of machines in the background and the cables or radio links between them, or is it the interlinked content, or both? Is this already a comprehensive definition? The modern Internet is everywhere, as it functions as the base for communication which happens on top of the physical network using information entities like documents and messages. A new term is getting more and more attention: "The Internet of Things" (IoT). Historically, the ARPANET is the ancestor of the Internet, it represents the early version of the technical backbone. The World Wide Web (WWW) can be seen as a layer of structured content which was made available by connected servers. Social Networks, like Twitter, LinkedIn, Facebook, or LifeJournal evolved in the context of heavy social interaction between Internet users who like to share content also privately. Nowadays, the content of such communication is also persisted within the system and heavily interlinked like the static pages of the early WWW. The result is a hybrid of computer and communication networks, with dynamic content networks on top. New social communities emerge and wrap around the content and finally around the technical devices. Communication between people was initially more about real life aspects, but more and more also about existing digital content. Finally, the communication itself becomes part of the content as soon as messages are stored, shared and interlinked. Beside social communities, more and more additional devices, which are not part of the technical infrastructure of the internet, are added to the network. Such devices provide even more data and contribute to communication, as they can generate messages like simple delivery confirmation or even alerts in critical conditions. Such interdependent networks are already present everywhere, but their impact on society and individuals is not well understood at his point in time. 

Because over-simplified network representations are not appropriate for complex systems research we need a clear definition of context even if the networks are describing different domains.
 
\section{Definition of Neighborhood Networks}
\label{sec.contextgraph}
For practical reasons we have to develop an approach which allows us to collect a reasonable amount of data starting in a well defined environment - the scope or the focus of the study - and additionally including a representative amount of data from its neighborhood, even if this is incomplete. 
 
We have collected four exemplary data sets in order to develop our methodology
and to demonstrate and systematically evaluate its performance. These test data
sets consist of pre-processed log data and local network structures for several
Wikipedia articles. More details are shown in the data catalog in chapter \ref{chap.DATACATALOG} section \ref{chap.DATACAT.REL}.

The primary structure of the Wikipedia network was retrieved with the
Mediawiki-Explorer package, which uses a Java based implementation of the
Media-Wiki-API client \cite{WikiAPI}. 

All data sets include a few selected central nodes (CNs, see Table \ref{tab:1}) as well as 
their local and global contexts (see Fig.~ \ref{fig.PLOSONE}). The local context is defined by all articles (nodes) directly linked to the CN in the same Wikipedia, i.e. the same 
language version. This local neighborhood group of articles is denoted by LN. 
Each inter-wiki-link (IWL) connects the CN to a node addressing the same topic 
in another language (group IWL). The IWL group defines a global representation 
of the chosen semantic concept in all languages. We note that inter-wiki-links
are not necessarily bi-directional, so that a CN regarding the same topic in 
another language may have a slightly different IWL group. Our IWL groups are 
always defined by the inter-wiki-links of the CN.

Finally, all articles directly 
linked to articles in the IWL group form the global neighborhood, a group denoted 
by GN. This scheme, illustrated in Fig. \ref{fig.PLOSONE}, allows aggregation of data regarding
specific topics in any language. At the same time, it is possible to separate the 
whole content stored for a single topic (or term) for each individual language to 
enable a language dependent analysis. But, it is not important if some of the 
selected pages are also linked to each other within a group or across the group 
boundaries. 


Here, our interest is in the role of Wikipedia as a public crowd based source for news in the context of market activity, and we are especially focused on the readers side. The number of article downloads reflects the state of a larger part of the society which can hardly be influenced by a dominating opinion of a single publisher using a shiny picture or provocative headlines on page one. Readers select articles intentionally and they are not flooded by topics which just sell well. Moreover, Wikipedia is not a commercial system nor is it influenced by advertisement like in the case of Google search.   

The creation of the research context means in our case that keywords or Wikipedia pages have to be selected according to a given research topic. A market study has to cover entities, related to the market, such as participants, competitors, products and also related subjects. Our approach uses existing implicit semantic relationships between Wikipedia pages to discover such term neighborhoods automatically as illustrated in figure \ref{fig1.NeighborhoodNetworks}. Based on  
%JK
these local neighborhood networks we have a set of domain specific topics and the pages which  
%JK
they are embedded in. Using the embedding as kind of a background signal allows us to normalize the directly measured values in a context sensitive and time resolved way. 

\subsection{Single Concepts vs. Groups and Categories}
The local neighborhood of a single concept forms a tree as long only next neighbors are taken into account. In many cases this would not provide a sufficient amount of data for statistical analysis. Therefore we can extend the data collection procedure in two directions. First, a deeper crawl is possible, this means, we collect data for a higher recursion depth or we follow more links. Second, we can select more CN pages, especially if the same concept has different names or facets. In this case each facet contributes to the overall amount of information. Figure \ref{fig.PLOSONE}.a shows the group definition as a set diagram without explicit links. Links are used here for group definition only. Elements do not have to have links to all elements in the same group. All possible links between groups are highlighted in the adjacency matrix in Fig. \ref{fig.PLOSONE}.b, which is also a simplification of the network, shown in Fig. \ref{fig.PLOSONE}.c. The network as shown here is only a representation of one single aspect, derived directly from the data. We have a structural network which describes a particular state of the system at a given time. It does not reflect the dynamics of the system at the same time. 

\begin{figure}[th!]
\begin{center} \includegraphics[width=6in]{semanpix/PLOSONE/figure1/sketch2v5.png} \end{center}
   \caption[\textbf{Selection and preparation of datasets}. (a) For each topic (central node, CN), we study all directly linked nodes in the same language (local neighborhood, LN), all nodes regarding the same topic in other languages (linked by inter-wiki links, IWL), and the all nodes linked to nodes in the IWL group (global neighborhood, GN). (b) The CN and the IWL group form the core of the local network for one topic, while both neighborhoods form its hull. Black colored node pairs are ignored because they express long range relations between groups which are considered to be less relevant.  (c) Network representation of all nodes regarding the central node 1.1 (from table \ref{tab.CS1.tab1}).]{\textbf{Selection and preparation of datasets}. (a) For each topic (central node, CN), we study all directly linked nodes in the same language (local neighborhood, LN), all nodes regarding the same topic in other languages (linked by inter-wiki links, IWL), and the all nodes linked to nodes in the IWL group (global neighborhood, GN). (b) The CN and the IWL group form the core of the local network for one topic, while both neighborhoods form its hull. Black colored node pairs are ignored because they express long range relations between groups which are considered to be less relevant.  (c) Network representation of all nodes regarding the central node 1.1 (from table \ref{tab.CS1.tab1}).}
\label{fig.PLOSONE}
\end{figure}

The primary goal in this chapter is to define the 'core' and the 'hull' (see Fig. \ref{fig.PLOSONE}.b) which represent the research scope embedded in its neighborhood. As mentioned before we can either use only one node as CN. The semantic core is formed by CN and IWL (all languages) In this case. The semantic neighborhood is LN, GN (all languages). The result is a tree, as long no back-links to pages are used. In order to handle a bidirectional embedding in the neighborhood, all in-links would have to be considered as illustrated in Fig. \ref{fig1.NeighborhoodNetworks}.a. The second approach is based on a list page chosen as CN. A category page can be used as well. The approach does not start with the central node but with a 'Meta page'. This has an impact on the data collection procedure. IWL contains now the Meta pages in other languages. LN and GN are not the neighborhood but all core topics and form the set of CN pages like shown before. This approach requires an additional step for data preparation.\\

An additional alternative approach for local network aggregation is based on the concepts of the linked data web. DBPedia is called 'A Nucleus for a Web of Open Data' \cite{Auer07dbpedia}. The major part of the semi- and unstructured data from Wikipedia is available in a transformed highly structured representation  \cite{isem2013daiber,isem2011mendesetal}. This allows arbitrary queries in SPARQL \footnote{SPARQL: \textbf{S}imple \textbf{P}rotocol \textbf{a}nd \textbf{R}DF \textbf{Q}uery \textbf{L}anguage; defines a standard query language and data access protocol for use with the Resource Description Framework (RDF) data model.}, which are more flexible than queries in SQL-Database. Partial import into local databases, and data discovery procedures support research as well as a the possibility of logical reasoning based on variable ontologies. 
 
 
\section*{Conclusion}

Selection of a context and a neighborhood allows comparison of otherwise non-comparable systems, using network measures. Beside descriptive statistics of node properties and edge properties we need a reasonable size before network measures can be calculated in a meaningful way. Especially comparisons of systems from different domains and analysis of the same system over time are important. The structural network can now be seen as a kind of "ground state" of a system. Different systems can have a comparable or even very different structures. But as soon as more layers of functional networks are added, it is very helpful to have a reference. Each functional layer can now be presented in the context of its specific structural network, compared to it and network measures can even be expressed as relative measures or differences.   


\chapter{Data Selection and Study Design}

The dynamic network of Wikipedia pages is created and influenced by the social networks of interconnected users. We consider multiple networks because Wikipedia is not just one large multilingual system but rather a combination of independent sub projects grouped by language (according to \url{http://meta.wikimedia.org/wiki/List_of_Wikipedias} currently 288) or specific content category types (e.g., Wikibooks \cite{WIKIBOOKS} or Wictionary \cite{WIKTIONARY}). Additional processes - such as server and data maintenance work in the background - has also an impact on system availability and usage statistic. The growth of the network, its pages, and the growth of page clusters are attendant on the ongoing restructuring process. Although, we do not study the growth process of the network in detail, rather than the interactions on top of the network at a given point in time, it is very important to notice, that the system in the focus of our study is not in an equilibrium state. 

The Wikipedia page network has a well defined inherent structure. This complex structure of static links and user page relations forms an evolving network in which links are added and removed over time. Both, the edit- and the access-processes coexist and influence each other. It seems to be intuitive, that network growth, content usage and contribution of new content are highly interdependent. 

One of the goals of this work is a systematic and quantitative comparison of two distinct but coupled processes which are going on in or on top of a complex system. The first process is interactive modification of Wikipedia pages by Wikipedia user communities or individuals. The second process is information retrieval, either sporadic consumption, or systematic research by humans or even automatic systems, such as web crawlers or mobile applications (see Wikipedia page with title "Tools/Alternative browsing" \cite{WIKIPEDIA.Mobile.App}). Because automatic page retrieval influences the outcome of data analysis procedures it is important to describe the raw data carefully. This allows identification end elimination of hidden biases caused by a variety of reasons. Therefore, I introduce our new approach for contextual data preparation. Based on the idea of semantic context networks and the herewith defined idea of a semantic neighborhood we developed a generic procedure applicable to all types of studies, focused on Wikipedia or even more general, time series analysis on data collected from dynamic evolving systems.

\section{How to Select the Right Time Series?}
Even if the system is non stationary we have to prepare the data in a way which allows us to treat the system like a stationary one. This assumption can be correct if the selected time windows are not too long. Monthly data sets give us 720 data points on hourly resolution or 30 data points on daily resolution, which is fine for correlation analysis. Especially the analysis of daily pattern as shown in \cite{2013arXiv1308.1776S} or weekly pattern in access-rate time series allows a classification of network nodes according to typical usage behavior. 

The data preparation procedure includes pre-aggregation and grouping according to the inherent structure of the local neighborhood network which is selected for the individual study. Thus, we need an auto-adaptive approach, especially because we can not know much about the data quality in the beginning. Beside the raw input data, which is used in the following analysis procedures, also the contextual meta data is generated and conserved in our shared knowledge base. This method enables traceability and allows context sensitive interpretation of results, e.g., outliers and systematic trends can be identified and eliminated this way. 

In this chapter we present typical properties of the data set. Data was extracted from Wikipedia server log files and from the life Wikipedia system. During this step we prepared time series buckets (TSB). The page content was also collected. The full page text is available for further analysis as a contextual corpus dump (CCD). 

This enables context sensitive group based analysis. CCD and TSB also allow fast random access to individual pages and time series for any given page in an offline environment. This chapter shows results of a data set inspection. We begin with descriptive statistics of some example groups. 

Four different local neighborhood networks for English Wikipedia pages have been selected in order to illustrate important time series and structural network properties, such as daily and weekly patterns beside the degree distribution of underlying networks. One can now choose from raw data, trends, and detrended data. The later can be seen as a kind of residual analysis. The comparison of, e.g. daily and weekly trends can be used to classify nodes of the network. 

In previous studies \cite{2013arXiv1308.1776S, Boeker2012, Schreck2012, Kaempf2012b, crane2008robust}
individual time series have been analyzed independently from each other. Now, in this work, also the context of the pages is taken into account. Further details about context definition are provided in section \ref{sec.contextgraph}. This allows a systematic contextual comparison of comprehensive node groups covering several topics, even if a clear separation of topics is not possible. 

Data set profiling as a general procedure combined with early visual inspection and simple quantification of data set properties are critical, especially if obtained from large scale systems. This approach allows early validation and plausibility tests as soon as intermediate results are available.

Table \ref{tab.example.datasets} shows the data set profiles. Of special interest is the number of existing pages in the local neighborhood network and the number of available access- and edit-rate time series. We inspect the data coverage rate $r_{DC}$, beside the average access and edit activity per group. For economical and technical reasons it is not always possible to collect all data about all pages. This makes appropriate data set profiles even more important. 

% The table shows the dataset profile 
%%ETOHSA.DOAD http://semanpix.de/opendata/wiki/index.php?title=Category:MDD
\begin{table}[h!]
 \begin{tabular}{|l|r||r|r|r||r|r|r|}
\hline
%\rowcolor{LightGray}
\multicolumn{2}{|l||}{\textbf{Page Network} } & \multicolumn{3}{l||}{\textbf{Access Activity}} & \multicolumn{3}{l|}{\textbf{Edit Activity}}  \\
\hline
\rowcolor{Yellow}
\multicolumn{8}{|c|}{\textbf{Formula One}} \\
\hline 
%\rowcolor{LightGray}
Group & $z_{\rm pages}$ & $ z_{\rm ats}$ & $a_{\rm access}^{\ast}$  & $r_{\rm dc}$ & $ z_{\rm ets}$ & $a_{\rm edits}^{\ast\ast}$ & $r_{\rm dc}$ \\
\hline 
2 CN & 1 & 1 & 3800 & 100 $\%$ & 1 & 623 & 100 $\%$	\\
2 IWL & 91 & 3 & 621 & 3 $\%$ & 91 & 43 & 100 $\%$	\\
2 LN & 1128 & 70 & 1955 & 6 $\%$ & 1125 & 58 & 99 $\%$	\\
2 GN & 17293 & 1066 & 437 & 6 $\%$ & 16907 & 16 & 97 $\%$	\\
\hline 
\rowcolor{Yellow}
\multicolumn{8}{|c|}{\textbf{Influenza}} \\
\hline 
%\rowcolor{LightGray}
Group & $z_{\rm pages}$ & $ z_{\rm ats}$ & $a_{\rm access}^{\ast}$  & $r_{\rm dc}$ & $ z_{\rm ets}$ & $a_{\rm edits}^{\ast\ast}$ & $r_{\rm dc}$ \\
\hline 
4 CN & 1 & 1 & 3407 & 100 $\%$ & 1 & 335 & 100 $\%$	\\
4 IWL & 107 & 12 & 516 & 11 $\%$ & 110 & 13 & 103 $\%$	\\
4 LN & 684 & 203 & 1777 & 29 $\%$ & 772 & 87 & 113 $\%$	\\
4 GN & 7781 & 1005 & 522 & 12 $\%$ & 7634 & 14 & 98 $\%$	\\
\hline 
\end{tabular}
\caption[\textbf{Data set Profiles}. Comparison of two example data sets. Pages of popular topics have been selected to illustrate time series and network properties.]{\textbf{Data set Profiles}. Two example data sets from Wikipedia pages about popular topics have been selected to illustrate time series and network properties which are relevant for this work. Group sizes ($z_{\rm pages}$), number of available time series per group ($z_{\rm ats}$ and $z_{\rm ets}$ ), average daily access, annually edit activity per node, and data coverage ($r_{\rm dc}$) are compared for core (groups CN, and IWL) and hull (groups LN, and GN) from Local Neighborhood Networks (LNN). The colored row shows the data set name, which is also the page name of the selected central node taken from English Wikipedia. Technical aspects of the data collection and preparation processes are explained in more depth in chapter \ref{chap.DATACATALOG}.}
\label{tab.example.datasets} 
\label{table.NNProperties}
\end{table}

Column $z_{\rm ats}$ and $z_{\rm ets}$ in table \ref{tab.example.datasets} show the number of available time series for access-activity and edit-activity. For all existing pages it is possible to load the edit history. The coverage ratio for edit event series is usually 100\%, sometimes it is less, if pages were removed since the network structure was collected. Depending on the crawl mode one can also have a higher coverage (see groups 4.IWL and 4.LN in column $r_{\rm dc}$ in table \ref{tab.example.datasets}). In case of a life crawl, we can detect the differences in the page network, while the static crawl mode only looks up the edit history of the previously collected page names. In this case the coverage can not be above 100\%.  Even if no edit activity was detected during a given time range, we did not distinguish between no activity or a non existing page. If the growth dynamic is taken into account during a time dependent study, one should carefully distinguish both cases because a not existing page is not the same as a non active page. If the page creation date is after the end of the period of interest, this page should be considered as non existent. In our case, the data coverage ratio is calculated for access- and edit-activity. 
The average number of access-events per node and day is shown in column $a_{\rm access}^{\ast}$. Column $a_{\rm edits}^{\ast\ast}$ contains the average number of edit events per node per year.

This values allow an estimation of the available amount of data and the collection cost (time and resources), and it helps to estimate the expectable significance of results. If the coverage ratio is very low, as in the case of the page about \textit{Formula One}, one should interpret the results carefully. The reason for this low coverage can be a technical problem during data collection. As data transformation and aggregation is done via self made systems, additional validation and plausibility tests are required. If no technical problem can be found, there is still the chance of existence of a hidden bias. Our approach can even be applied if the groups are not complete. In general, the central node is the one, which was selected because of their relation to a specific topic. The neighborhood is used for auto-adoptive-normalization. The more time series we can get the better the accuracy would be. But even if only three rows are available, this means not that the results are completely wrong. 

The selection of appropriate central nodes is crucial for purely data driven analysis. The most common problem during early study design was the selection of a Wikipedia page which was only a redirect page. In many cases, no access count data exists for such pages. One has to follow the redirect link and the page name has to be replaced. Also disambiguation pages should not be selected as a starting point, except, one wants to study properties of such pages. 

Table \ref{tab.example.datasets} also highlights some typical problems found in the raw data set. From four initially selected central nodes, only the two about \textit{Formula One} and \textit{Influenza} are shown here. We use those also for demonstration of the newly developed analysis methods. 

\subsection{Properties of Selected Local Neighborhood Networks}
A simple local neighborhood graph has a tree structure if it is extracted for one language starting in one node, following $n=1$ levels of links. The left panel of figure  \ref{fig.DataSetDescription1.b}) shows multiple of such trees which form the multilingual neighborhood of the page \textit{"Econophysics"} This is not a simple tree anymore. An obvious network structure appears for $n=2$ (see right panel of figure \ref{fig.DataSetDescription1.b}).

%
% Here I compare the tree and the real network
%
% Color coding in the tree:   Just the groups C, IWL, GN, LN ( image was taken from PAPER )
%
For studies about topics which can not be defined by one term, such as political topics, economical aspects, art, culture, or financial markets, it is even a good idea to have more than one central node. We show such a market study in chapter \ref{TBD}. 
%%ETOSHA.REF
Instead of only one, 42 different Wikipedia pages were selected as central nodes. All are representative for the emerging Big-Data market or at least historically related to the topic. Combining all those trees in one graph also produces a network structure. Cluster analysis reveals several modules (see figure \ref{TBD}). 
%%ETOSHA.REF
Such modules can either represent clusters of highly interlinked pages within a given semantic space or more likely pages from the same language. Figure \ref{fig.DataSetDescription1.b} shows color coded modularity classes for pages from multiple languages. This clearly shows, how important a proper validation of the selected data is. An early inspection of the network structure allows validation and plausibility checks before expensive data extraction and network analysis algorithms are applied. 
Note, how such clusters are formed in a sub graph depends on the method of data extraction. 

We can conclude: (1) Data collection separated by language works best for trees ($n=1$). The focus of such a study is preferable the lingual space. (2) Content and topics influence the clusters in case of deeper crawl procedures with $n>1$. In this case much more neighbors are taken into account. Efficiency of this method can decrease very fast. (3) With $n>3$ the network will be very large in most cases. Instead of a deep crawl, a broad selection of central nodes should be considered as this allows creation of meaningful interconnected neighborhood networks even with a flat crawl procedure.

The local neighborhood networks around a central node contain all next neighbors of all core nodes. The core nodes represent the same semantic concept in multiple languages. Links between such core nodes are of a special type. Technically they are handled in a different way and they imply also the semantic meaning which is comparable with the property '\textit{same as}' which is used in RDF\footnote{According to \cite{Lassila1999}: '\textit{A specific resource together with a named property plus the value of that property for that resource is an RDF statement. These three individual parts of a statement are called, respectively, the subject, the predicate, and the object. The object of a statement (i.e., the property value) can be another resource or it can be a literal; i.e., a resource (specified by a URI) or a simple string or other primitive datatype defined by XML. In RDF terms, a literal may have content that is XML markup but is not further evaluated by the RDF processor. There are some syntactic restrictions on how markup in literals may be expressed}'. Our approach allows creation of LNN graphs from arbitrary web pages or even based on semantic graphs, represented in RDF.} graphs. 
%%ETOSHA.Task - show how RDF graphs are created from Wikipedia in the DBPedia project.
All Wikipedia links from this pages are handled as part of the neighborhood network. If the data collection procedure is executed with depth $n=1$ only a set of trees is generated, as shown in figure \ref{fig.DataSetDescription1.b}.a. More context information is collected if also the second neighbors are taken into account. In some use cases it is even interesting to collect related data from other context, e.g. linked documents outside Wikipedia, pages in other Wikipedia projects, such as Wikibooks or Wiktionary or even the discussion pages inside the Wikipedia system. Such context layers are very interesting for more advanced studies about communication processes and communication patterns. For this work we use only the second neighborhood to illustrate the usage of this data. The goal of this extended neighborhood embedding is a quantitative comparison of different topics, covered by the pages which form the core of the neighborhood network. We collect all the nodes and links of the second neighbors and calculate structural metrics for this structural page networks. The term structural is used here to differentiate this kind of network from functional networks. We start with this type of a network as the given state of the system. The state is not necessarily stationary although it could be. A network life cycle analysis uses several networks which represent the system at different times. During such a period we consider the system to be stationary, which means in this case, that not too many nodes are removed or added (see also chapter \ref{sec.WIKIGROWTH} for more details on Wikipedia growth).
 
\label{ext.fig.DataSetDescription1.b} 
\input{semanpix/DataSetDescription1.b/imageLS}

Figure \ref{fig.DataSetDescription1.b} shows such a second level neighborhood for the Wikipedia page about the famous "Milgram Experiment". The network is not just a set of simple trees any more. A systematic comparison of topics regarding their embedding is possible. We show common network measures, available in the network visualization software Gephi \cite{ICWSM09154} in table \ref{table.NNProperties}.

\label{ext.fig.DataSetDescription1.c} 
\input{semanpix/DataSetDescription1.c/imageLS}

A topic oriented analysis would be usefull, if we are able to measure a characteristic property which allows to distinguish "broad topic" from a group of very specific pages. Network visualization gives a first impression very fast, but in some cases the networks are to large. In this case we have to care about a numerical representation, automatically computed from raw data. I suggest to count the number of triangles and to calculate the diameter of the LNN representing a particular topic. An efficient detailed analysis of thousands of such LNN graphs is possible, based on our preliminary results, but because of limited time and computational resources we could not apply machine learning algorithms for automatic classification of the LLN graphs obtained from Wikipedia.   

For some examples we collected the LNN graphs, access-rate time series, and edit-event time series. The time series dashboards are presented in the following sections.
 
\label{ext.table.NNProperties} 
\input{semanpix/table.NNProperties/imageLS}


\subsection{Contextual Time Series Dashboards}
Contextualization based on local neighborhoods, as introduced in the previous section, allows grouping without '\textit{a priori}' information, especially in a non stationary environment. Only the existing structure at a given point in time is taken into account. Even if more nodes exist later or if nodes are removed or split this approach works stale. Because the system evolves over time - this also means more users might be attracted - a higher connectivity in the functional network or a changed topology can be a consequence. It is really important to know the structural information and how it evolves over time. With this in mind, a comparison of averaged time series data allows a further contextualization. One can compare the activity pattern for different groups rather than the absolute access activity to individual nodes. Individual events can be artificial and very important because of this. But it can also be considered as an outlyer or the result of a technical problem inside the system. Figure \ref{fig.DataSetDescription1.a} shows this for two different LNN from very different topics. The access activity for both pages show the same pattern during the highlighted period. This is a clear indicator for a systematic error somewhere inside the data or the system which produces the data. One can only distinguish both cases if background or context information is available. 

\label{ext.fig.DataSetDescription1.a} 
\input{semanpix/DataSetDescription1.a/imageLS}

Figure \ref{fig.DataSetDescription1.a} allows already a direct comparison of the patterns, such as peaks (see label A), plateaus of constant interest (see label B), or increasing trends which are overlaid by oscillations (see label C). We use the TRRI for a direct comparison of local and global neighborhoods. Usually both are not well comparable, but becaues TRRI is a relative measure it can be applied. As shown in \ref{fig.DataSetDescription1.a}.d, we are able to identify local maxima above the seasonal trends. Also the comparison of the seasonal trends is more clear with this relative measure.

\label{ext.fig.DataSetDescription1.d} 
\input{semanpix/DataSetDescription1.d/imageLS}

In general, the edit activity is far less than the access activity to Wikipedia pages. In all cases we found, that the edit activity for the central node (CN, green crosses) is above or comparable to the edit activity in the local neighborhood (LN, black curve) which is on a comparable level for all four selected groups. A lower activity in group IWL indicates less interest in the topic in other languages compared to the chosen language of the core node (a,c, and d). For group 2.CN we found a high edit activity also for other languages, especially at the beginning and the end of the season.

\section{Peaks, Hidden Bias, and Trends}

How can I distinguish between a unique peak and a repeatable pattern? This question seams to be trivial. But it is not, a peak can appear regularly on the same day of the year. The frequency of the phenomenon and the selected timescale influence this primarily. 

\subsection{Weekly Patterns on Hourly and Daily Resolution}
If the scope or the length of the time series is limited one can define a smaller time scale and calculate the average values, e.g. for all Mondays, for all Tuesdays, and so on in order to calculate the weekly average values as shown in figure \ref{fig.TSTrends0} in hourly and in figure \label{fig.TSTrends1} in daily resolution. Such weekly patterns reveal a typical fluctuation in the order of 50\% of the maximum but on Sundays the activity is more than double as high as the average activity on normal days. A peak in the weekly patterns can be caused by recurring events or even by one single large events (compare with the single peak in the activity time series for the Wikipedia page 'Illuminati' in \cite{Kaempf2012b} figure 1). Not only the weekly activity pattern is influenced. At the time when the peak occurs, also the variance is significantly higher. In case of recurring peaks triggered by recurring events, like formula one races on Sundays (but not all Sundays) the variance would be much smaller. The ratio between absolute value and variance can be used as a measure for periodic re occurrence  or sporadic events. 

\label{ext.fig.TSTrends0} 
\input{semanpix/TSTrends0/imageLS}

Wikipedia access time series show seasonal trends on multiple time scales. Daily access patterns are dominantly caused by the day night cycle. Some illustrative examples are shown in \cite{2013arXiv1308.1776S} and for edit activity in \cite{Yasseri2012}. A strong effect can be observed for pages written in languages with a very strong linguistic localization. This periodic effect is much weaker for pages in English language because of their wide spread globally relevance. The day-night cycle can be modeled by a periodic function with a characteristic scaling factor $s_d$ for each day of the week. Because $sin( \omega t)$ has negative values, $sin^2$ should be considered. The difference with this theoretical cycle and real data can also be used to compare topics to each other using similarity or distance measures. 

Other classes of seasonal patterns are less or not periodic and therefore can not easily be modeled by a simple function. Depending on occurrence of events in the real world such pattern can be represented by a series of strong peaks. Such peaks are found dominantly on a specific day of the week for some weeks of the year only, e.g., on Sundays as shown for the Wikipedia page "Formula One" (see fig. \ref{fig.TSTrends0}.a). A longer seasonal trend with a maximum during the winter period and a minimum during the summer time can be shown for the page about "Influenza" (see fig. \ref{fig.TSTrends0}.b).

\label{ext.fig.TSTrends2} 
\input{semanpix/TSTrends2/imageLS}

Figure \ref{fig.TSTrends0} shows typical patterns on hourly resolution. At daily resolution (see figure \ref{fig.TSTrends2}) shows the schematic differences between the two topics more clearly. For figure \ref{fig.TSTrends2} the data was rearranged, so that the first day is Monday. This allows a more intuitive interpretation according to a calendar week. One has to be careful here. In figure \ref{fig.TSTrends0} a time window is aligned with the raw data, which starts on the 1-st of January of 2010. This was a Friday. Both representations should be used in an appropriate context. For a general interpretation, the "re organized" data seems to be preferable. 
One can see that on weekends the access activity significantly differs. It decreases in the core and hull network around the page 'Influenza' and in case of formula one, we can even see a difference between the core and hull.
The core, formed by the page and all international representations of the same (CN and IWL) has increased activity on weekends, due to the races while the neighborhood, in English language and also all international neighborhoods show a decreased activity on weekends. A normalization to the maximum value can highlight this structure even more. 

This allows a classification of the pages. The weekly pattern reveals if a page is continuously in line with the neighborhood as in case of Influenza or if a specific time exist, during which the access activity differs from that found in the neighborhood, as in case of 'Formula One'. The top row shows, in different colors, the different behavior. This property can easily be described with a signed number. The bottom row shows not such an obviously clear difference, so we treat this as a neutral node, while the other one shows a clear polarizing behavior. Such a node property is can not be measured from just one single time series. One needs the structural properties of the neighborhood graph to define the appropriate node groups for which the averaging procedure is then applied. This kind of a node property, if assigned to the central nodes of a page network, can influence the layout of the graph if the layout procedure takes this effect into account. Beside \textit{'Social Gravity'} (see Bannister et al. \cite{Bannister2013}) it seems to be reasonable to use also the new concept of 'Neighborhood Polarization', a makroscopic analogy to spin which describe a nodes orientation with in a force-directed layout and to measure the structure induced stress (SIS). This approach is introduced in chapter 14 (see Fig. \ref{fig.StructureInducedStress}). 

%%ETOSHA.LINK https://drive.google.com/drive/u/1/#search?q=Fruchterman

%E.table_end
\begin{table}[h]
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{LightGray}
  & Formula One & Influenza  \\
Groups & $R_{pearson}$ & $R_{pearson}$  \\
\hline 
CN - IWL  &  0.97	& 0.99 \\
\hline 
CN - LN   &  -0.36  & 0.99  \\
\hline  
CN - GN   & -0.19    &  0.98    \\
\hline 
LN - GN    & 0.93  &  0.99 \\
\hline
\end{tabular}
\caption[\textbf{Comparison of weekly trends with Pearson-Correlation}. Pattern comparison allows simple classification schema.]{\textbf{Comparison of weekly trends with Pearson-Correlation}. A quantitative comparison of weekly trend patterns is a fast and efficient approach for node classification. Nodes which behave like their neighborhood (right column) can be distinguished from those with opposite trends (left column). }
\label{tab.quantitative.pattern.comparison} 
\end{table}
%E.table_end

Table \ref{tab.quantitative.pattern.comparison} shows results of correlation analysis for  comparison of weekly trends. Such a quantitative comparison of weekly trend patterns is fast and efficient. It allows pairwise node classification and early detection of anomalies. Node pair with comparable properties behave the same way. This leads to high correlation values which express a strong similarity (see right column). A different type of pages, which are not in line with their neighborhood can easily distinguished from those because of their low correlation values (see left column).

\subsection{Seasonality in Detrended Time Series}
Because there is no homogeneous distribution of Wikipedia users on earth and because people usually use the internet less during the night a strong day-night cycle can clearly be observed (see figure \ref{fig.TSTrends0}). This pattern can be eliminated by aggregation of daily access rates from hourly data. The previously mentioned patterns are still visible. A comparison of weekly patterns, individually calculated for central page and the neighborhood reveal even another characteristic property of Wikipedia pages. The page about "Formula One" shows the maximum access-rates on Sundays, while the neighborhood pages have an access-rate minimum on weekends. A different but coherent pattern was found for the page "Influenza". The central node and all neighborhood nodes show the same pattern which indicates existence of a comparable behavior.

\label{ext.fig.DataSetDescription1.e} 
\input{semanpix/DataSetDescription1.e/imageLS}

This trends are useful for two reasons. First, they allow a classification of nodes even in a time dependent mode with variable granularity. Furthermore, removing this weekly trends, as shown in figure \ref{fig.DataSetDescription1.e} works s a smoothing and normalization procedure and leads to time series which can be compared directly with each other. The top row in figure \ref{fig.DataSetDescription1.e} shows the averages of the logarithm of the access-rate data for one year for the different neighborhoods (IWL, LN, and GN) around the central node CN. The bottom row presents the results after the local and global time resolved relevance index was calculated. This data allows a better comparison between the time series on a daily base. E.g., the differences between L.TRRI and G.TRRI or just the sign of the difference can be used to define another node property, called activity polarization. A node has a positive polarization if L.TRRI $>$ G.TRRI and a negative polarization otherwise. The node activity can now be interpreted as stable if the polarization does not change during a longer time (see figure \ref{fig.DataSetDescription1.e}.d) where except of a two spikes no significant changes are detected during summer time. The fluctuating polarization as shown in \ref{fig.DataSetDescription1.e}.c is characteristic for instable nodes. Bursts, triggered by real world events - in this case the formula one races - dominate the overall activity of the page. This kind of page classification helps during interpretation of results obtained from further analysis as explained in the following chapters.

\clearpage
\newpage

\subsection{Frequency Analysis}
%
% What is the relation between Frequency and Sampling Rate?
%
Fast Fourier Transformation (FFT) was applied to hourly access-rate time-series in order to show the power spectrum of raw data series from one year. The page about '\textit{Formula One}' was used as an example. Figure \ref{fig.FrequencyDistribution5}.a shows the amplitude spectrum (large black dots) for the raw data with logarithmic x-axis and allows a comparison with the power spectrum from detrended time series (green dots). Figure \ref{fig.FrequencyDistribution5} (b)  shows the normalized spectrum. After applying the detrending procedure - this means the raw data was divided by the weekly trends - the typical peak at a value of $1/24$ corresponds to the aggregation resolution of one hour and remains but in general the higher frequencies are suppressed. 
If one value is measured per second, the highest detectable frequency is 0.5 Hz. For hourly data the frequency band ends at $1 / 7200$ Hz. The lowest frequency is determined by the length of the series, which is in our case 8760 hours or $31.536 \cdot  10^6$ s which relates to $f=3.2 \cdot  10^{-8}$ Hz. For practical reasons such frequencies are not useful.
 
\label{ext.fig.FrequencyDistribution5} 
\input{semanpix/FrequencyDistribution5/imageLS}

Distribution of values and correlations between the values in a time series are two different properties but both are influenced by shuffling as it introduces randomness in the order of time series values. Simple shuffling is used to prepare artificial time series for significance tests. This procedure preserves the distribution function of the values but correlations will be destroyed in this process, also the power spectrum would be changed by the shuffling procedure.

Although the daily cycle and weekly pattern are clearly visible (see previous section) the spectral analysis does not provide relevant insights. The sampling rate is to low and the time series length is to short. The power spectrum shows some oscillations and increased values for the higher frequencies (low values on x-axis). For purely random data one would expect white noise, which means all frequencies would appear with the same probability. From figure \ref{fig.FrequencyDistribution5} one can conclude that the access process consists of a random part which is paired with periodic process or at least correlated access-events.

\section{Activity Correlation of Coupled Processes}
For each individual time series we calculate the total access activity and the total edit activity. Because Pearson-Correlation is very sensitive and results can be misleading especially if the values are not from a Gaussian distribution we also apply the Rank-Correlation to the data points obtained from the previously defined time series groups.

We evaluate the Null-Hypothesis: a correlation between the edit-activity and the access-activity of Wikipedia pages exists. According to the results in table which show the Pearson-Correlation and Rank-Correlation coefficients \ref{tab.example.access-edit-correlation} we can not reject it for the hull of both example pages. The correlation is less significant in case of the core due to the small number of data points available for this group. Pearson-Correlation should not be used in this situation, it would even indicate teh opposite result, which is a stronger correlation. Local neighborhood networks can be compared based on this measure over a period of time. Thus, a time resolved classification, depending on the correlation properties is possible, but only if a sufficient number of nodes exist in the selected groups.
  
% data.chapter.8.5
%\label{ext.fig.DataSetCharacteristics3} 
%\input{semanpix/DataSetCharacteristics3/imageLS}


% Show the Box-Plot for the total access- and edit-rate per group and category.

%\label{ext.fig.ValueDistribution4} 
%\input{semanpix/ValueDistribution4/imageLS}

% Show the bivariate distrubution ... and correlation analysis results for 
% rank-correlation between access- and edit-rates per group.

% Show the dataset profile 
%%ETOHSA.DOAD http://semanpix.de/opendata/wiki/index.php?title=Category:MDD

\label{table.AccessEditCorrelation} 
\input{semanpix/table.AccessEditCorrelation/imageLS}

Table \ref{tab.example.access-edit-correlation} shows the differences between the applied methods. Based on Pearson-Correlation we would conclude, that for core and hull a high and  correlation exist. Based on Rank-Correlation a different result appears. We can see a difference between core and hull, which shows the higher correlation for both examples.

% \textbf{Use a significance test tool:} http://www.socscistatistics.com/pvalues/pearsondistribution.aspx

\clearpage
\newpage


 

\setchapterpreamble[u]{%
\dictum[Albert Einstein]{Look deep into nature, and then you will understand everything better.}
\vspace{1cm}
}




%Read more at http://www.brainyquote.com/
%search_results.html#WPTd0hZyBjsg6jk7.99				

%%   THIS CHAPTER IS based on the ARXIV report.
%%
%%   v07_MK.tex
%%   --------------------------------------------------------------------

\chapter{The Life Cycle of Social Content Networks}
Different Wikipedia projects grow very differently. This is not surprising, because they are maintained by different communities. Thus, they are influenced by different economical, political, and cultural conditions. Because each Wikipedia sub project is created in a different language, one can say, that each Wikipedia project represents a different cultural context wherein different topics are important. This view is based on differences in how languages are used and how different cultural aspects are reflected within community driven content aggregation. 

A second perspective exist. On a higher abstraction level, all Wikipedias can be unified, by ignoring the cultural and lingual differences. In this case one can say: Wikipedia is the global encyclopedia. It is also a crowd based information and knowledge creation system, a growing system with inherent memory. For multiple languages, there exist several Wikipedia instances. All are interconnected clusters, representing subsystems and may have comparable properties. If a reasonable approach of normalization of the data would exist, one could compare the project life cycles or phases. The goal is, to verify this hypothesis. Thus, we analyze the growth procedure of four Wikipedia projects, especially the English, Swedish, Dutch, and Hebrew Wikipedia projects. We find, that comparable properties exist beyond number of pages and links. This allows us to describe the life cycle phases based on growth rates and structural embedding ratios. A preliminary report was published online by Schreck \textit{et al.} \cite{Schreck2013}. 

%\url{http://arxiv.org/pdf/1308.1776v1.pdf}.

\section{Cultural Aspects of Global Online Networks }
The detailed impact of different cultures on Wikipedia content and on the processes, such as content creation and consumption are not in the scope of this work. In our data driven studies we follow a natural path and differentiate by language. This is easy, because all Wikipedia projects for all languages coexist and they are interrelated already. Each language defines one lingual dimension if a global analysis is desired. 
An advanced approach uses more specific dimensions. They were derived by Hofsted \textit{et al.} \cite{Hofsted1984} from global survey data. As they show, it is possible to apply factor analysis to determine the predominant cultural dimensions. Hoftsted \textit{et al.} initially defined four cultural dimensions regarding fundamental anthropological problem fields. The dimensions are named: power distance (PDI), individualism (IDV), uncertainty avoidance (UAI) and masculinity (MAS). Long-term orientation (LTO) and Indulgence versus restraint (IVR) were added later as additional cultural dimensions. Although this approach is data driven it is not applicable to a global system like Wikipedia. Wikipedia covers many different topics. A clear segregation between cultures is not possible. One reason is, that many people speak multiple languages. Even if they have a different actual intention, they may contribute to the same Wikipedia project or to a different one, depending on their current background or context. Culture is one context, but obviously not the only one which influences the representation of topics within Wikipedia (see figure \ref{fig.sm2a} for an illustration of the impact of the lingual context on a topic's representation in different languages). Therefore, Wikipedia seems to be a good source for advanced studies on lingual  differences in knowledge formation and knowledge sharing which is related to cultural contexts as well.   

% Hofstede, Geert (1984). Culture's Consequences: International Differences in Work-Related Values (2nd ed.). Beverly Hills CA: SAGE Publications. ISBN 0-8039-1444-X.

% http://jasss.soc.surrey.ac.uk/12/1/6/appendixB/Axelrod1997.html

% \cite{Pfau2012} The co-evolving of cultures, social networks






\section{Growth of Wikipedia Projects}
The Wikipedia projects are more than just networks of pages, to which a new page is added at a given time. Pages provide information, they innervate new ideas, lead to questions, and as a consequence, new pages are added by different people. The editorial process can be highly controversial as Yasseri \textit{et al.} \cite{Yasseri2012} and Eckstrand \textit{et al.} \cite{Ekstrand2009} show. The technical system is embedded within user communities which consist of editors and readers . Not all people contribute to Wikipedia, but a critical mass of users seams to be required by a Wikipedia project in order to survive. The evolution of Wikipedia project sizes was already analyzed by Ortega \textit{et al.} \cite{Ortega2007}. They found that the contributions to Wikipedia are dominantly made by several so called "power users". Based on a calculation of the Gini coefficients for the top ten Wikipedias they state that approximately 90$\%$ of all users are responsible for less than 10$\%$ of the content. A comparable distribution of user activities in several other wikis - non of them are Wikipedia projects - was found by Stuckman and Purtilo \cite{Stuckman2009}. Such a strong bias towards some very active users has to be taken into account. I think, that analysis of the Wiki life cycle should not only be built on the editorial activity as presented by Gorgeon and Swanson \cite{Gorgeon2009}. They studied the evolution of the topic or concept "Web 2.0" in Wikipedia based on article size, number of editorial actions and number of contributors. As a result, they define four phases for an article: Seeding, Germination, Growth, Maturity (for details see section 5 in \cite{Gorgeon2009}). The life cycle phases already take the activity and controversial character of editorial events into account. One can clearly conclude, that editorial activity does not always lead to an increase of content, because higher quality can be achieved by clear statements which are often the result of shorter sentences. Too long articles can be seen as misleading or distracting. To short articles are not providing background information. Several different categories or types of articles exist in Wikipedia. Ortega \textit{et al.} \cite{Ortega2007} show, that article size distributions are bi-modal for English and Polish Wikipedia projects. 

These studies do not care about the network structure of the articles. Based on the node degree or on a centrality measure one can differentiate between leaf nodes, which contain definitions and well accepted facts and more central pages which are related to many topics and which define context as they aggregate several leave nodes. Such additional aspects show, that edit activity is not only related to a change of words or sentences but also to a structural change. Furthermore, the embedding of a page is important. In many cases it is even not possible to work with just one page, because the selected topic is represented by different pages within the same language. Aggregation over all pages of the topic - or even a full category - and contextual normalization within the local embedding was developed as a part of this work (see chapter \ref{chap.ContextsensitiveRelevance}). Such aggregated measures can contribute to advanced life cycle models on the microscopic scale as they are obtained from individual pages data. 

A social network is defined by interactions between many people. No matter if the final result is creation of a new resource in a content network or if it leads to a specific temporal state of minds of all connected participants. One can analyze the underlying structure in both cases. According to Borge-Holthoefer \textit{et al.} \cite{BorgeHolthoefer2011} the evolution dynamics of a social community can be described by the size of the giant component, plotted as a function of time. Changes of growth rate can be interpreted as an indicator of existence of such a particular social network which has no physical representation, such as Wikipedia topics which are formed by interconnected pages. By calculating the giant component of networks we use already more details, such as the link structure, instead of just counting words. In the next step, we apply a semantic analysis of the pages content. Calculation of the semantic distance allows us to quantify the similarity of pages and the semantic flow between them. Such a semantic flow network can be compared to the static link network. The important question is now: Are semantic similar pages more likely to be linked or is semantic similarity a pre-cursor for link creation?  
 
The social ties of interconnect users and editors influence the process of content creation. An edit war, e.g.,  is something which goes on without explicitly being announced. But this mental state within the community is measurable and influneces the life cycle of articles and probably the entire Wiki. Such procedures bind and require energy, attention, and information can even be lost over time.

 
\section{Towards an Integrated Growth Model for Social Content Networks}

As described in section \ref{sec.networkgrowth}, the \textbf{random graph model} is used to create new links between already existing network nodes with equal probability for all possible nodes. A second important model is called \textbf{preferential attachment}. New nodes are connected to an existing network, influenced by properties of existing nodes (e.g., with an attachment probability depending on node degree). Thus, nodes with some neighbors have a higher chance to get new nodes attached to it. In this model, new nodes can also be added without any link and even disconnected clusters can appear. Such simple models are helpful if only the final structure of the generated network should be analyzed. They can not be used to describe the evolution of systems like Wikipedia entirely because they neglect the change of internal system states and dynamic structure. Both do not represent changes in the growth rate nor do they handle different phases within the systems life cycle, which are characterized by variable growth rates and variable attachment probabilities.

%\label{ext.fig.NewGrowthModel} 
%\input{semanpix/NewGrowthModel/imageLS}

The goal of this section is to formalize this concept better and to describe preliminary results from growth analysis, applied to data from four Wikipedia projects spanning a time range of 12 years. To illustrate the model better, I use the concept of Emissivity as an analogy. Although the analogy is weak it helps to understand the many facets within one coherent framework. Therefore, I compare Wikipedia with a physical body which consists of matter and has a given structure and temperature. In Wikipedia we can not such matter and also no temperature. Because content in digital documents can easily be copied one has not to care about conservation of mass. In order to describe a flow of information we have to track the embedding of the system. In the simplest case, it is surrounded by a field of information, which can be absorbed. This can lead to the growth of the system. In case of an equilibrium - which is the ideal case - we can assume that we have a constant exchange of information between the system and the neighborhood. In case of Wikipedia, we can clearly say, the more information it contains, and the better the structure supports easy access to contained information the higher the systems impact and its usefulness will be. With this in mind we can use the analogy and compare Wikipedia with a solid body, which exists in a field of radiation. The incoming energy flow leads to an increase of internal energy and to internal heating. The body emits energy according to its internal state. In an equilibrium state it emits the same amount of energy as it absorbs. A higher temperature is causing a higher radiation intensity.

%\textbf{Explain Strahlungsgleichgewicht}
%\textit{Energie wird in z.B. in Form von Strahlung in ein System übertragen. Die Effizienz der Übertragung hängt von vielen Faktoren ab. Vernachlässigt man Wirkungsquerschnitt, Wellenlänge und Pulsform und betrachtet nur die Menge an Energie, die tatsächlich vom System aufgenommen wurde, dann bleibt dennoch zu unterscheiden, welche Form der inneren Energie erhöht wurde. Verschiedene Prozessbeschreibungen oder Modellvorstellungen helfen dabei, solche Situationen zu erklären. Die Erhöhung der Temperatur ist eine recht einfache Vorstellung, die Anregung von Rotationsmoden eine andere, mit einem komplizierteren Modell verbundene. }

%Das Ziel dieser Analyse ist es, zu betrachten, ob die Aktivität der Wikipedia Editoren, die sich in Form von Edit Ereignissen zählen lässt, zur messbaren Strukturveränderung des Systems und zum Volumen Wachstum in Beziehung zu setzen ist. Gibt es Phasen, in denen der eine odere Anteil dominiert? Wie kann man solche Phasen erkennen?

In order to incorporate measured system properties into a formal description of the system life cycle, a new integrated growth model is required. Inspired by the previously mentioned idea of Emissivity we use equation Eq. (\ref{emissivity}) to describe the process of network growth based on information aggregation. 

\begin{equation}
\Delta I_{\text{system}} = I_{\text{link create}} + I_{\text{node create}} + I_{\text{node change}} + I_{\text{link change}} =  \sum_{events} c_i \cdot v_{event} \thickapprox a_{\text{edit}} 
\label{emissivity} 
\end{equation}

This model describes the growth of the system not only by counting pages and text volume. Instead of volume we define the information content (comparable with a temperature) $I_{\text{system}}$ which is influenced by the number new links and new pages ($ I_{\text{link create}}$ and $I_{\text{node create}}$). Beside adding new elements, which increases the entropy, we can also change the internal structure of the network or the content be splitting nodes, changing text and changing nodes between pages ($I_{\text{node change}}$ and $I_{\text{link change}}$). 

$\Delta I_{\text{system}}$ is the amount of information which is absorbed by the system as a result of edit activity. This activity is not constant, instead it seems to be higher if more information is available. During the life cycle of the system, the contribution $c_i$ of the different events are also changing. In the beginning, we can see more creation of new elements, later, change events dominate (see figure 10.4.a). Obviously, creation of links and the creation of new pages is primarily a structural change. Additionally, content creation leads also to more information within Wikipedia. Because structure and context also contain information, also the evolution or reorganization of the network structure leads to more information. If a large page is just split into smaller but interlinked pages, it is much easier to retrieve information. Relations to other nodes in the network can be found simply by traversing the links. Because the context and the meaning of a certain text have to be known, contextual nodes and links provide the semantic of an information system. A closed vocabulary (e.g., given by category pages or by an explicit ontology) are very simple examples for such context. A more flexible data driven approach was recently presented by Schwartz \textit{et al.} \cite{Schwartz2013} and is called "The open-vocabulary approach". 

No matter how the semantic structure is provided or derived from data, if it exist, the Wikipedia pages can be used like a semantic network. Finally, the inter-wiki link structure and the external links to referenced resources define the neighborhood and a multilingual context. A formal technical representation of semantic Wikipedia data is available as a semantic graph, as provided by the DBPedia project \cite{DBPedia}.

Our growth model covers the creation of new nodes as well as the creation of new links beside changes to the existing content and structure rather than the networks topology. In the case of Wikipedia we can easily count the number of edit events. But what goes on exactly during such an edit event is not measured in our current study. Although each edit event is different and different activity leads to different detail results, we unify this to one contribution. Such a contribution covers one, two or all of the mentioned changes on a different level which is not further resolved at this time.

From the four selected Wikipedia projects we extracted all link creation events and all edit events. Each time a new link appears, also a new page can be created, if both pages between the new link is created do not already exist. All events are grouped by language and sorted by time stamp. Based on this time series the number of new created pages $n_N$ and the number of new created links $l_N$ is calculated on daily resolution. 

If appropriate computational resources are available, one should also calculate the topological properties of the network as a function of time. This can be done at global or at local scale. The global topology is calculated for the full graph, while a local topology takes only the local neighborhood around some pre selected nodes which represent the topic one is interested in well at much lower cost.

%\label{ext.fig.AnalysisDetailsGrowthAnalysis} 
%\input{semanpix/AnalysisDetailsGrowthAnalysis/imageLS}

Time resolved calculation of topological properties requires an incremental update of a large growing in data structure. During each update step, real data is used, and in parallel, simulations are possible within the same computational framework. This allows inspection and also updates of time dependent attachment probabilities. Preliminary results are presented in the remaining pages of this chapter.
 
\section{Properties of the Wikipedia Growth Process}

%Several models are used to describe the growth process of networks.  Two very popular network 
%models are the random graph model and the scale free network.  Both models describe how the 
%internal structure evolves in time, based on the degree distribution.  In the first case, 
%one assumes that all nodes (pages) already exist, and the growth process consists of adding 
%links, one in each time step.  In the second case, one page is added in each time step.  This 
%means a new link and a new page are created at the same time.

In a real network, like the Wikipedia content network, the processes of adding new pages and 
adding new links between pages are coupled and cannot be separated from each other.  In order 
to describe the growth of four selected Wikipedia projects in more detail we analyze the growth 
rates for the number of pages and the number of links. Because several different link types exist, we also compare the growth rates of the number of links for those types and show the link-page ratio in figure 10.3.a. {\it Internal
links} are links within the same Wikipedia (same language) and redirects to another page of
the same Wikipedia.  Internal links represent semantic relations between the terms the pages 
are about or just relations between topics or concepts which are used within a certain page.
If the meaning of a term is ambiguous, special pages help to show users all possible meanings 
(based on other pages).  Such pages do not contribute much text, but this structural information 
is of a high value and increases the usability of Wikipedia.  {\it External links} are links 
to another language (Interwiki links) and links to pages outside the Wikipedia project (e.g., 
references).  The frequency of such links represents an important quality indicator for 
Wikipedia articles. Results shown in the following subsections have been published online (see  \cite{Schreck2013}). 


\subsection{Evolution of the Degree Distribution}

\label{ext.fig.EvolutionOfDegreeDistribution} 
\input{semanpix/EvolutionOfDegreeDistribution/imageLS}

All links between articles and links to external sources contribute to the Wikipedia's structure which evolves over time.  The creation of a 
new link is a result of an edit activity of an user.  Figure 10.1 shows the temporal evolution of
the internal link degree distribution for all pages of the Swedish Wikipedia.  Redirects and 
external links are disregarded in this plot.  Already since the beginning in 2002 the degree 
distribution can be described by a power law, with the exception of pages with a very low 
degree (low number of links).  While pages are added over time, the distribution changes and its 
power-law shape becomes more obvious, since the range of degrees becomes wider.  Actually, most 
of the pages have much more than ten internal links and are well described by a power-law
degree distribution.  Only the number of pages with less than ten internal links is smaller 
than assumed in the scale-free model that predicts power-law degree distributions. This also means, that this model over estimates the number of pages with a small number of links. 

\subsection{Growth of the Content Network and Structural Changes}


Figure 10.2 (a) shows the total number of pages for four Wikipedia projects (Swedish, English, 
Dutch, and Hebrew).  The number of pages $N_P(t)$ is growing by the number of new pages 
$n_P(t) = N_P(t) - N_P(t-1)$ per time interval $\Delta t = 1$ month.  Figure 10.2 (b) shows the 
growth rate $\gamma$ for an exponential growth model $N_P(t) = N_P(t-1) \exp(\gamma)$, 
which has been determined by $\gamma \approx n_P(t)/N_P(t)$.  Note that an increased $\Delta t$
has been used if $n_P(t)=0$.

% GrowthRate4Languages
\label{ext.fig.GrowthRate4Languages} 
\input{semanpix/GrowthRate4Languages/imageLS}

In the beginning the growth rate $\gamma$ is quite large.  Later, a tendency towards saturation 
can be identified.  This shows that the character of edit events changed over time.  In the 
early stage of a Wikipedia project most of the edit events are related to the creation of new 
pages, while later on the internal structure evolves.  For the English Wikipedia project, one 
can see an intermediate regime with an exponential growth ($\gamma \approx 0.7$).  Such an 
exponential growth cannot be unambiguously identified for the Swedish Wikipedia.  Interestingly,
the page-growth rate has been drastically increasing during the last few months (in 2013) for 
the Dutch and -- even more dramatically -- for the Swedish Wikipedia.  Acutally, the Swedish 
and the Dutch Wikipedia started to create articles using bots. 

%
%
%
\label{ext.fig.Figure17REPSV} 
\input{semanpix/Figure17REPSV/imageLS}

Figure 10.3 (a) confirms that editorial activity tends to focus more on the addition of links than
the creation of new pages during later states of Wikipedia evolution.  It shows the ratio of 
the total number of pages $N_P(t)$ and the total number of links $l(t)$ as function of time. 
For all languages this ratio decreases during most of the time after a relatively large value
(around 0.2, i.e., approximately five links per article) in the beginning.  The final values
are between 0.015 and 0.04, i.e. at approximately 25-60 links per article.  For the Dutch and 
the Swedish Wikipedia the initial change (between 2001 and 2003) is quite sudden.  In general,
all four languages show a stronger decay of the page number to link number ratio in the beginning 
and a much slower decay later on.  This behavior suggests that an exponential decay model may 
also be appropriate.  However, we cannot find any regimes with unique or approximately constant 
decay rates for any of the considered four languages.  The different decay rates of the page 
number to link number ratio might also be indicators for two different network growth processes.

The Swedish Wikipedia has initially $\approx 5$ links per page and later the number of links 
per page increases to an average of $\approx 25$.  This is in line with the change in the 
degree distribution, which is shown in figure 10.1.  Here one can see a continuous shift 
towards a dominating structural growth process, while the growth of content volume -- measured 
in number of pages -- becomes less important.  The current ratio of page number to link number
for the Swedish Wikipedia is quite similar those for the English and the Dutch version, while 
the Hebrew Wikipedia has about twice as many links per article.  During the quick growth of the 
Swedish Wikipedia article number in the past few months (see Figs.~10.2 (a,b)), the article to link 
number ratio has slightly grown, which may indicate a slight change of the structure towards 
properties typical for Wikipedias at earlier stages of evolution.  Although, this weak growth 
is still comparable with typical fluctuations of the ratio (just about twice as large), it may
indicate that creating articles by bots leads to a step back in the quality of content. Next we separate the changes of internal and external link numbers.  External links (to other
language versions or references outside Wikipedia) are particularly important for confirmation 
of the article content and can thus be regarded as an important quality indicator for the 
articles. Figure 10.3 (b) shows the ratio of the number of external links to the number of all
links (internal and external). The increasing curves show, that the ratio of external 
links grows for most of the time in all four Wikipedias. We note that there are two major 
groups of external links: just 'further reading' links (often in bad articles) and references 
(more likely in good articles). The habit of adding references increased in the last years, 
while one got more discouraged adding just simple links; they are usually also limited to 
3-5 per article.



%\subsection{Attachement Probability}
%
%A link is created to a node with degree k=x. For all link creation events we count the number of nodes with degree k within a given range $k_min ... k_max$	to draw a histogram which shows the distribution function of the attachment probability. 

%\textbf{We have to compare this result with the theoretical distribution used in in the BA-Model ...}


%Link creation in the presence of edit-activity and access-activity has to be studied if the model should be modified for per link creation predictions.

%So far we investigated the system properties only. But for a prediction of a probability for link creation between two nodes, a separate approach is required. For a given time, one wants to calculate a link creation probability for a selected node pair. Therefore the following data is taken into account: the short term history of both nodes regarding edit and access activity. 

%The large amount of Wikipedia data allows application of a machine learning algorithms such as decision trees and rule-based models for classification. 

%But one has to be careful. Because the system is not stationary, the models may not behave well unless they are trained with data from within the same phase. Here we investigate data from one regime only. A detailed study of the impact of non stationary conditions has to be investigated in a separate project.  

%Here we consider the logistic regression to classify the event under consideration as one of type A, which means, a link is probably created within the next time window or of type B, which means, no link will be created. Other properties, such as correlation between time series, existence of peaks in both series can be used as additional features which may have an influence on the quality of the model. 

%\textbf{More work is required here ...\\
%Further research (maybe later on) ...}

\subsection{Phases and Phase Transitions}
Finally we try to distinguish different phases - but in a much less restricted way, compared to the clear definition of phase transitions in physics. If a particular property of the growth process is dominating, we consider this as a phase in the life cycle of the system. A more precise term could be \textit{regime}. More research on more data is required before real phase transitions can be propagated.
 
In Fig. 10.3 (b) one can also find 
indicators for two regimes for each Wikipedia project except the English: fast growth from 
approximately 2003 to 2005 followed by a behavior close to saturation after 2005. Table 10.1
shows the times where the qualitative behavior changes. The transition time A ($t_A$) is 
determined from Figure 10.3 (a), which shows the ratio of total number of pages and total number 
of links while the transition time C ($t_C$) is based on the plot in Figure 10.3 (b), which shows 
the ratio of external and internal links. For all languages we find that $t_A$ is before 
$t_C$, but the differences vary from 4 to 52 months depending on the languages.  
 
\begin{table}[h!]
\begin{center}
\begin{tabular}{c|c|c|c}
Language & $t_A$ & $t_C$ & $t_C - t_A$\\ \hline
SV & 05/2003 & 10/2007 & 43\\
EN & 12/2001 & 12/2002 & 12\\
NL & 01/2002 & 05/2006 & 52\\
HE & 09/2005 & 01/2006 & 4
\caption{\textbf{Quality changes as indicators for life cycle stages.} The times where the qualitative behavior of the Wikis changes are the transition time $t_A$ (the ratio of total number of pages and total number of links while the transition) and time $t_C$ (the ratio of external and internal links).}
\label{Tab:2}
\end{tabular}
\end{center}
\end{table}

The Swedish Wikipedia has already reached the largest ratio of external links among all links 
in 2010 and has continued to increase this ratio during the last 2.5 years (see Fig. 10.3). 
This is an indication for a very good reference quality of average articles in the Swedish 
Wikipedia, better than in the Dutch Wikipedia. Note that the ratio of external links is very 
much lower in the English Wikipedia, just approximately half as large as in the Swedish 
Wikipedia (data from 2010). The slight drop of the Swedish curve in Fig. 10.3 in the last 
months is probably associated with the drastic increase of the total number of articles 
(see Fig. 10.2). However, it is too weak to be considered as an indication of a drop in 
article reference quality, and there was a significant larger increase during 2012 just 
before the slight drop. We note that  bot generated articles usually have a quite high 
density of references, meaning just one sentence but 2-3 references to publications which, however, may not be linked using a web link. 









\subsection*{Conclusion}
As shown in this chapter, one can measure and study the life cycle properties of Wikipedia projects, based on access activity logs and the page edit history. Simple system properties, such as number of pages, number of links and their change rates were shown in Figures 10.2 and 10.3. Each new page and each link creation event can be extracted from the Wikipedia edit history and real time studies would be possible in the future. Although Wikipedia does not provide aggregates of edit events on a daily or hourly base currently, it seams to be reasonable to provide such data beside the available access log data. This would enable the  research community to study coupled dynamic processes of content creation and information consumption on a global complex system with much less overhead, which is currently caused by expensive data extraction and pre-processing procedures.   

\setchapterpreamble[u]{%
\dictum[Robert Penn Warren, \textit{All the King's Men}]{Reality is not a function of the event as event, but of the relationship of that event to past, and future, events.}
\vspace{1cm}}

%\doublespacing

\chapter{Modeling Complex Systems as Networks to Connect Physics, Social Science, and Economy}

Large datasets allow better statistical accuracy, and combining different types of data enables multi-faceted models. The rise of affordable data analysis engines and large scale storage devices allow broader access to this methods. 

Some well known pioneers of large scale data analysis are companies such as Google, Yahoo, Facebook, Apple, and Twitter. In general, research activities are rather unknown outside the expert communities, but nevertheless the scientific community has its benefits from recent technological improvements. 

Google improves the ranking algorithm since its invention continuously. Recently, a Google research team includes a measure of a page's trustworthiness. Historically, the dominating factor was a page's reputation measured by the page rank algorithm. They simply count number of incorrect facts within a page, as they assume, that a page with less wrong information should be considered to be more trustworthy. \textbf{(arxiv.org/abs/1502.03519v1)}. The 'Knowledge-Based Trust' approach uses also a multi-layer model to represent many relevant aspects with out loosing granularity by early aggregation. Google started early to provide a rich personal user experience. Therefore, they contextualize search results according to user's location or language. The browser history and cookies are also inspected, to learn more about the user. All this information is collected and finally merged into a final result, the score of a search result. 

For advanced analysis, especially for studies of system dynamics this tight integration of many factors in to one score is not appropriate. In order to understand how a system reacts on an external influence, one has to measure and analyze internal properties as a function of time. Google offers access to some of the internal data, but in order to create new studies, new experiments and new evaluation methods, one has to build specific multi-layer models. 

%Another example is given by Yehuda Koren in his article: \textit{'Factorization Meets the Neighborhood: a Multifaceted} 

%R. Bell and Y. Koren, “Lessons from the Netflix Prize Challenge”, SIGKDD Explorations 9 (2007), 75–79.
Physical reality defines an important facet of human life. Science is part of our life and has the goal, to provide insights into our life and explanations about our observations. Scientific models should simply help to understand what surrounds us and how we can interact with this neighborhood. The better our understanding of nature, the better is our ability to adopt to it and even to influence it - this includes also the non physical aspects like those, studied in social science and cognitive science. Describing the nature is not simple, especially because many different things are highly interdependent and over-simplification would lead to insufficient results very soon. Even if we are able to understand and influence nature, it is an ongoing controversial problem, to figure out, if individuals, or organizations should be proactive and take over control. In many fields this is accepted and part of our long term strategies. There are many others, which provide a base for scientific and political debates. Many discussions are based on scientific expertise, others are influenced by believes and even fears.

Independent from final decisions, whether to influence actively or not, modeling enables us to analyze and simulate conditions, which probably are not desired in reality. This way, we gain insight and deeper understanding for better contributions to ongoing and upcoming discussions, or just for short term decisions in a private or business context. 

System theory provides concepts to understand and study complexity. Process models and well defined procedures based on automation allow us to handle complex systems but at the same time we loose control as soon as complexity increases. This can happen if such systems interact with each other. Feedback loops are a characteristic property of complex system. Modeling is considered to be a really neutral scientific approach and a convenient way of turning hypothesis, ideas, and data into knowledge about any complex system.   

No matter on what scale our research is done, we can choose strong simplifications, which usually means, we also disconnect things from their neighborhood. It is important to find approaches which allow us to do our analysis without such a harmful cut in order to minimize the impact of it on the results. If disconnecting subsystems from each other can not be avoided, it is even more important to understand the impact of this segregation on final results. The question is now: Are results really applicable? or do they mean nothing, because things behave very different if they are disconnected from each other?

Active measurements can have an impact on the system. Experimental setups also influence the final outcome. We can not easily stop doing simplified experiments, but we have to find ways to obtain data from undisturbed interconnected large scale systems to build more realistic models. This means, that data collection procedures should not influence the systems directly. Our goal is, to design experiments with less influence on  participants. Buy using so called real world data we can already reduce the artificial impact of measurement procedures and experimental setups. Especially mobile communication devices, embedded sensors in cars, cameras \textbf{(CITE Studie von MALTA)} and counters in airports and train stations, but also activity logs on web sites and payment logs provide a lot of data which support advanced studies of human behavior. Modern mobile phones have more functionality than early personal computers. They have a lot of built in sensors, such as GPS antennas, acceleration sensors, and temperature sensors, but also microphones and cameras. All these rather cheap components can be used for research \textbf{(CITE Studie von Martin Wirtz, London Olympics, CITE Model von JAN und KAMIL)}. Crowd sourcing became very popular during the last years, not only in economy also in research. This shows, that modern scientific methods are integrated directly into business processes. Furthermore, this also indicates, that science is not necessarily bound to product development or specific fields of application, but becomes more important to get information back from real world processes to support operational optimization in real time. 

Experimental design in social science is not free of risks or problems, e.g., one has to care about self selection bias, especially in case of social-network studies. Using internet related systems as single source - even if mobile access is possible - the results are influenced by the limited accessibility of the internet. Infrastructure, economic status, and political decisions in individual regions are the reasons therefore and can not be ignored, especially not if global systems are modeled. 

A simplification is possible - by focusing on a subsystem only - but one has to define the system boundaries very well. Worst case, nothing is known about the surrounding system - which means, an unknown non quantifiable bias still exist.

Physicists study many-body-systems which consist of a large number of interacting objects. A clear description and improved understanding of the microscopic properties of such systems are the objectives of a vast category of physical problems. This category is called many-body problems. Beside analytic approaches in continuous space like mean-field theory and many-body perturbation theory, various discrete numerical methods like lattice-field theory and Monte-Carlo approaches have been developed and applied. Further more it seems to be a reasonable approach to apply this methods to social science. Although a direct transformation of the methods is not possible it is worth to emphasize the possible cases in the beginning as a starting point to us weak analogies, before stable scientific methods are available. 

Finally, it is important to integrate multiple research disciplines. This is what complex
systems analysis nowadays stands for. Data driven methods, large scale data processing and high performance computing are the technical aspects and enable integration of economy, social science and physics by using networks as modeling technique. Network models aim on many body systems with limited simplification and a high degree of contextual embedding.
 
%%ETOSHA.TODO : cite the examples ... http://www.itp.uni-hannover.de/saalburg/Lectures/wiese.pdf

\label{ext.fig.ConceptFNCSA} 
\input{semanpix/ConceptFNCSA/imageLS}

Figure \ref{fig.ConceptFNCSA} illustrates dependencies between measurable data and hidden system properties which can be analyzed on microscopic, mesoscopic, and on macroscopic scale. Hierarchical models can also be created this way.

As an example one can think of crowd analysis, focused on measuring the mood of a large group of people. One way to access the state of individual persons is to listen carefully to what they say. Communication between people is an important source for information. The approach depends strongly on the location of people and on their communication style. A silent crowd and a group of loud crying people, which are marching a long a street, show obviously different moods. In this case, the measurable intensity of the sound the crowd produces is a quantity which can be related to the mood. A much different approach is required if silent, written communication is used, and especially if sender and receiver are not connected to each other synchronously. Text analysis on exchanged messages and sentiment analysis are used to access the hidden properties which represent the mood of participating people. Ideally, one would combine both approaches to investigate consistency and to cover multiple channels as this reflects reality in a natural way.

Comprehensive analysis about the perception of a particular brand is another example. One should combine data from different channels, such as Google, Twitter, Facebook, the companies own website, public communities, and also company internal communication channels. Using only public campaign data is dangerous as long as no knowledge about the system around the campaign is available. 

This work introduces a method to implement reference studies based on public open data from Wikipedia. Our new approach can be generalized to arbitrary data sets. It is a common technique to combining proprietary data with open public data, this is another advantage of large scale data analysis, because not all research groups or companies have to own all data.

System properties like long term memory effects can be revealed from communication patterns (see Oliveira and Barabasi 2005 \cite{Oliveira2005}) no matter if the traditional approach of analyzing the time intervals of letters sent between two persons or a modern approach is used. In the later, more and more digital messages and mobile communication devices and multiple online systems are involved. This indicates already, that modeling techniques should be of a hybrid nature, as they combine time series and network analysis procedures.
Typical interactions between persons and also between persons and their environment can be modeled using the Social Force model (SF) \cite{Helbing1995}. Simulation and numerical analysis is based on the Lattice Gas model (LG) \cite{Succi2001}. 

%%ETOSHA.TODO Show how SOCIAL FORCE MODEL and LATTICE GAS MODEL work.

SF describes multiple aspects of interactions which finally are superposed. This leads to attraction or repulsion forces in a continuous space. The network links between people which were mentioned before can represent such forces. Also qualitative properties, such as types of relations, which are not measurable and therefore defined as categorical values can be expressed in a network model. Interaction roles are used instead of forces to model the dynamic view of different system aspects in LG. Positions in real space and internal state properties of the elements (particles or agents) are updated depending on several conditions based on logical reasoning. This leads to positional updates of agents on the underlying lattice. Now it is possible to describe the systems state at a given time based on connectivity and distances between agents. Topological properties of such snapshot networks, also called temporal networks, represent specific aspects, and allow studies about their evolution in time. 

The fact that different types of interactions co-exist more or less obviously is typical for both models, SF and LG. Simulation and analysis of dynamic system properties is also possible in both, but the internal structure of the system is not accessible directly, although the interactions are inherent in both models. 

A quantum mechanical description of a generic system is given by the configuration vectors of all its objects. Like in the two before mentioned models, those configuration vectors do not represent the interactions between elements nor the internal structure of the system at a given point in time.  In a more general case, like in social networks, the location of objects, such as persons, in real space is not relevant and all relevant interaction roles are typically not known. The intrinsic structure of social system - such as family relations or hierarchies in organizations - have an important influence. Although, because such relations are usually entirely hidden they must not be neglected. 

Structural system properties of, e.g., social networks are analyzed with a variety of methods, but it is often hard to identify and to describe the hidden interaction roles. In general, all established network analysis algorithms require a predefined adjacency matrix as representation of the network. Because this structure is at least measurable one can relate the change in structure to the variation of interaction roles. 

Scalar entries of an adjacency matrix can show only one single type of interaction between two elements. This means, that multiple interaction types require multiple matrices. Such matrices are considered to be layers in a multilayer network and describe one single facet or aspect of the system. All layers together represent the entire system including its natural embedding. 

Traditionally, each layer was studied independently. In order to describe emerging properties, the individual networks have to be combined. The result is a multilayer network with different link types. Because different aspects can have different influence on the systems time evolution, knowledge about the right scaling and the right weight-functions is required.
 
The majority of established network analysis algorithms can not handle multiple weights for one node or link, which means, they can not handle multilayer networks directly. Some special cases are handled by bipartite or k-partite networks. Both have the limitation that no interaction between nodes of the same type are considered.

This chapter presents a generic approach to construct multiplex- or multilayer networks from dynamic node properties. This allows calculation of dynamic correlation properties, distance and similarity measures as structural numbers are derived from measured time series data. This way, several computational procedures provide network layers with different meaning.

Measured time series data and direct accessible node properties are combined in this approach (see chapter \label{chap.ContextsensitiveRelevance}). Obvious structural relations are, e.g., page links between Wikipedia pages and temporal collaboration links between co-workers. Such information is used to define static links between nodes ( Fig.\ref{fig.ConceptFNCSA} B). Temporal correlation or dependency links describe the hidden non accessible functional layers (Fig.\ref{fig.ConceptFNCSA} F). 

The following sections describe a formal framework for network creation procedures followed by a discussion about application of this networks. 
   
\section{A Formalism for Network Reconstruction}
\label{NetReconstructionFramework}

One essential question which helps us to prepare useful networks as models of complex systems is: "How can the relevant interactions be described as network links?" Such interactions happen on multiple levels. Individual elements interact with each other. The collective behavior of a group influences an individual element (or person), but, all individual actions of many elements (or people) contribute and define the collective group behavior. Therefore, multiple link layers are combined. A combination of layers creates a network of networks (NoN) which are in general k-partite networks with multiple types of links between nodes of different types.

Link projection is required, to merge the contribution of individual link layers. Each layer represents in general one single aspect, e.g., a reaction channel or a communication channel. One has to apply a connectivity-projection-function (CPF), to calculate a single link strength value for each pair of nodes.  Such a link strength property can include and combine information from all available layers. Thus, a CPF reduces the number of dimensions of the system. The resulting network representation emphasizes one individual aspect without decoupling or disconnecting a component from the entire system.\\

Multiple layers can be defined in a hierarchy of abstraction levels. This allows microscopic and macroscopic properties to be combined in one single model. The multi-layer approach connects such isolated views within one single system view and can be handled either with statistical methods or numerically via simulations. The advantage of this approach is the combination of simulation and analysis techniques in one single theoretical and technical framework.

Nodes are represented by labeled vectors. The adjacency matrix becomes a tensor and the elements of this tensor can also be vectors instead of scalar values, e.g., if time series are taken into account. A simple summation of the link-vector components would not provide useful result. Therefore, a connectivity function CF is used to calculate the strength of a link between nodes based on their microscopic properties and on the network properties of the close neighborhood. The neighborhood of a node defines a subsystem which also influences the properties of a particular node. This approach uses a direct coupling of node and system properties which leads to a closed feedback loop. 

In order to model a system as a multi-layer network one has to select the appropriate type of measurable data (see A, B, and C in Fig. \ref{fig.ConceptFNCSA}). Depending on the characteristics of the time series data one has to select an appropriate connectivity function (see table \ref{tab11.3}). Finally it is important to define the orientation of the links. This influences the selection of the right metric and connectivity function, because not all possible CF provide information about orientation. 

Our goal is to study dynamic properties of complex systems without the need of decoupling static and dynamical properties. There are more questions which influence the model definition procedures such as:
\begin{itemize}
	\item How fast is some internal system property changing?
	\item What metric allows us to describe this changes as function of time? 
\end{itemize}
\textbf{Network construction} is the process of defining and quantifying relations between entities. Such links can be obvious, such as the relation between two people living in the same place, or hidden, such as a shared political opinion, which is not expressed in a direct way. Studies on opinion dynamics, crowd dynamics, election dynamics, and on the influence of religion on decision making processes are attracting research topics with growing interest. The article titled \textit{'Modelling Opinion Formation with Physics Tools: Call for Closer Link with Reality'} \cite{Sobkowicz2009} criticizes the lag of connection between research, created models, and application to real world problems. But more importantly, the lag of analysis on data obtained from real world. Especially the emerging amount of apparently unlimited data sources can help to change this. 

One contribution of this work is, to formalize the procedure of extracting links between interacting nodes from disjoint data sets. The formalism allows a comparison or at least a validation of comparability of obtained results. 

\textbf{Network reconstruction} is the process of calculating relations between nodes. Similarity measures are used instead of well defined linking rules. This allows indirect link definitions. The results of such calculations can later be interpreted as link strength value to quantify the relation between two entities. Before such a link can be called a link, the similarity matrix has to be transformed into an adjacency matrix. This is done by filters, with a fixed or variable threshold, probability filters, or based on structural information (see chapter \ref{significantlinks}). 

The difference between construction and re-construction is, that in a constructed network all existing links really are obvious and physically exist. A reconstructed network is generated from data, which does only indirectly describe a relation between the linked nodes. This way it is possible to find hidden links which physically do not exist. Network re-construction is a statistical method. If results are valid or not has to be validated by significance tests (see chapter \ref{significantlinks}).

Because a large variety of network (re)construction methods exist and selected parameters for each method directly influence the outcome, it is worth to define a generic network construction procedure. 

Especially in case of complex dynamic systems it is important to compare network properties quantitatively. Ideally, one would define equations of motion based on the configuration properties of the system. Structural analysis of systems with obvious relations between elements or components is already well established. If links physically exist it is simple to measure their properties. But many obvious relations exist only in the information domain. The measurement process has to be replaced by data acquisition procedures. 

One example is the relationship between parents and their children. In the beginning, before birth, a physical connection really exist. Later it is in general only the knowledge about the relation which allows us to track the family structure. Such family networks are not static. Usually three to four generations life together at a given time. According to the book of 'Guinness World Records' the maximum number of generations alive in a single family has been seven. 
%The youngest great-great-great-great-grandparent being Augusta Bunge (USA) aged 109 years 97 days, followed by her daughter aged 89, her grand-daughter aged 70, her great-grand-daughter aged 52, her great-great grand-daughter aged 33 and her great-great-great grand-daughter aged 15 on the birth of her great-great-great-great grandson on 21 January 1989.'} 
Each individual birth adds one more well defined link to the network, a child-parent relation. But at the same time all existing links can be interpreted in a new way. Each generation has it's own specific habits attitudes and believes. This requires additional interaction layers and additional link types for each different interaction role. Structural differences in this networks seem to be related with social structure of society on several interaction levels, from family, to local community, and whole countries including economy.

In many cases it is not important to track all details and all members of a family nor all possible interactions between them. Network nodes can simply be connected to a common network node which acts as a stub for a particular aspect. The result is a local star structure (see Fig. \ref{fig.StructuralMetrics}). If instead of the additional node a relation between all members is generated we would have a clique of highly connected nodes and many more links. Which approach is better? Without information about the planed analysis procedures this question can not yet be answered.

%Another example of an indirect analysis comes from classical mechanic. Acceleration can not be measured in a direct way because it has no physical representation. It is cause by a force which interacts with an object. This object changes its state or properties as a response to the interaction. We can measure the location of an object and if the location is changed we find the speed $v=ds/ dt$. An acceleration can be detected if the speed is not constant. A first step is defining a relation between the process which should be investigated. We say that the change in speed is proportional to the force. The force is what we are interested in, but it can not be measured directly. 

We measure the change rate of a system property of an object which is influenced by the process we want to study. For network studies, this means we have to identify measures which allow to quantify external influences. Especially if structural information is not direct accessible we can investigate the structure of functional networks, created from time series which are obtained by objective measurement procedures on individual system elements.

Such functional networks are useful to compare different processes even if there is no direct accessible variable to measure. Our approach is based on a comparison of the impact of external influences on a system by analyzing hidden variables, which are influenced or changed by the external process. 

In our examples we used Wikipedia, growths over time, primarily by adding new pages. Measuring the total text volume of pages seems to be a good indicator to quantify the growth or knowledge formation process. Links are very important in Wikipedia and links contribute also to the content. Page links form the backbone of the system and allow navigation and structural analysis. Beside this variables we can also track all pairs of pages which are edited or used in parallel, even if they are not linked to each other. This reveals overlapping interest in different topics even if no links exist between them at this point in time. Such non existing links can not be analyzed because they do not exist. 

%%%

We apply computational methods to measured data, which describes properties of individual elements of the system in order to create functional links. It is important to note, that properties of such functional networks depend strongly on the data  aggregation and pre-processing procedures. An intermediate result of the network reconstruction procedure is a time dependent adjacency matrix. This matrix can be analyzed in several ways. Depending on chosen computational methods one gets 'structural metrics' which represent the entire system. Structural metrics quantify the impact of an external process on the systems internal properties, even if no variables are accessible directly. Figure \ref{fig.StructuralMetrics} illustrates a process with impact on the systems' structure but no direct accessible properties. 

\label{ext.fig.StructuralMetrics} 
\input{semanpix/StructuralMetrics/imageLS}

The multi-layer network approach enforces a feedback loop to cover essential properties of complex systems. Finally, structural metrics allow time dependent analysis of system properties of large interacting systems or subsystems (even with multiple parallel channels) without a need of 'over simplification' and segregation.

Figure \ref{fig.statnet} shows two different networks created for one system. Obvious links are used to calculate an initial graph layout. Clusters are colored to highlight the community structure of the network. This static facet does not explain the dynamic properties. Therefore we show the functional network from access rate time series. The size of nodes in Fig. \ref{fig.statnet}.b indicates the node degree within the functional network. The node degree for each node can be different in each layer. Based on such structural metrics it is now possible to compare and track the system over time. Analysis of dependencies between several aspects is another use-case and will be covered in future work.

\begin{figure}[th!]
\begin{center}\includegraphics[width=6in]{semanpix/PLOSONE/images/comp3.eps}\end{center}
\caption{Comparison of the local networks regarding topic "Illuminati (book)" based on (a) direct Wikipedia links between all nodes in the local and global neighborhood (CN, LN, IWL, and GN), and (b) functional link strengths calculated from user access-rate
time series. In (a) the node colors reflect the community structure of the
underlying static network; link color corresponds to the color of the source
node and shows the link direction. The node size in both networks is
proportional to the node degree $k$. An undirected correlation network is shown
in (b). Correlation links are filtered by link strength ($\rm CC \ge 0.75$).
The layout was calculated in Gephi \cite{ICWSM09154} with the \textit{ForceAtlas2}
algorithm based on the static link network, but in (b) only the correlation
links are plotted.}
% {\bf Gleicher Massstab fuer (a) und (b)!} ===>  comp3.eps ist dazu eingefuegt
\label{fig.statnet}
\end{figure}

The layout of both networks in Fig.\ref{fig.LayerCompare} uses the geographical embedding. Some Wikipedia pages provide latitude and longitude values directly, others are linked to a page for which geo-location data is available. Structural differences between two network layers are visible. Detailed quantitative analysis of such systems requires additional algorithms. Our goal for this chapter is, to define a framework for graph reconstruction - even if structural analysis algorithms are already involved at this level, network creation is in the focus, not yet a detailed study of dependencies between individual properties. 

\label{ext.fig.LayerCompare} 
\input{semanpix/LayerCompare/imageLS}

The following equations describe a formal procedure to calculate structural metrics ${\rm S_{m}}$ for arbitrary complex systems from measured time series data. 

Raw data, usually time series ${\rm X}(t)$, obtained by a measurement procedure on system elements, or from simulation results, is processed by a time series creation function $ \mathcal{A_{\rm {TS}_{\rm {creation}}}}$. This creation function can be, e.g., a filter, or a peak detection algorithm to produce event time series (see section \ref{ES}). Results of this steps are again independent time series denoted as ${\rm X}^{'}$.

\begin{equation}
{\rm X}^{'} = \mathcal{A}_{\rm {TS}_{\rm {creation}}}\left( {\rm X}(t) \right)\\ 
\label{eq.TSCreation}
\end{equation}

Those are processed by a grouping operation. Pairs or triples of time series are typically the results of grouping. Depending on the chosen function a set of two, three or n-dimensional vectors which contain a time series in each component is generated.
${\rm X}^{'}$ is called a time series bucket and contains a set of time series with compatible properties, such as as the same metric, time resolution, length, start and end time. 
\begin{equation}
{\rm G} = \mathcal{S}_{\rm {TS}_{\rm {grouping}}}\left( {\rm X}^{'} \right)  \\ 
\label{eq.GROUP}
\end{equation}
Creation of all pairs or triples of time series can be done as part of the analysis procedure. If multiple variations are planed in during the following steps, ${\rm G}$ should be persisted. 

Now, we apply the link creation function $\mathcal{F}_{\rm {Link}_{\rm {creation}}}$ to the time series tuple bucket ${\rm G}$. Distance or similarity measures require time series pairs, whereas triples are required for dependency networks. The result of the operation is an adjacency matrix ${\rm \textbf{A}_m}$ which has to be cleaned now. The goal of this step is to remove non relevant links.

\begin{equation}
{\rm \textbf{A}_m} = \mathcal{F}_{\rm {Link}_{\rm {creation}}} \left( {\rm G}  \right) \\ 
\label{eq.LINKMATRIX}
\end{equation}

Finally, the two dimensional adjacency matrix is available for further analysis. 

\begin{equation}
{\rm S_{m}}= \mathcal{T}_{\rm {Graph}} \left( {\rm \textbf{A}_m} \right) 
\label{eq.SM}
\end{equation}

Methods from random matrix theorie \cite{Plerou2002} as well as traditional network analysis procedures can now be applied to ${\rm \textbf{A}_m}$ in order to obtain the time dependent structural metrics ${\rm S_{m}}$.

$\mathcal{A}$ is an event aggregation operation,
$\mathcal{S}$ is a time series set operation, and $\mathcal{T}$ is a graph analysis procedure which provides topological properties of a network layer.

The next sections cover the link creation phase (Phase I).The link filter phase (Phase II) is explained in the next chapter. Both phases are part of $\mathcal{F}$, the link creation operation. 

\clearpage
\newpage

\section{Reconstruction of Multilayer Networks}
\label{chap.CREATION}

A variety of network types exist, but literature \textbf{(see BOOK Estrada, BOOK Newman, BOOK Dorogostev, BOOK Havlin)} about networks and network analysis algorithms usually cover network models, but not network creation procedures. Petter Holmes published a review on new developments in temporal networks (\textbf{http://arxiv.org/pdf/1508.01303v1.pdf}). 

\textit{Rephrsase my side note in thesis Radebach 2010 on page 42.}


Arbitrary analysis algorithms can easily be written down, but a particular implementation can be difficult, especially if the network is to large to be stored one one single computer. Some algorithms are limited to a specific network representations. A simple transformation from one into another representation is required. Mathematically this is not worth to be mentioned, but the technical and economical dimensions are important here. Even if the data is not manipulated, re-partitioning of a huge data set can be very expensive. This means, demand for large scale compute resources and large memory, and long processing times are typical, even if the analysis procedure is not very exciting, from a mathematical perspective. 
Recent research and engineering work (see \cite{Google.Pregel}) and \cite{GraphX}) introduced generalizations of existing data models for graph representation (see section \ref{GraphX.PROPGRAPH.CUT}. 

Not only technical requirements defined by analysis algorithms influence the representation of networks. Also the analysis goals have an influence on network type selection. Transformations for conversion between different types are not all reversible, this means, in some cases the resulting representation is more compact and therefore more efficient, but information is lost during the transformation procedure. To solve this problem, we handle all raw data separate. As long as initial data is available for each individual network node and link, it is possible to recreate all intermediate results on demand. Two access pattern have been found very useful. First, all node properties are initially stored in a key-value store. Grouping, sorting and filtering can be applied to such data in parallel. Finally, random access to individual node properties, based on the node id is essential. For known static networks, it is very efficient to store all in-going and all out-going links in an adjacency list. This means, based on a single node key, all available information can be retrieved by only three requests per node from storage. Link properties require three keys at least. A link exist between two nodes, source and target. In general the order is relevant, but not for symmetric links. Because multiple different link types can exist between nodes, the type has to be defined by a metric name, which also will be used as a part of the key to address the link property data. For simplification we simply work with full link matrices, this requires memory to store $N^2 \cdot m$ values, where $N$ is the number of nodes and $m$ the number of metrics. In case of contextual dependency networks we have to handle triples and store $N^3 \cdot m$ link property values.  

Different real world scenarios require special network representations. Very simple models require only one type of nodes and on type of links, which can even be defined by a fixed link strength. Weighted links are already more flexible. Multiple node types lead to bi-partite or k-partite networks. Multiplex network are used to describe multiple interconnected aspects of one complex system in one modek. Table \ref{tab11.1} lists important network types according to typical applications.

The dependent properties are derived from networks at a given time. But what does the snapshot represent at this point in time? Is it information, aggregated over a period of time, is the graph static, which means there is no change or is it just static within a very short time horizont, which makes it quasi-static. Table \ref{tab11.2} compares three important gaph types with respect to time.

Table \ref{tab11.3} shows useful link creation functions $\mathcal{F}_{\rm {Link}_{\rm {creation}}}$ for documents and time series, obtained from Wikipedia. The approach is also useful for other types of complex systems, in which social interactions are influencing documents, such as messages or web pages. Because the interaction of human beings also influences economy, it seams to be reasonable to use such data also to study the interaction between human communication and the economic processes. Wikipedia pages are used as stub, which represents, e.g., financial markets. This allows to measure user interest in this specific topic. More details and preliminary results can be found in chapter \label{CASEIII}. 

\begin{table}
\begin{tabular}{|C{3cm}|L{5cm}|L{5cm}|L{2cm}|}
\hline 
Network Type& Characteristics & Applications & References \\ 
\hline 
Simple Graph, Network & Only one type of edges and vertices respectively nodes and links exists. Their properties are not time dependent, just scalar values. & The majority of graph algorithms require such a representation. & • \\ 
\hline 
k-partite Network & One type of links and $k$ types of nodes exist. &Cluster detection and identification of central nodes are used to highlight hidden roles of, e.g., the "influencers" in social networks or information shortcuts. & \textbf{CITE PAPERS about influencers and information shortcuts} \\ 
\hline 
Multiplex Network & Many types of links but only one type of nodes exists. For each link type a simple network (called layer) appears. & Analysis of multi-channel processes requires different link properties for each process. Parallel processes can now be modeled as layers.& • \\ 
\hline 
k-partite Multiplex Networks & Many types of links and multiple types of nodes exists. & The crowd, formed by people visiting a festival represents multiple different types of person which interact in different ways with each other. All this details influence the overall state of the entire system. & CITE PAPERS FROM MALTA Crowd analysis \\ 
\hline 
\end{tabular} 
\caption{\textbf{Some applications require specific network types}. Depending on use-cases, networks consist of homogeneous nodes and links or even of a heterogeneous mix of both.} 
\label{tab11.1} 
\end{table}



\begin{table}

\begin{tabular}{|C{5cm}|L{5cm}|L{5cm}|}
\hline 
Network Type& Characteristics & Formula \\ 
\hline 
\specialcellc{$G_s(t)$\\ Static (SN) }& Links and nodes exist already at a given point in time $t_s$. They do not appear or disappear during the defined time range $t=[t_s, ..., t_e]$. & $G_s(t) = G(t) = ( V(t), E(t) ) $  \\ 
\hline 
\specialcellc{$G_a(t)$\\Aggregated (AN)}& Links and nodes can appear during a defined range in time, the network contains all links and nodes which \textbf{ever exist} during the time interval $t=[t_s, ..., t_e]$& $\int_{t_s}^{t_e} \! G(t) \, \mathrm{d}t$  \\ 
\hline 

\specialcellc{ $G_t(t)$\\Temporal (TN) }& Links and nodes exist within a defined time range $t$, and if $(t_e - t_s)\rightarrow0$ it can be seen as a static network. & $G_t(t) = G(t)\vert_{t_s}$  \\ 
\hline 
\end{tabular} 
\caption{\textbf{Classification of Network types regarding existence of nodes and links}. A graph $G$ consists of a set of Vertices $V$ and a set of Edges $E$. Static networks (SN) do not change over time, but the time range can be defined according to a use-case. For very short time ranges all networks can be seen as static snapshots or temporal networks (TN). Aggregation networks (AN) represent the system during a defined period in time but not at individual times.} 
\label{tab11.2} 
\end{table}


%\begin{figure}[h]
%\includegraphics{semanpix/Fig.A/EditUndAccessBIPARTITE.eps}
%\end{figure}

\begin{table}
\begin{tabular}{|L{2.1cm}|C{4.5cm}|C{4.5cm}|C{5cm}|}
\hline 
\multirow{2}{*}{Network Type}  & \multicolumn{2}{c|}{\specialcellc{ Functional Networks\\\emph{for content related processes}}} & \multirow{2}{*}{Content Networks}\\ 
 & \emph{Consumption} & \emph{Creation} &   \\ 
\hline 
\textbf{un-directed} &\specialcellc{Pearson cross-correlation\**\\Spearman rank-correlation}& Event-Synchronization, $Q$\**&\specialcellc{n-gram co-occurrence\\term-vector cosine similarity\**}\\ 
\hline 
\textbf{directed} &\specialcellc{Granger Causality\\Contextual Dependency\**}&Event-Synchronization, $q$\**& \specialcellc{Semantic-Flow\**\\Hyperlinks}\\
\hline 
\end{tabular} 
\caption{\textbf{Link creation functions}. Different types of node interactions exist, e.g., there are  functional and structural aspects. The table presents measures which can be applied to Wikipedia time series data (generalization to other socio-technical or socio-economic systems is straight forward). The formal description in Eq. (11.3) uses the symbol $\mathcal{F}_{\rm {Link}_{\rm {creation}}} \left( {\rm G}  \right)$ as a generic placeholder for any pair- or set-function applied to time series tuples ${\rm G}$ (not $G$, which represents a graph as in table \ref{tab11.2}). The functions here allow creation of individual layers, contributing to multi-layer networks where each layer represents an individual aspect of the complex system. All marked (\**) methods were implemented as part of this work.} 
\label{tab11.3} 
\end{table}









The remaining part of this chapter shows the creation of layers for multi-layer networks, as individual separated steps. The approach in general is still using a simplification technique. Data is extracted for one individual aspect - other data is ignored. This way, the layers are still disconnected from the complex nature of the entire system. The complexity is lost only temporarily. In a final step, when all aspects - where each is represented by an individual layer - are integrated in a multi-layer network, or in a network of networks, we can see each aspect embedded within the original context.
Another approach is, e.g., comparison of different embedding scopes. By comparing the calculated structural properties for two networks with different embedding it is possible to measure the influence of contextual-variation. 

\subsection{Static Link Layer} 
The static link layer in our research context is given by the Wikipedia page links and all the inter-wiki links between pages in different languages. One can consider all existing links between arbitrary web resources as part of such a layer. In general, all hyperlinks in the HTML code, which is finally interpreted by web browsers, are used to traverse the content graph. Static means here, that this link structure defines a skeleton on which our analysis method is built on. Because this systems do not change very fast we can call them quasi-static. 

The network of hyperlinks in the world wide web is directed, also the Wiki page network. But the network of Wikipedia pages is a very specific case. It has also links to external resources. External pages can link back to Wikipedia pages as well. This way we can see Wikipedia networks as embedded global networks. The Mediawiki software allows us to request both, in-going and out-going links. For other web resources, we can only find out-going links directly. In order to know all incoming links, one would have to index the entire WWW because each resource can potentially be linked to any Wikipedia page. Indexing the entire web is not possible for individuals. For large companies it is an enormous effort to manage all that data, even for giants like Google, Microsoft, and Yahoo, which provide the largest indexes of today's and historic web pages. They offer convenient data analysis and search functionality. Offering content and collecting usage statistics at the same time is a very useful technique. Also Wikipedia could benefit from such a tight integration of content delivery with quality and usage analysis, but currently, such a system does not exist yet.   

Wiki links are not only created by humans, but also by automatic software based systems, called robots. Because links can be removed in case of deleting a page, we may have to reload the data for specific analysis steps or we have to load and store a snapshot at a given time.

The (semi)-static link layer is the foundation for our analysis and can be used as a reference for studies on other media channels. Other layers can be compared with this reference layer regarding structural properties. Furthermore, it is possible to bring obtained results into a broader context. E.g., if an unknown relation between the  backbone of a system and the functionality exists the structural properties should change if usage patterns change. Different functions can be caused by different structures or vice versa. For manually selected web resources one has to expect a self selection bias. With contextual reference data it is possible to identify dependencies and biases to support a reliable interpretation of results.

\subsection{Content based Networks}

%\textbf{START - FROM WIKIPEDIA}
%\textit{In computer science, a similarity measure or similarity function is a real-valued function that quantifies the similarity between two objects. Although no single definition of a similarity measure exists, usually similarity measures are in some sense the inverse of distance metrics: they take on large values for similar objects and either zero or a negative value for very dissimilar objects. E.g., in the context of cluster analysis, Frey and Dueck suggest defining a similarity measure}

%$s(x, y) = -||x - y||_2^2$
%where $||x - y||_2^2$ is the squared Euclidean distance.[1]

%\textit{In information retrieval, cosine similarity is a commonly used similarity measure, defined on vectors arising from the bag of words model. In machine learning, common kernel functions such as the RBF kernel can be viewed as similarity functions.[2]}
%\textbf{END - FROM WIKIPEDIA}

Content networks are formed by explicit links between documents, expressed as citation or simply by Hyperlinks. The fact, that two documents belong to the same category or are written by the same author can be interpreted as a link between both as well. A third reason for linking documents is based on similarity of the content. 

Instead of well defined explicit links we can also extract latent links from existing page content. Similarity of term vectors is measured by cosine-similarity. Text and language analysis is required therefore. Multilingual studies are rather complicated. Wikipedia supports multilingual research with its internal structure. Inter-wiki links and a variety of sub-projects for all relevant languages enable our hybrid approach, which combines explicit and implicit content networks. We can start with manually selected pages, collect more related pages with a direct link to this or from this page. From text content we switch to structural information without a need of individual translations. Neighborhood graphs provide a simple abstraction and a language sensitive method without a need for translation.

\subsubsection*{Similarity Networks and Distance Networks}

Similarity networks consist of nodes which represent documents and links between similar documents. Links are generated by calculating similarity measures. If two elements are similar to each other they get linked. If similarity $s$ has a high value the distance $d$ between both elements is short. Otherwise, if the distance is high, similarity is considered to be low and elements are not linked. If $s \in [-1 , ..., +1]$ it can not be used as a distance. In order to have a minimal distance of $d=0$ in case of non similar objects we apply the transformation: $d=(1-s)/2$.

Two text documents are equal to each other, if the same words appear in the same order in both. Such a similarity measure for texts is also useful for exact identity matching. For practical reasons, texts are not analyzed word by word, rather their term vectors or n-gram vectors are used. A term vector is a special case of an n-gram vector and contains n-grams there $n=1$. Normalization techniques such as stemming and term disambiguation are applied to a document corpus during the preparation phase to achieve more robust results.    

%The levenstein distance is useful for a comparison of words or sentences  \cite{Levenstein1965} \textbf{Levenstein1965}.

%\textbf{START - FROM WIKIPEDIA}

%\textit{In information theory and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other. It is named after Vladimir Levenshtein, who considered this distance in 1965.[1]}

%\textbf{END - FROM WIKIPEDIA}

Calculation of cosine-similarity is often applied to compare short texts or sections in documents within a large corpus for information retrieval. Simply speaking, one takes the term-vector of one document or part of it and looks for documents which are similar to or are nearby the given one. For practical reasons the original document is not used during analysis. Index and search terms together with specific logical rules lead to reasonable convenient search results and drive one of the most dominant access patterns in the WWW. 

In general, the term distance is used to define how far one has to go from one node to reach another node in a network. Note, the shortest path is a very popular measure for many applications, such as efficient routing of traffic on roads, rail networks, and communication networks. This distance is related to an existing graph and simply counts the number of path segments between nodes. A link property, such as travel time or distance in real space can be used as a weight, optionally. Here, nodes are linked to each other if they are close enough to each other in the term space. What close enough means depends on the application. Examples are radio networks, which are used by mobile phones. A "short distance link" gives a high probability of forwarding or propagation of  any kind of information within the near neighborhood in which  all nodes are linked to each other or at least to one common central node. In such a case, if the majority of the elements or even all, have a link to each other, the systems risk to collapse because of an avalanche effect is higher than in case of many isolated nodes. The mobile phone related part of a real cell-phone network shows a star structure for technical reasons, which is much less critical, but relies entirely on the availability of the central node. 

Distance networks allow a transformation of a social system into a mean-field model. For a given fixed volume of gas in an equilibrium, pressure and temperature are tracked to describe the system entirely as long as the density is constant. A high node degree of a node in a distance network indicates a high "pressure" on a node as a result of too many interactions. A system of many isolated smaller clusters where the degree $k$ is limited and small can be interpreted as one with a lower pressure. This is not exact thermodynamics, but if an plausible analogy can be found, one can probably identify precursors for critical conditions in social science or in economical contexts.

\subsubsection*{Event Co-Occurrence and Collaboration Networks}

Since metadata is also part of many documents we are able to extract creation time, time of last change, and authors. This allows us to define event-co-occurrence and collaboration networks, which are considered to be content related networks as well. Both build a bridge between content and social networks. This way, text documents can be linked indirectly to each other, e.g., by one author which contributed to multiple documents. Two authors can collaborate on one document which defines now a link between them. Many others of such relations exist. Typically, bipartite networks are used to study relations between objects of different types. One can easily eliminate one type of nodes by replacing all entities of that type by links. This approach allows access to a single subsystem of one node type only and thereby a comparison of subsystem properties. A possible negative impact of dominating structural metrics of one subsystem - caused by mixing two systems of very different properties - is eliminated by this replacement technique. 

If one person contributed to a set of documents within a short period of time, this person may be the reason for a high correlation in edit activity. If more than one person work on a set of documents, we can conclude, that they share a common interest in the topic the documents belong to. We can not clearly differentiate if this interest is an agreement or if they disagree with the content. Nor can we identify, if people are working towards the same goal or if the high activity is a result of a conflict as reported by Yasseri \textit{et al.} \cite{Yasseri2012b}. 

Beside individual spontaneous events, also series of events can be used to construct networks. In this, case we calculate the level of synchronicity for pairs of event series. Events are measured directly or the result of a transformation of raw continuous time series. Using extracted features, such as strong peaks allows a variation of the intensity  and it allows a massive reduction of data which has to be processed. More details about functional networks are provided in section \ref{ESN} about event-synchronization networks. 

\subsubsection*{Semantic Similarity and Semantic Flow Networks}
\label{sem.sim} 
%http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.403.5446&rep=rep1&type=pdf 
The third content network uses semantic similarity instead of cosine-similarity. A comprehensive survey on text similarity measures was published by Gomaa and Fahmy \cite{Gomaa2013}.	They group several algorithms in three categories: (a) String-based, (b) Corpus-based, and (c) Knowledge-based similarities and demonstrate combinations of those.

Samer and Rada \cite{Samer2011} describe \textit{Semantic Relatedness} as \textit{the task of finding and quantifying the strength of the semantic connections that exist between textual units}. Their approach is an unsupervised method for calculating semantic relatedness as semantic profiles for words. Those profiles are
extracted \textit{by using salient conceptual features gathered from
encyclopedic knowledge} such as Wikipedia. They used two different distance metrics, cosine-similarity and SOCPMI. SOCPMI is a slightly modified version of the Second Order Co-Occurrence Pointwise Mutual Information introduced by Islam and Inkpen \cite{Islam2006}. They note, that the overall performance of their approach seems to be independent from the selected distance metric. In their interpretation, a word is defined by a set of concepts which share its context and are weighted by their pointwise mutual information. 
 
%Islam, A., and Inkpen, D. 2006. Second order co-occurrence pmi
%for determining the semantic similarity of words. Proceedings of
%the International Conference on Language Resources and Evaluation
%(LREC 2006) 1033–1038.

%Semantic-Similarity_Samer2011.pdf 

%http://arxiv.org/pdf/1009.4797v2.pdf}
Masucci \textit{et al.} \cite{Masucci2011} introduced a method \textit{'to infer the directional information flow between populations whose elements are described by n-dimensional vectors of symbolic attributes'}. What they call 'n-dimensional vectors of symbolic attributes' can be compared with term- or n-gram vectors. This allows us to apply their method to text documents. They use the Jensen-Shannon divergence and the Shannon entropy, which both have a wide manifold applications in science. Beside a genetic flow network the present a semantic flow network, constructed from Wikipedia pages. 

Fig.\ref{fig.SemFlowNetSensitivity} shows a comparison of (a) cosine-similarity and (b) Jensen-Shannon distance for a variation of n-gram size in the range from one to eight. 
 
\label{ext.fig.SemFlowNetSensitivity} 
\input{semanpix/SemFlowNetSensitivity/imageLS}

For the next steps, we select an n=3 as size of n-grams for creation of a a semantic similarity network from a question and answer (QnA) system. Such a QnA system contains documents of two types, questions and answers, which can be connected to an answer by a directed explicit link. This way, we have a bipartite network with two link types. Because some questions are not answered yet, they are isolated nodes. Valid facts can be stored in such a system, even if no question is related to this fact. Also those fact-nodes would be not linked to others. The second link type is based on semantic similarity, using Eq. (\ref{eq.semsim}). Figure \ref{fig.SemanticSimilaritNetworkQnAb}.a shows two strongly connected clusters. Surprisingly, the two clusters do not represent the two node types, questions and answers. To emphasize the internal structure, all explicit links were removed and two different filter thresholds are applied in Figure \ref{fig.SemanticSimilaritNetworkQnAb}.b. This highlights clusters of related items, independent of an explicit link structure. A topic model is not required for this technique. 

%\label{ext.fig.SemanticSimilaritNetworkQnAa} 
%\input{semanpix/SemanticSimilaritNetworkQnAa/imageLS}

\label{ext.fig.SemanticSimilaritNetworkQnAb} 
\input{semanpix/SemanticSimilaritNetworkQnAb/imageLS}

Explicit links are added as a second link layer in figure \ref{fig.SemanticSimilaritNetworkQnAc}. This multi-layer approach contributes context information to existing items in a collection. Here, we are able to identify related questions, because they are linked to the same or a similar answer, or because they have a high semantic similarity to the question itself. Studying the evolution of such structures as a function of time, together with access-rate and edit-activity time-series can be used as an experimental setup to study knowledge-formation based on self-organization. This is related to a process, in which persons contribute without any assignment. If additional roles are applied, or automatic tools such as de-duplication or disambiguation procedures operate on the data beside persons a change in the structure can be expected. In this chapter we only provide a construction mechanism for mixed social media and content networks represented as a multi-layer multi network model.

\label{ext.fig.SemanticSimilaritNetworkQnAc} 
\input{semanpix/SemanticSimilaritNetworkQnAc/imageLS}

Information flow analysis requires multiple inputs. The content can be represented by documents or messages. Users are the source for information and also information consumers. This means, the set of users, which is for itself a network, is linked to the content by at least two different link types. In our data set we use access-rate time series to express the interest of Wikipedia users in content consumption and the edit-event time series to track users contribution. Based on such data, it is possible to measure the total activity. A more detailed analysis is possible if content can be categorized. There are many different approaches for categorization and classification of Wikipedia content. Those approaches can be generalized to content from arbitrary systems, also from non digital sources such as books, print media, or spoken text from TV and radio stations.  

Wikipedia provides an implicit content categorization. Therefore, a page is linked to a category page. Grouping articles by language provides a different classification schema. Even if no explicit content categorization is available, it is possible to derive a topic model from any text corpus. 

Latent Semantic Analysis (LSA) is based on Singular Value Decomposition (SVD). The goal is to identify a lower-dimensional representation of the content. LSA provides insights into large document sets, by analyzing the relationships
between the words within the documents. Therefore, a set of relevant concepts is extracted from the corpus. According to Ryza \textit{et al.} \cite{Ryza2015} such a \textit{concept consists of three attributes: a level of affinity for each document
in the corpus, a level of affinity for each term in the corpus, and an importance
score reflecting how useful the concept is in describing variance in the data set.}

Blei \textit{et al.} \cite{Blei2003} introduced an approach, called Latent Dirichlet Allocation (LDA), which is a generative probabilistic model for collections of discrete data such as text corpora. According to Blei \textit{et al.} \textit{LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document.} The resulting topic model is an $N_{topic} \times N_{doc}$ matrix in which the elements describe the level of participation of a document in each topic.

Amanico \textit{et al.} \cite{Amanico2013} proposed methods which combine semantic and structural properties of texts. The topology allows capturing stylistic features concerning authorship and text quality, they state.

Recently, "The Open-Vocabulary Approach" was proposed by Schwartz \textit{et al.} \cite{Schwartz2013}. Especially in large scale social media systems, such as Facebook, or Google+, it seems to be relevant to recognize the relation between language usage and properties of persons, which communicate with each other. They found, that an language based approach allows to distinguish people by personality, gender, and age. This illustrates, that techniques which work well in isolated systems may not lead to the same results in interconnected complex systems. 

 

\subsection{Functional Networks}
Functional networks represent relations between network nodes, which are not measurable directly. The idea behind functional network creation is, that if a property of two elements changes synchronously (even with a given time delay) one can assume that a common influencing factor which has an impact on both exists. 

Therefore, we apply an operation $\mathcal{F}_{\rm {Link}_{\rm {creation}}}$ to a tuple (or simply a pair) of time series to calculate a value, which represents the relation between the objects, from which the time series were obtained (see Eq. \ref{eq.LINKMATRIX}). This means, if access activity to Wikipedia pages is correlated we assume also a correlation in interest in both pages, so we define a link between both pages to express common interest in both at the same time. This link is a temporary link, and depends on: (a) length of sliding window, (b) filter procedure for time series, and (c) filter procedure for link strength (see chapter \ref{significantlinks}). Some correlation methods provide only a link strength, no orientation. Cyclic patterns and strong peaks can have a dominating influence on correlation results. Therefore we study the impact of strong peaks and additional quality metrics in more detail in the next chapter. 

\subsubsection{Correlation Networks From Continuous Time Series}
Interactions between systems can be described by linear response theory (see section \ref{sec.linresponse}). If events - no matter if triggered externally or internally - affect two or more elements of a system, the impact is measurable as a change in a particular property, usually at the same time or within a given time range. The time delay $\tau$ and also the changed shape or decreased intensity is caused by damping. 

Figure \ref{fig.LIN.RES.THEORY} (\textbf{fig.LIN.RES.THEORY}) shows a pair of simulated time series to illustrate an interaction between two subsystems $I$ and $F$. We compare the measured signals and apply a fit function to calculate $\tau$ and the damping factor $d$. The shape of the response function allows more detailed analysis, especially the response on different channels is accessible this way, if individual frequency ranges, called bands, are analyzed independently. One functional network layer can be created for each band. Response functions are measured in different locations on different components. We assume, that all are a response to the same external stimulation. This allows us to create a network representation of the interaction based on damping and time delay. 

Those parameters are also useful for normalization of measured raw time series. Normalization allows finally a direct comparison of time series, especially, if peak shapes and sizes vary. Nodes with a very similar "response" shape can be considered to be part of a co-occurrence network. Roggen \textit{et al.} \cite{Roggen2011} used this technique and a clustering approach to identify people which walk together in a group. They measure acceleration of individual persons and apply a clustering algorithm to obtained time series. Analysis of individual parameters like damping factors and time delay provides detailed information about each individual interaction. Cluster analysis offers information on group and system level. Both techniques should be combined to define appropriate model parameters. 

Time delay values can be used to define a distance network, which uses a link strength value to express how far away nodes are from each other or how expensive it is to traverse the link between both. This concept is also applicable to a network of states, where links exist only if a state transition is possible and the link strength represents the required activation energy to initiate the transition process.  

Our example time series (see Fig. \ref{sampleseries}) show two examples of obvious responses, measurable by daily Wikipedia access-rates (c,e). Contextual-detrending was applied to data from local neighborhood network (see section \ref{chap.ContextsensitiveRelevance} for method description). The result is shown in a time series dashboard in Fig. \ref{fig.TSDB}.

We consider functional networks as useful objects for studying the interaction properties between social networks of users and content networks. The approach can be generalized to arbitrary time series, such as climate data, financial data, or machine and sensor data. For each pair of nodes we calculate the strength of the (Pearson) cross-correlation between the 
time series of user access rates after the weekly trends have been removed
(see also \cite{PHYSA.2012}). The value of the cross-correlation coefficient
defines the strength of the
functional link between the two considered nodes. Repeating the procedure for each 
pair of nodes yields a functional network representation of the user-access rate 
cross-correlations. The calculations are also performed for temporal slices of width 
$\Delta t$ beginning at $t_0$, so that time-dependent (dynamically evolving)
functional networks are obtained. Specifically, we calculate for each pair $(i,j)$ 
of nodes:
\begin{equation}
{\rm CC}_a^{(i,j)}(t_0) = {1 \over \sigma_i \sigma_j} \left[{1 \over \Delta t} 
\sum_{t=t_0}^{t_0+\Delta t} a_i(t) a_j(t) - \left({1 \over \Delta t}
\sum_{t=t_0}^{t_0+\Delta t} a_i(t)\right) \left({1 \over \Delta t}
\sum_{t=t_0}^{t_0+\Delta t} a_j(t)\right) \right], 
\label{EQ.FCC.PEARSON}
\end{equation} 
\begin{equation}
{\rm where} \quad \sigma_i = \sqrt{{1 \over \Delta t} \sum_{t=t_0}^{t_0+\Delta t} 
a^2_i(t) - \left({1 \over \Delta t}\sum_{t=t_0}^{t_0+\Delta t} a_i(t)\right)^2}
\end{equation} 
and $\sigma_j$ accordingly. 

Similarly, editorial activity can be studied. However, since edit events are
rather sparse, event synchronization coefficients \cite{Quiroga.2002} should
replace the Pearson cross-correlation coefficients (see next section). As weak correlations
between user access-rate time series also occur randomly (because of limited
statistics), we re-normalized computed link strengths (see \textit{Palus et al.} \cite{Palus2011}).
The calculation of ${\rm CC}_a^{(i,j)}(t_0)$ was
repeated ten times for randomly shuffled time series $a_i(t)$ and $a_j(t)$ to
determine normalization factors ${\rm CC}_{sa}^{(i,j)}(t_0)$ for each pair of
nodes $(i,j)$. Finally, we calculated the \textbf{adjusted link strength} as:
\begin{equation}
l_a^{(i,j)}(t_0) = {\rm CC}_a^{(i,j)}(t_0) / {\rm CC}_{sa}^{(i,j)}(t_0).
\end{equation} 

The corresponding functional network expresses how nodes are influencing each other. Here we are interested in finding node properties, that can be derived from such a dynamic correlation network. Beside the node degree, several centrality measures and other network analysis procedures can be applied. This way, functional networks provide data which describes dynamic not direct accessible aspects of complex systems.

Figure \ref{fig.statnet} in the beginning of this chapter compares the static view of a  local neighborhood networks for one Wikipedia page with the corresponding functional network, based on (a) direct Wikipedia page links and (b) functional links, calculated from user access-rate time series. One can clearly see that the most relevant (most
connected, central) nodes are different in both representations of exactly
the same Wikipedia articles. Centrality and PageRank were calculated for each node. Resulting values are the components in a relevance vector, one for each node. Such a multidimensional ranking combines individual aspects which are not visible in one single network only. 

Alternatively, we tested a more robust method, called 'Normalized Link Strength', which was also used by Berezin \textit{et al.} \cite{Berezin2011}. Instead of calculation of the time delay variance, we introduce a quality metric to measures the impact of strong narrow peaks in the next chapter. We calculate the \textbf{normalized link strength} as:

\begin{equation}
l_n^{(i,j)}(t_0) =  \frac{ \langle {\rm CC}_a^{(i,j)}(t_0) \rangle - max( {\rm CC}_{a}^{(i,j)}(t_0) ) }{ \sigma ( {\rm CC}_a^{(i,j)}(t_0) ) }.
\end{equation} 


\textbf{Here I have to compare the link strength distribution for simple pearson-correlation, shuffled, and normalized distribution.}

 
 
 
Correlation networks with link strengths calculated from Pearson correlation are un-directed networks. Even if one time series leads the other significantly, this information is not represented in the link definition as it is just a scalar value. If the correlation function with time delay $F_{CC}(\tau)$ time is used instead, one can find the characteristic delay between both processes by looking for the maximum in the correlation function. Based on this delay it is possible to extract information about the orientation or timely order, which is than translated into a direction. Berezin \textit{et al.} interpreted the variance of all $\tau$ values as criteria to separate real links from random links. 

David Tam \cite{Tam2007} proofed, that a computational function performed by a time-delayed neural network which implements Hebbian associative learning-rules computes the equivalent of the cross-correlation function of time series. He shows the relation between the correlation coefficients and the trained connection-weights. This concept allows very efficient implementations in hardware, which can provide real time results, opposed to our current approach, which relies entirely on pre-aggregated time series data.



%The link strength laij is calculated as the Pearson cross-correlation between time i and j series without time delay after the weekly trends had been removed. 
%For a static analysis the length of the time series is about 8400 hours, and for a more dynamic view we chop the whole 
%time series into segments with a length of 720 hours (30 days), see [4] for a related approach used for the reconstruction
% of climate networks. 
%The event-synchronisation based link strength $Q_{ij}$ is calculated like it is was done by Quiroga in the 
%context of EEG analysis [5]. 




\subsubsection{Event-Synchronization Networks}
\label{ESN} 
Rather than just a correlation strength $Q$ (see Eq.\ref{eqn:ES_str}), the event synchronization method also provides information about the direction, $q$ (see Eq.\ref{eqn:ES_del}) or the order in time. This way, it is easy to see if an external process leads to significant features in one time series early, before in the other time series. One has to be careful here, this information does not allow a conclusion about causation or causal dependencies between the elements, from which data was obtained.

Event time series are used by Malik \textit{et al.} \cite{Malik2012} (see page 975 figure 3). We adopted their approach and create a functional network layer to represent user contributions to Wikipedia in form of editorial activity. 

An alternative metric, to measure a distance between two spike trains (which are sparse event series) is presented by Houghton and Kreuz \cite{HoughtonKreuz2012a} in their paper \textit{'On the efficient calculation of van Rossum distances'}. 
%\textbf{[15] Houghton C, Kreuz T:} \textbf{On the efficient calculation of van Rossum distances. Network: Computation in Neural Systems 23, 48 (2012) [PDF].}
Like in our case, many applications require a matrix of distances between all the spike trains in a set. Furthermore, the calculation of a multi-neuron distance between two populations of spike trains is a rather expensive approach. They present an algorithm to render these calculation less computationally expensive, making the complexity linear in the number of spikes rather than quadratic. 

%%ETOSHA.Ref http://wwwold.fi.isc.cnr.it/users/thomas.kreuz/research/synchrony.html

\subsubsection{Dependency Networks and the Context Cohesive Force}
\label{adopt.ICF}  

Opgen-Rhein and Strimmer published a method to generate a causation network for high-dimensional plant gene expression data \cite{Opgen-Rhein2007}. They describe partial correlation as the correlation that remains after regressing the effect of other variables away. Beside the correlation they also take the variance of the signals into account and define a link direction based on the most exogeneous variable. Such a directed link only exists, if the logarithm of the two variances is significantly different from zero, which means the variances are different and allow the definition of a direction. Finally, they create a directed acyclic graph, which is a subgraph of the undirected correlation network.  																																					
%
%
%%ETOSHA.DOC https://docs.google.com/a/cloudera.com/document/d/1g4im1ynhs5LYOlBRVgJPgXXmLqT0ZJk4eQOQl_3OhbQ/edit#heading=h.u9got7f8su9d

Another method of reconstruction of dependency networks uses triples of nodes to calculate a link strength between two nodes in the presence of a third. We define the \textit{Context Cohesive Force} (CCF), which is a generalization of the \textit{Index Cohesive Force} (ICF). ICF, introduced by Kenett \textit{et al.} \cite{Kenett2011}, was used for network re-construction from financial time series. Here, we use it as new approach to study interlinked social communication networks and social content networks together, in the presence of other systems, in which both are embedded in.

%Wikipedia combines two typical characteristics (it is a content and a communication network) in one system as it stores content of a more durable nature and allows discussions about that content. The text on Wikipedia pages forms the backbone of the system. This content is created as a result of a self organized consensus driven open public procedure. During this procedure we find interactions between editors. Such interactivity is manifested in discussions which are related to articles and stored on separate pages. In our current study we did not use discussion pages, but event time series, which represent the editorial process.
 
%%ETOSHA.NOTE http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0058109

Consider two systems A and B to be bi-directional coupled. They interact with each other and consist of elements e$_A$ and e$_B$. For individual element we select one property, in case of stock market analysis, e.g., the log of daily returns (or log of the daily price differences) and the hourly access activity in case of Wikipedia analysis. 

This random variable is measured at a given sampling rate. We have to choose a length for a sliding window in order to generate time dependent results. Time resolution and the length of interval overlap are specific properties of the analysis scope and depend on the type of analysis. 

As financial data is available on a daily base, we use also daily access-rate data to be compatible. Even if intra-day trading data would be available, it would not contribute much more useful information, because we use the hourly Wikipedia access rate time series. 

Our goal is to measure the influence of system B on internal correlations (intra correlation) of system A. Therefore, the average intra-correlations $\langle {\rm CC}_a^{(i,j)}(t_0) \rangle$ are calculated for all pairs of nodes in system A using Eq. \ref{eq.LINKMATRIX}. We avoid self loops and because the cross-correlation function is symmetric, we calculate the correlation strength ${\rm CC}_a^{(i,j)}(t_0)$ only for time series pairs $a_{i,j}$ there $i > j$.

According to \cite{Kenett2011} the partial correlation $\rho(i,j|m)$ between Wikipedia pages $i$ and $j$ in the context of a mediation Wikipedia page $m$ (this can also be a category site, which represents system B or a totally different metric obtained from a related system) is calculated as:

\begin{equation}
\rho(i,j|m) = \frac{C(i,j)-C(i,m)C(j,m)}{\sqrt{(1-C^{2}(i,m))(1-C^{2}(j,m)))}}
\end{equation}

where $C(i,j)$ is ${\rm CC}_a^{(i,j)}(t_0)$ for simplification.
Now, we can interpret $\rho(i,j|m)$ as the residual correlation between the pages $i$ and $j$ which does not include the correlation between both and the page $m$. 

Previously, the index cohesive force (ICF) for stock prices grouped by stock index was defined by \cite{Kenett2011} as the ratio of raw and residual correlations. In order to generalize this idea, we use contextual neighborhood networks around Wikipedia pages. This allows us to apply the method to semantic concepts grouped by topics or semantic categories. 

We define the Context Cohesive Force (CCF) as the ratio of the average pair correlation and the average partial correlations: 

 
\begin{equation}
{\rm CCF}(t)=\frac{<C(i,j)>_t }{<\rho(i,j|m)>_t}  
\label{eq.CCF} 
\end{equation}

In comparison to the observed effect the index has on stock correlations we analyze the correlations between the Wikipedia page which represents the stocks index and the companies pages access-rate time series. Kenett \textit{et al.} found, that larger changes of the index results in higher stock correlations. Based on those findings we conclude with the following hypothesis: If movements in stock markets cause an increase of interest in financial topics in Wikipedia one would measure (a) an increase of intra-wiki correlations between pages, related to a given market and (b) an increase in partial correlations between the stock market data and the Wikipedia access-rate data for related pages. We discuss our preliminary results in chapter \label{RESULTS}. 


 
\subsubsection{Correlation Networks from Non-stationary Time Series}

%\textbf{http://arxiv.org/abs/1310.3984}
%\textit{In this short report, we investigate the ability of the DCCA coefficient to measure correlation level between non-stationary series. Based on a wide Monte Carlo simulation study, we show that the DCCA coefficient can estimate the correlation coefficient accurately regardless the strength of non-stationarity (measured by the fractional differencing parameter d). For a comparison, we also report the results for the standard Pearson's correlation coefficient. The DCCA coefficient dominates the Pearson's coefficient for non-stationary series.}

An increasing demand for alternative approaches, which can handle non-stationary time series as well, can be explained easily. Since more and more data becomes available, but data is collected in sometimes unstable or non-stationary environments, one can not apply Pearson correlation, because the results are not reliable in such conditions. The influence of extreme events and outliers which are characteristic properties of real-world data sets, especially from socials media systems has to be eliminated or addressed in a specific way. The first approach, is used to concentrate on extreme events and outliers only. The time series are transformed into event-time time series by event detection algorithms. The event-synchronization method can now be applied. 

Application of random matrix theory (RMT) is a second alternative. According to Podobnik \textit{et al.} \cite{Podobnik2010} RMT is used to analyze time-lag cross correlations in complex systems. They address the question whether these cross-correlations exhibit power-law scale-invariant properties. Therefore they apply time-lag RMT (TLRMT) to time series from finance, physiology, and genomics. They found long range correlations in the finance data set by comparing the calculated eigenvalues with expected eigenvalues from random matrices. This way, they could demonstrate a different properties for return and volatility\footnote{DEFINE return and volatility !!!}.\\

$https://ideas.repec.org/a/eee/phsmap/v391y2012i1p187-208.html$
\cite{Leonidas2012}
\textit{Using the eigenvalues and eigenvectors of correlations matrices of some of the main financial market indices in the world, we show that high volatility of markets is directly linked with strong correlations between them. This means that markets tend to behave as one during great crashes. In order to do so, we investigate financial market crises that occurred in the years 1987 (Black Monday), 1998 (Russian crisis), 2001 (Burst of the dot-com bubble and September 11), and 2008 (Subprime Mortgage Crisis), which mark some of the largest downturns of financial markets in the last three decades.}\\

  
%  http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.100.084102
Podobnik and Stanley \cite{Podobnik2008} introduced a method, called detrended cross-correlation analysis (DCCA), which is a generalization of detrended fluctuation analysis (DFA) and is based on detrended covariance. Investigation of power-law cross correlations between pairs of different non-stationary time series is the purpose of this method. The DCCA coefficient 
$\rho_{DCCA}$ was introduced by Zebende \cite{Zebende2013} and is based on detrended cross-correlation analysis. Kristoufek et al. \cite{Kristoufek2014} also conclude, that the DCCA coefficient can be used to measure correlation between non-stationary time series.


% http://arxiv.org/pdf/1310.3984.pdf

% http://www.sciencedirect.com/science/article/pii/S037843711400079X

Because the DCCA coefficient can be used for non-stationary series it allows analysis of raw time series, even if they contain trends or extreme events. Most importantly, the DCCA  coefficient provides information about correlations at different scales. The Pearson correlation coefficient can be calculated for time series of different length and at different times, but this should not be confounded with analysis on different time scales. 

% http://www.sciencedirect.com/science/article/pii/S0378437113000253


% http://www.researchgate.net/post/what_are_the_differences_between_detrended_cross-correlation_coefficientDCCA_coefficinet_and_Pearson_correlation_coefficient

% [17] Podobnik, B., D. Horvatic, A. Petersen, and H. E. Stanley (2009). Cross-correlations between volume change and price change. PNAS 106(52), 22079–22084.
% [18] Podobnik, B., Z.-Q. Jiang, W.-X. Zhou, and H. E. Stanley (2011). Statistical tests for power-law cross-correlated processes. Physical Review E 84, 066118.

% [19] Podobnik, B. and H. Stanley (2008). Detrended cross-correlation analysis: A new method for analyzing two nonstationary time series. Physical Review Letters 100, 084102.
% http://polymer.bu.edu/hes/articles/ps08.pdf

The long range cross-coefficient $\rho_{ij(s)}$ measures the correlation between two series on a specific scale. In order to analyze the scaling behavior the long range correlation exponent $\lambda$ is calculated by linear regression in the log-log representation of $\rho_{ij(s)}$. Gang-Jin \textit{et al.}  \cite{GangJin2013} applied this method to data from foreign exchange market (FX). They created a series of Minimum Spanning trees (MST) from financial time series (log-return of daily FX rates of 44 major currencies in the period of 2007–2012). Instead of using a time resolved analysis procedure, they study the properties on different time scales and identified different topological properties in functional networks, created for multiple time scales.





%\subsection{More Examples}
%
%
%\subsubsection{Granger Causality Graphs}
%
%\cite{Amblard2011} \textbf{1002.1446v1.pdf}
%\textit{Directed information theory deals with communication channels with feedback. When applied
%to networks, a natural extension based on causal conditioning is needed. We show here that
%measures built from directed information theory in networks can be used to assess Granger causality
%graphs of stochastic processes. We show that directed information theory includes measures such
%as the transfer entropy, and that it is the adequate information theoretic framework needed for
%neuroscience applications, such as connectivity inference problems.}
%
%\cite{Caraiani2013}
%\textit{We construct complex networks based on GDP data from two data sets on G7 and OECD economies. Besides the well-known correlation-based networks, we also use a specific tool for presenting causality in economics, the Granger causality. We consider different filtering methods to derive the stationary component of the GDP series for each of the countries in the samples. The networks were found to be sensitive to the detrending method. While the correlation networks provide information on comovement between the national economies, the Granger causality networks can better predict fluctuations in countries’ GDP. By using them, we can obtain directed networks allows us to determine the relative influence of different countries on the global economy network. The US appears as the key player for both the G7 and OECD samples.}
%
%\subsubsection{Bipartite Networks}
% \label{BiPaNet} 
%\textit{(1) Selection of Data sets}\\
%\textbf{Group A} - DAX: 30 German companies listed in the DAX.
%\textbf{Group B} - S\& P500: 500 American companies listed in the SS\& P500 index.
%We use the daily trading volume ($TV$), log return of prices ($LRP$) and $abs(LRP)$ calculated from closing price. Data is loaded 
%from Yahoo! Financial Services (see \cite{YahooFinancialServices}).
%
%\textit{(2) Selection and Filtering of corresponding Wikipedia pages}\\
%One has to lookup the Wikipedia pages with a name equal to a company name determined in step one and in some cases the list pages, if such are available are helpful. In some cases this step is manual work which means, it is possible to have wrong names or wrong character encodings which can break the data collection procedures.
%
%\textit{(3) Extraction of the individual raw data series according to created lists from step one and two.}\\
%
%\textit{(4) Extraction of overlapping time intervals which are available in both data sets.}\\
%
%\textit{(5) Test distribution of values.}\\
%For each of the collected time series we calculate the Shapiro-Wilk-Test. This is necessary as we can not use the pearson correlation method for time series if values are not from a normal distribution.
%
%\textit{(6) Calculation of cross-correlation functions.}\\
%For all relevant pairs of time series we calculate a cross-correlation function which consists of cross correlation coefficients for a delay value $\tau$ in the range $-5 < \tau < 5$ which represents a shift of a full trading week. To create a dynamical view we use non overlapping time windows of different lengths to study the dependence of correlation strengths on the length of the selected time scale.
%
%\textbf{Allignment of trading data has to be analyzed in more detail. Is it a really good idea to cut the non trading days? Shouldn't we fill the gaps with the last available value?
%}
%\textit{(7) Calculation of average correlation strength - INTRA CORRELATION.}\\
%\textbf{a)} based on all obtained cross-correlation functions\\
%\textbf{b)} for each delay based on that values which have the maximum correlation value at this special delay.\\
%
%\textit{(8) Calculation of average correlation strength - INTER CORRELATION.}\\
%
%\textit{(9) Calculation of dependency networks - META CORRELATION.}\\
%
%\textit{(10) Filter the results from step seven and step eight for time series with an Shapiro-Wilk-Test value exceeding a certain threshold}
%
%\textit{(9) Filter the results from step seven and step eight for time series with an Shapiro-Wilk-Test value exceeding a certain threshold}
%




\section{From Time Series to Dynamic System Properties}
Each network reconstruction step provides an adjacency matrix, one for each time interval. A macroscopic description of the system is based on structural properties, such as traditional network measures, or one of the many new algorithms which were developed recently. The new algorithms allow us to handle node and edge property vectors. We are not longer limited to scalar values for each node and edge \textbf{(see WEIGHTED Measures Donges; MultiplexPagerank; All-Around nodes)}. 

\subsection{Time Dependent Multivariate Network Metrics }
 
%\cite{WeightedNodes} Weighted Nodes and weighted measures ...

%\cite{Multiplex PageRank} Multiplex Page Rank ...
\cite{Halu2013}

% 
The PageRank algorithm assigns a scalar value to each node of a static graph. Static means in this case, that during the iterative calculation of the final values b, e.g.,  the power iteration method (see \cite{Berberich04} Eq. 5) the link structure and thus the transition probabilities are not changed. Weighted transition and random jump probabilities can take the freshness and activity of webpages ito account. Berberich et al. introduce the T-Rank approach, as an extension of the widely accepted PageRank. This new approach is an extension of the Markov chain model and allows a time aware authorit score for nodes to express their relevance compared to others. The problem of this method is the lag of required metadata to calculate freshness and activity. During this work we could find, that exactly this kind of information is available on Wikipedia. This sounds really easy, but the technical requierements are huge. One has to handle the full Wikipedia network (more than 10.000.000 pages) as well as slices of the access activity and edit activity. The slices represent the time range, access rates the activity of the pages and editorial activity expresses the freshness of an article. Based on the method of local neighborhood graphs, which is developed in this work, it is possible to present the temporal relevance of a particular node including the structural properties. Our current approach is based on node similarity, no matter how the underlying network structure looks like. The T-Rank based approach does not take the activity similarity into account, but reflects the existing network structure. Both methods appear as complementary approaches for time dependent analysis of complex networks. 

\cite{Berberich04}

%\cite{All-Around} All around nodes ... (Multidimensional metric for ranking)
\cite{Hou2012}
% Identifying the most influential nodes in complex networks provides a strong basis for understanding spreading dynamics and ensuring more efficient spread of information. Due to the heterogeneous degree distribution, we observe that current centrality measures are correlated in their results of nodes ranking. This paper introduces the concept of all-around nodes, which act like all-around players with good performance in combined metrics. Then, an all-around distance is presented for quantifying the influence of nodes. The experimental results of susceptible-infectious-recovered (SIR) dynamics suggest that the proposed all-around distance can act as a more accurate, stable indicator of influential nodes. 


Temporal networks represent the system at a specific time, usually in the middle of the interval, defined by the length of the series. Beside the time resolved approach we can use the DCCA based method, which allow a time-scale dependent analysis.

Now we want to identify and quantify changes of system properties to identify a law, which describes the time evolution of the underlying system. A large variety of  different properties can be analyzed and currently nobody knows knows for sure, which property depends on which other. A complex systems has some intrinsic structure which is in the focus of our research - therefore a fundamentally new approach seems to be required. It is important to integrated data driven methods, such as machine learning, with simulation techniques, and analytic models. This allows us to study the details first, to combine those and to validate under several conditions, before a general conclusion can be drawn, based on all those observations and results from different disciplines. 

%No matter what kind of dependency is assumed, if the analyzed property is analyzed as a function of time or time scale, one has to figure out appropriate measures and representations of the graph.

A detailed topological analysis of the temporal networks is the next step after network reconstruction. Beside simple descriptive statistics of node and link properties it is interesting to find out more relations between the properties to which one has now access to. Especially time scale and speed of the studied process are important for choosing the right analysis intervals. In the beginning, the inherent time scale of the process one analyses is not known. A time-scale dependent analysis using DCCA could shed light in and help to identify the right time resolution parameters. 

Because the link strength can be either a scalar value or a vector - in case of multiple layers each layer contributes one dimension to such a link vector - one has to choose appropriate network measures. Here we call those \textit{'multidimensional topological measures'}. Especially in case of Multilayer Networks (MN) it is essential to apply modified variants of established algorithms, such as the Multiplex PageRank introduced by Arda \textit{et al.} \cite{Arda2013}. Weighted network measures to linear and nonlinear correlation networks were developed and applied by Donges \textit{et al.} \cite{Donges2009, Donges2009a, Donges2011}, in order to take the influences of multiple sensitive measures into account. Another example for Multilayer network analysis is presented by Cui \textit{et al.} \cite{Cui2010}. They could not directly use the PageRank algorithm to predict importance
of authors or papers. Because the number of references of a paper is not the same as the number 
of out links of a web page used in the original method, they modify the PageRank algorithm. In their case, it is not the nature of a link property, which requires such a modification, but the entire model they use to study the probability of link creation.

A systematic comparison of network properties can be based on matrix theory entirely. Random matrix theory provides a sophisticated framework therefore, but as the focus of this work is not interpretation of the network properties, rather than preparing the network representation at several times and different scales we do not investigate those details here.

\subsection{Outlook: A Time Evolution Operator}
%\textbf{What is the role of our matrices / layers in regards to Hamiltonian theory?}
We use measurable system properties to define configuration vectors. Time evolution of the system can be studied, if equations of motion are known. In case of arbitrary systems it is not possible to apply the Hamiltonian formalism directly because too many observable exist, which can not be seen as independent without investigation. It seems to be possible, to learn more about the systems dynamics, if we investigate the time evolution based on properties of correlation matrices and the transformation matrices. 

This approach is based on the idea of quantum mechanical states, defined by state vectors. The systems state is defined by configurations, in our case by an adjacency matrix $A_m (t_i)$. In each time step an evolution operator $E$ transforms $A_m (t_i)$ into $A_m (t_{i+1})$. If the temporary networks allow an extraction of $E$ - if $det(A_m (t_i) = 0$ - we can interprete $E$ as time evolution operator. Otherwise we still can use an additive model, and calculate $D(t_{I}) = A_m (t_{i+1}) - A_m (t_{i})$. Structural properties of $D$ allow an analysis regarding stability of structural properties and shows if the process, represented by the functional network is stationary. This way, we can quantify inherent unknown and unaccessible properties. 

\section{Discussion}

Essentially one can differentiate between node properties (such as degree $k$, page rank $r_p$, or centrality $c_N$) and global properties which represent the entire system. How do those properties depend on each other? This question is related to a huge class of unsolved problems. A scalable network analysis framework, which integrates date from several sources in a robust reconstruction procedure, and efficient simulation techniques is a critical factor. Finally, if we want to learn more about the impact of so called influencing nodes, we must learn how the influence which seems to be obvious is really represented and what phenomenon causes the dependencies which sometimes can be identified but not yet explained. Machine learning techniques are very successful in several solutions. They give a probability for certain events and can help in critical and also trivial decision processes. Those algorithms have no explanatory power. They do not identify the driving forces behind several processes. Analysis of the time evolution of the structure of complex systems and of the the evolution operator might be an appropriate tool therefore. 

Several approaches for network reconstruction methods for individual functional layers were presented in this chapter. Two aspects should be highlighted again, before this chapter closes with a general warning, formulated by Keogh and Lin \cite{Keogh2005}. According to their article, titled \textit{'Clustering of Time Series Subsequences is Meaningless: Implications for Previous and Future Research'}, it is meaningless to interpret clusters obtained from time series snippets. A network reconstruction method based on sliding windows uses a comparable metric to define a distance, but a MST and a PMFG are not clustering algorithms, which were studied by Keogh \textit{et al.}. The impact on this level has to be analyzed in the future. A general approach is already available. For networks, created with the DCCA method, one can compare the structural properties on different scales without shortening the time series. This can be used as  consistency check and for comparison with results obtained from sliding windows of different length.

Many different network types exist. They can be analyzed by a variety of network measures. Many studies show a very specific network which stands for a particular property or aspect of the system they study. Complementary or even overlapping alternative representations of aspects within the same system can also be analyzed. Therefore, different network types have to be combined. In our case we use construction methods, and reconstruction methods, appropriate for the available data.

We have seen rather static content based networks and time dependent networks which describe the evolution of the system and the moving parts. Time resolved analysis allows creation of snapshots, which represent the system at a given point in time. Many of such snapshots provide the data for time dependent structural measures of the complex system.

%Traditionally, network algorithms are applied to individual networks, this allows us to study individual snapshot with the same techniques. An integrated view emerges if miltiplex networks are considered (SEE PAPER MULTIPLEX PageRank).
 
%WHAT is there main message and what impact has it on our work?
The way how time dependent functional networks are created is somehow related to the critizised procedure of time series clustering. Keogh and Lin \cite{Keogh2005} write: 
\textit{"Given the recent explosion of interest in streaming data and online algorithms, clustering of time-series subsequences, extracted via a sliding window, has received much attention".}
They make claim, that \textit{clustering of time-series subsequences is meaningless} and \textit{clusters extracted from these time series are forced to obey a certain constraint that is pathologically unlikely to be satisfied by any dataset, and because of this, the clusters extracted by any clustering algorithm are essentially random}. The new approach they propose is on based on the concept of time-series motifs.

Since network reconstruction is not a clustering approach, rather one individual part of a more complex procedure, which allows clustering based on the obtained network data in a second step. Our own simple experiment - conducted before we were aware of the work from Keogh and Lin - showed, that the clusters found in the networks were characterized by specific motifs in the time series such as peaks and peak sequences. We did not apply a sliding window technique but we were able to differentiate or to isolate individual phenomena which appeared at different points in time in long time series. 

With regard to Keogh and Lin it seems to be important to study the impact of their findings on presented network construction and reconstruction procedures in the future. Of special importance is also, if different time series based methods show different properties, dependent on the sliding window technique. Regarding the link strength distributions we can already conclude, that transformations like filtering, detrending, and the logarithm function change the intermediate results - the link strength distribution of temporal network - now it is important, to study the relation between those transformations and the final topological properties. 
 
\clearpage
\newpage



\chapter{Identification of Significant Correlation Links}

\label{significantlinks} 
\label{IdentifySignificantLinks}

%%ETOSHA.NOTE link back to chapter \section{Multivariate Time Series Analysis}

%%ETOSHA.TODO Mention the Random Matrix Approach from:	slides-py538kenett3mar.pdf

In many cases, the links between network nodes are well defined and can be measured or observed directly. For such obvious links it is straight forward to analyze the topological properties of the corresponding networks. Because those metrics, which describe the relations between objects, can also be influenced by the network structure, it is even harder to identify the relevant links - especially if they were created by network reconstruction methods as described in the previous chapter. 

Correlation networks and dependency networks have been used recently to describe emergence of extreme events in earths climate system, such as the El Ni\~njo \cite{Berezin2011, Gozolchiani2011} and the interdependence of components within the global economy \cite{Kenett2012}. Initially, such networks are complete graphs and consist of a weighted adjacency matrix. Before traditional network analysis algorithms can be applied to such weighted networks, one has to identify the significant and therefore relevant links. An alternative is to apply weighted network measures as introduced by Wiedermann \textit{et al.} \cite{Wiedermann2013}.

Usually it is not clear, how strong a selected measure - correlation measure or similarity measure for link creation - is influenced by the structure of the underlying obvious network. Even the topology of functional networks may depend on this obvious structure. This can be interpreted as a disturbing bias, or even as the  driving force, depending on which properties of a complex system should be studied. 

How stable are the results in the presence of external influences and intrinsic changes? And what impact do different link creation methods and filter methods have? It is important to verify, if the applied methods have a direct influence on the selected topology measure. Furthermore, it is of a high relevance to know how stable the calculated link strength distributions are over time. 

In some cases, such as in social media analysis, it is not useful to separate only the strongest links, because the weak links can stand for important characteristics of the system too. In this case one might merge the results from two or more filter approaches appropriately.  

In this chapter we describe a new approach to identify relevant links, based on two quality metrics in addition to the well known normalized link strength calculation procedure. One additional link property is related to the degree of randomness and the second is related to the shape of the calculated correlation function.

Furthermore, we investigate a class of algorithms, which allows filtering of fully connected networks. The goal hereby is, to obtain the most meaningful information. A very simple approach is based on a static threshold. Creating the Minimum Spanning Tree (MST) is another widely used method. A common algorithm to calculate the MST is the Kruskal Algorithm \cite{KruskalALGORITHM}. Many other authors applied this algorithm to extract informative sub-graphs from reconstructed complete networks. Because the MST is just a tree, it is not possible nor useful to apply algorithms like motive statistics to the resulting sub-graph. The Planar Maximally Filtered Graph (PMFG) was introduced by \cite{Tumminello2005} to overcome this limitation. A PMFG is another sub-graph which retains more structural information than the MST. Both types of sub-graphs can be seen as structural link filters and both are parameter-less. Because of this good properties they were used so often recently. 

Both methods are based on the assumption, that the highest correlation values are the most relevant in a system. One can also invert this concept to calculate the Maximum Spanning Tree by negating the weights for each edge and applying Kruskal's algorithm (see \cite{Pemmaraju2003} and \cite{WOLFRAM.MST}).
But this assumption is not true in general, especially in case of correlation properties it turned out that a relative measure provides a more realistic view. Beside this, also weak links can have a well pronounced sharp correlation peak. How those links can be separated from the noise, which is represented by the huge amount of weak links with no specific peak is shown in this chapter. 

\section{Critique on Existing Approaches}
% https://pantherfile.uwm.edu/aatsonis/www/publications/2008-06_Tsonis-AA_TopologyandPredictabilityofElNinoandLaNinaNetworks-2.pdf
Defining link strengths between not obviously linked elements is not new. Modifications to the similarity measure are proposed and validated, e.g., Tsonis and Swanson \cite{PhysRevLett.100.228502}, constructed 
networks from measured surface temperature for El Ni\~no and for La Ni\~na years in their paper from 2008. They investigated topological properties of that correlation networks and found, that in the presence of El Ni\~no, the network has significantly fewer links, lower clustering coefficient, and lower characteristic path length.

This highlights a difference in both networks: the El Ni\~no network is less communicative and less stable than the El Ni\~na network. They write: \textit{ A pair is considered as connected if the absolute
value of their cross correlation $r >= 0.5$. This criterion
is based on parametric and nonparametric significance
tests. According to the t test with $N = 60$, a value of $r =
0.5$ is statistically significant above the $99\%$ level.}

Furthermore, in a side note they state: \textit{The choice of $r = 0.5$, while it guarantees statistical significance, is somewhat arbitrary. We find that while
other values might affect the connectivity structure of the
network, the effect of different correlation thresholds is
negligible on the conclusions reached in this} (in their) \textit{study}.

A static link strength threshold might easily be defined for one single network based on the shape of the probability density function of the link strength distribution or based on a predefined confidence level. According to Berezin \textit{et al.} \cite{Berezin2011} the goal of filtering the link strength data is, to separate the set $L_P$ (all links caused by real physical dependence) from $L_N$ (just random correlations or noise). Both types of links are part of the set $L = L_N \cup  L_P $. Berezin \textit{et al.} worked with a confidence level of $98\%$. Depending on the distribution of the link strength values a resulting filter threshold is calculated per time interval and per region, in order to have 2$\%$ of links with the highest calculated link strength in set $L_p$. In order to define a reliable threshold, they plot two factors: (a) link time delay variation $STD(T_{l,r})$ and average link strength $\overline{W}(l,r)$ and find a rather stable shape in the 2D histogram which illustrates a crossover between two regimes. They expand the one dimensional problem to a second dimension. This way they can take a second indicator into account. Time delay variation is used beside the average link strength, to classify links, or candidates for significant links.

They report, that the qualitative behavior is consistent across different regions on the globe, but not constant everywhere. The threshold also varies over time. They base their conclusion and threshold selection on an increased sensitivity found around the crossover region.

In case of a growing social media systems, such as Wikipedia, it is not possible to apply this method directly. A climate network consists of a constant number of nodes. All nodes have a well defined position and thus also fixed distances to each other. Available climate data time series are longer than 20.000 data points (daily values for 58 years). The research focus is on climate changes, but the model system has rather stable properties. This is not the case for the Wikipedia page networks.

Even if the number of relevant objects is constant, such as a group of selected cities, or pages about companies and products (in the context of economical analysis, e.g., for globally interwoven financial markets), the number of pages in their neighborhood, and thus the system size are not stable. The number of user, which influence the system is growing and user activity shows clear seasonal patterns. Therefore it is important to apply more robust methods. They must be robust regarding all those variable boundary conditions and normalization procedures, which stabilize the measured data before the time dependent analysis is applied.

In this work I created correlation networks based on a new concept. Instead of only the Pearson correlation or the normalized link strength, obtained from a correlation function, I use two additional measures as quality metrics. Furthermore I developed a procedure to identify the local neighborhoods to implement a contextual detrending within the semantic neighborhood of the content network before correlation analysis is applied.

Especially if a structural analysis is applied to the resulting time dependent networks, a fixed filter parameter, based on an absolute link strength is not a good solution. 
%In this case one should use a well defined confidence interval as selection criteria. 
Creation of MST and PMFG do not require additional parameters. All three methods have one negative property in common, they completely ignore the weak ties. Systematically ignoring the weak ties leads to information loss. 

A fixed filter threshold should not be used for time dependent studies without prior normalization of the link strength values. Normalizing the values of all time ranges to a global maximum (highest value ever seen in a particular study) or even the local maximum (which is specific for each time range) can help to solve this problem. In this case it is even more important to specify the normalization procedures, because they have a major impact on final results. Initially, we use the same approach as described by Berezin \textit{et al}. \cite{Berezin2011} in their section \textit{The network construction method}. Such a normalization of the correlation function is used to obtain a rather stable correlation measure which is not depending on any particular time delay $\tau$. Such transformations have an obvious impact on the link strength distribution. In our case (see figures \ref{fig.Fig11E1a2a}, \ref{fig.Fig11E1a2b}, \ref{fig.Fig11E1a9a}, \ref{fig.Fig11E1a9b}, \ref{fig.Fig11E1a10a}, and \ref{fig.Fig11E1a10b}) a long tail becomes visible and the difference between the correlation values calculated for randomized data is more significant, especially as the randomized results do not have this long tail. 

In case of MST and PMFG the sub-graphs include those nodes where \textit{"edges represent the most relevant association correlations"}. But depending on the process which is analyzed, it is not always possible to use this single rather simple assumption. According to Onnela \textit{et al.} \cite{Onnela2007} it is important to take the different role of strong and weak links (they call them weak ties) into account. They found, that the size of the giant component varies differently, depending on the type of links which are removed from the network first. If the weak links are removed first, the size of the largest component decays faster than in the case there the strong links are removed first. This work is based on the so called "land mark paper" by Mark Granovetter \cite{Granovetter1973} titled \textit{"The strength of weak ties"} published in 1973. According to Granovetter, one can find links with different roles in social networks. The weak ties connect clusters formed by highly connected nodes, there the cluster internal links for itself are strong ties. A wrongly chosen filter could not identify such weak ties.   

This has many implications. The nodes connected via strong ties are in the same cluster because of their similarity. One can expect a very specific type of interactions between them. Shared interest in the same topic leads to discussions about a particular topic but probably less communication about "off topic" content happens at the same time within this group or cluster. New inspiration, new insights or just the missing puzzle can often not be found within a group of similar persons or in documents about the same topic. Connectivity to other clusters is important therefore. In this case the links are rather weak and could easily bee overseen. 

This can also be understood by using social interaction as an example. A close contact to colleagues within the same workgroup or within the family one lives with can be maintained with less effort - here I do not address the quality of such relations. But, staying in touch with relatives in a city, far away or keeping the good relation to friends from school over years seems to be impossible for many people, because of the nature of such weak ties. The nodes in a different cluster, which is formed by node less similar to the  initial node requires more attention and more effort in order to stay connected with them. On the other side, such weak links are \textit{"the universal key to the stability of networks and complex systems"}, this is also the title of a book by Peter Csermely \cite{csermely2009weak} published in 2009. This gives us a strong motivation also to identify significant but weak links.  

\section{The Percolation Threshold}
Percolation on a lattice or on networks is characterized by a critical value $p_c$, the so called percolation threshold. On lattices one can differentiate bond-percolation and site-percolation (see Fig. \ref{fig.PercolationTypes}.a). Site percolation means, that randomly all fields of the lattice can be occupied. The occupation probability at the time when a connection between two boundaries of the underlying geometry appears gives us the percolation threshold, which is in this case also the density of the elements on the lattice. A second model is called bond percolation. Instead of coloring the fields, we draw lines along the borders of fields. The density of lines in the lattice, where a connection between the boundaries exists defines the percolation threshold. At the critical point, there exist a continuous path between two borders but also a certain number of clusters, there is not just the one connecting cluster. One has to repeat the simulation experiment very often to find $p_c$ with a reasonable accuracy.


Percolation on a network must be analyzed in a different way. Because spatial embedding is not available in arbitrary networks, one observes the size of the second largest cluster in the network. How is the size of this cluster $N_s$ changing as a function of the overall network size $N$? In general, one observes that $N_s$ decreases if $N$ increases, but not close to the percolation threshold. As long as $p < p_c$ all clusters have a maximal size and if $p > p_c$ the largest cluster contains the most nodes. But in case of $p = p_c$ the second largest cluster does not grow, even if the overall network size is growing.
 
In some cases, if spatial embedding of a network is possible, a hybrid concept as illustrated in Fig. \ref{fig.PercolationTypes}.b can be applied. First, one transforms the problem to a bond-percolation problem on an unregular lattice. Depending on the orientation one is interested in, two outer boundaries are defined by two parallels through the two outer most nodes. This allows an analysis of percolation as a function of a chosen direction, which is orthogonal to previously defined boundaries.

\label{ext.fig.PercolationTypes} 
\input{semanpix/PercolationTypes/imageLS}

\cite{Bradde2010}
\textbf{Explain, how the "spatial embedding" was used to study critical fluctuations.}

\subsection{Calculation of the Percolation Threshold}
In general, an exact solution for $p_c$ does not exist for all geometries, especially because $p_c$ depends on the lattice structure and the topological properties of the network respectively. Newman and Ziff proposed a Monte Carlo based method to calculate $p_c$ \cite{Ziff2000}. Karrer \textit{et al.} \cite{Karrer2014} calculate $p_c$ as the value of $p$ where the size of the second largest component of a network reaches its maximum. 
% Karrer
% http://arxiv.org/pdf/1405.0483v2.pdf

Exact results for percolation on a 2D lattice were published by Domany \textit{et al.} in the article \textit{Directed Percolation in Two Dimensions: Numerical Analysis and an Exact Solution} \cite{Domany1981}. More exact solutions for percolation thresholds in networks were studied by Cohen \textit{et al.} \cite{cohen2010complex} and Buldyrev \textit{et al.} \cite{Buldyrev2010}. The percolation threshold can be calculated exactly for random graphs. In case of graphs with a fixed degree $k$ (random regular graphs) the percolation threshold is $p_c=1/k$. According to Cohen \textit{et al.} \cite{cohen2010complex} one can find $p_c=1/<k>$ in Erd\"os–R\'{e}nyi (ER) networks with a Poissonian degree distribution. Also for Networks of Networks (NoN) or so called interdependent networks a critical percolation threshold could be calculated exactly (see Buldyrev \textit{et. al.} \cite{Buldyrev2010}). Filippo Radicchi \cite{Radicchi2015} published a comparison of common methods in a recent article, titled \textit{Predicting thresholds in networks}. In his study of 109 real networks he found, that in less than $40\%$ the advanced approaches, based on the inverse of the largest eigenvalue of the networks adjacency matrix, performs better than the naive approach, based on the moments of the degree distribution. According to Radicchi, in general, all studied indicators behave worse as soon as the value of $p_c$ becomes large. 

% ETOSHA.CITE
% Cohen2010
%Jump up ^ Reuven Cohen, Shlomo Havlin (2010). Complex Networks: Structure, %Robustness and Function. Cambridge University Press.

% ETOSHA.CITE
% Buldyrev2010
%Jump up ^ S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, S. Havlin (2010). "Catastrophic cascade of failures in interdependent networks". Nature 464 (7291): 1025–28. arXiv:0907.1182. Bibcode:2010Natur.464.1025B. doi:10.1038/nature08932. PMID 20393559.

\subsection{Determine the Percolation Threshold by Simulation}
One way to find the percolation threshold in a network is based on random removal of links. In case of a correlation network we start with an adjacency matrix, which contains all relevant link strength values. Randomly chosen fields $a_{ij}$ are set to zero which means, we remove the link between node $i$ and $j$. This procedure is repeated until the first node has no links to others ($k_d = 0$). This is exactly when the first node is disconnected. The link density of the remaining network at this time gives one value for $\rho$. We repeated this procedure $n$ times and calculate $p_c = \sum( \rho ) / n$.  

A different approach removes the links with the smallest values first. This method is comparable to the construction procedure of an MST, but the results can not be compared directly. In case of the MST only one cluster exists. In case of percolation, there may exist many clusters. A critical percolation cluster is not an MST. Our simulation results contain one cluster only. But this cluster is not necessarily a tree nor is it an MST. 


\section{Filtering Correlation Matrices}
A bi-modal or a multi-modal link strength distribution would be the ideal case. One could define a threshold in between existing maxima. But in reality we do not find correlation values which allow such a clear separation. Instead we find distributions with shapes where even no simple model such as a Gaussian or power law can be fitted.

No matter what type of similarity measure we calculated and how the link strength distribution function looks like, it is almost always possible to quantify, how strong the distribution, obtained from real data differs from a distribution calculated for randomized data.

Therefore we produce two different distributions, and compare both to each other by a statistical test, such as the Kolmogorow-Smirnov test (see section \ref{KSTEST}). From this result one can conclude on a macroscopic level only, e.g., if there is a measurable correlation between subsystems or within the system at all, or if the randomized data does not lead to a significant different distribution. In this case one could even not interpret the correlation matrix in a useful way. 

In case of significant correlations, one has to find out, which links are the relevant and significant links. One can look for the most relevant nodes in a different network representation of the same system first. A comparison of the link distributions for those nodes with the link strength distributions of other nodes shows, if link strength depends on node properties systematically. 

By doing this, one would introduce a dependency on an existing network. Some applications would allow this, but in general a parameterless approach is preferred. Such a parameterless approach is based on an individual significance test per link, as described in the next section. In the remaining part of this chapter I illustrate a major differences between threshold filters and structural filters. Both can be used to prepare a network for further structural analysis. A percolation analysis is not possible in case of MST or PMFG, as those methods already use specific structural properties of the network.
 
\section{Interpretation of Calculated Link Strengths}
Initially, Pearson correlation between the access-rate time series of Wikipedia pages was used to identify the hidden link structure in Wikipedia neighborhoods. We expected to find a correlation between the access activity and the link creation event. If no link exists between pages, they could have a similar activity pattern because of some real world aspects which are not yet present in the content network. The question is, which measure can be used to calculate a predictor for link creation events from access-rate time series?

Because the access-rate time series contain strong sporadic peaks (compare with extreme events studied by Crane \textit{et al.} \cite{Crane14102008}) or periodic patterns (see Schreck \textit{et al.} \cite{Schreck.REPORT}), we analyze the impact of such peaks. Keogh \textit{et al.} \cite{Keogh2005} used three types of pulses with different shape to study the influence on clustering properties of time series sub sequences. The shapes are Funnel, Bell, and Cylinder (see Fig. 7, 8, and 18 in \cite{Keogh2005}). We work with single peaks added to white noise instead.

\subsection{Influence of single peaks}
It is well known, that single peaks in time series have a strong influence on the correlation values, if calculated via Eq. \ref{EQ.FCC.PEARSON}. Other approaches, which are in general also based on Pearson correlation have been analyzed in this work. Before it is possible to apply such a correlation measure to identify hidden links in large systems, it is important to understand the influence of noise, outliers, and defects in the data set. Gaps and missing values influence the results clearly. 

Chiu \textit{et al.} \cite{Chiu2003} describe the same problems in the context of motif detection. Especailly for longer periods with low data quality or missing data they use so called \textit{"don't care"} sections. This approach would finally also lead to a gap in the result time series, especially in case of time dependent analysis. Therefore, this approach seems to be useful only for data exploration, not for systematic analysis. 

One individual missing value can easily be replaced by the average value of the two closest neighbors. Such a simple replacement is not possible in case of longer periods of missing values. Missing values could be added in this case, if an appropriate model exists, which can be used to simulate the process. Otherwise, we observed an artificially increased correlation which has no real meaning.

Correlation values decrease systematically in the presence of strong noise. If peaks are added, one can observe an increased correlation, depending on the height and the density of peaks. Our next goal is, to compare the two measures and to study usability in a filter procedure.
 
For us it is important to know, how the quality metric for correlation-functions with sharp peaks behaves under certain conditions. Therefore we conduct an simulation experiment. Random time series (white noise) with one artificially created peak are used for calibration. We create such time series of 28 values of a Gaussian distribution with $ \langle x \rangle$=0 and $\sigma \in [1, ..., 10]$ and place a peak of height $h$ into the noisy correlation-function $F_{CC}$. This peak simulates a strong correlation at a given delay $\tau$. The value for $\tau$ is 10 in this procedure and has no impact on the result. The peak strength $h$ varies between 0 and 1000. This allows variation of the signal noise ratio. Time series without any peak can be seen as the cleaned time series, from which the strongest value was removed. Eq. (\ref{EQ.FCC.NORM}) and Eq. (\ref{EQ.FCC.CLEAN}) are used to calculate the link strength. First we analyze the influence of peak height $h$ on the maximum link strength (see Fig. \ref{fig.PeaksAndStrengths}.a). Based on the link shape quality metric we define a threshold which allows filtering of sharp peaks only, even if they are rather weak (see Fig. \ref{fig.PeaksAndStrengths}.b). 

\label{ext.fig.PeaksAndStrengths} 
\input{semanpix/PeaksAndStrengths/imageLS}

\subsection{A quality Measure for Pearson-Correlation Functions}
The correlation function $F_{CC}$ is calculated for an appropriate number of $\tau$-values. For data with recurring patterns one should choose $\tau > t_p$ where $t_p$ is the length of the period. In case of only one single sharp peak in $F_{CC}$ we have a strong indicator for a strong correlation at a given dealy $\tau$. Periodic cycles or patterns would cause multiple of such peaks. This is used to define a quality criteria. We calculate the normalized link-strength $s_n$ with Eq. 
\ref{EQ.FCC.NORM}. Now, we remove the maximum value from $F_{CC}$ and replace it with the average value of $F_{CC}$ and repeat the link strength calculation to obtain $s_{rem}$. Finally, we define $QM = \frac{s_{rem}}{s_n}$. This approach is not sensitive regarding the link strength and allows to identify weak, but clear correlation peaks within the noisy data.  

\subsection{Significance Tests}
In general, there exist two categories of significance tests. The first and simplest one compares the full link strength distributions $p_l$ calculated from raw data and $p_{shuffle}$ from shuffled data. All relevant time series properties, such as long term correlations, auto-correlation, and cross-correlation between two series, have to be changed or removed during shuffling while all aspects which do not influence the interpretation of results (distribution of values and spectral frequencies) have to be preserved. Instead of changing the measured data it is also possible to use time series sequences from different periods to calculate the link strength distribution $p_{dt}$, so that no correlation between the series is assumed. A cross-check allows a comparison of this distribution $p_{dt}$ with $p_{shuffle}$. Both should not differ significantly but they both should differ from $p_l$.

This test allows not yet an interpretation of individual link strengths but helps to describe the system on an abstract level. Based on this idea it is possible to verify, if a given process can be modeled as a network, and if the results are not just random or artifacts of the measurement or analysis procedures. 

A more detailed significance test is based on an individual comparison for each link. The calculated link strength is compared with a number of randomized results. Furthermore, one can calculate multiple correlation values on a higher time resolution, e.g., instead of daily data for one month, the hourly data for each day is used. Now one uses the median value from all days to represent the correlation during the month. This approach was evaluated by Berit Schreck \cite{Schreck.BA} in her Bachelor thesis titled: "Rekonstruktion komplexer Netzwerke mittels Kreuzkorrelationsmethode".

 
\section{Threshold Filter}
A threshold filter is based on a single property, derived from a density distribution function of link strength values. No structural properties and no node group properties are used to separate relevant links $L_N$ from non relevant links $L_N$.

\subsection{Fixed Threshold}
A typical approach defines a link strength threshold based on a certain portion of links, for which the strength is stronger than the threshold value. This approach works well, if the link strength distribution is asymmetric and shows a fat tail on the right side or if a bi-modal distribution exists. 

\label{ext.fig.Filter13.2} 
\input{semanpix/Filter13/imageLS2}

The metric used for the networks in Fig.\ref{fig.Filter13.2} is semantic similarity (see section \ref{sem.sim}). In this case the weak links are not relevant, because the network was obtained from static texts. Thus, we can apply the fixed filter threshold approach. Some strongly connected clusters appear first. With increasing link strength the clusters link density increases and at a threshold of $\approx 15\%$ also links between clusters emerge (see Fig.\ref{fig.Filter13.2}).

Because different link strength metrics lead to very different shapes of the PDF different thresholds are required. Also, because of variable distributions over time, a fixed threshold can not be used in general. Instead, a parameter less approach is required. 

In the following sections we test two methods for dynamic threshold detection and link classification.
Therefore we calculate two additional properties from previously created correlation functions. By stretching the PDF function in a two dimensional plane we are able to identify clusters of links with comparable properties. Our goal is a structural comparison of networks as a function of time. Therefore we use the adaptive threshold approach to define and identify relevant links. In a next step, a variation of the threshold shows if the requested
network properties change significantly as a result of the variation. This way, we can calculate time dependent network properties and stability criteria. Results will be presented in the next chapter. 

\subsection{Dynamic and Adaptive Thresholds}
\label{sec.adaptiveThreshold}

Because a clear indicator for separation of significant and non significant links does not exist, and because a threshold even if it could be found would not be stable over time we describe an geometric approach to derive the separation threshold from the measured PDF and a simple fit procedure. 

The method - illustrated in Fig.\label{fig.DynamicThreshhold}.a - is based on the assumption that random link strength distributions could be fitted by a parabola (open on the bottom side) if the PDF is plotted in log scale. The maximum value $x_{max}$ of this parabola is usually not exactly found at position $x=0$ - and this deviation of the maximum from its theoretical position can be considered to be an indicator of a property which is not understood so far. We interpret it as a stability criteria, beside the $\sigma$ value obtained from the fit. We have to find the value $x=t_s$ for which the probability of having a random link or a real link is $50\%$. If $x>t_u$, than we have a relevant link for sure, and for $x<t_l$ we consider all links as random links, regarding the property plotted on $x$ axis. The overlapping area must be separated Therefore we define a second geometrical shape, a line with negative slope, which represents an exponential function if log scale is not used. The point there both curves cross is defined as $x_l$ and if the do not cross we set $x_l = x_{max}$. The position there the parabola crosses the x axis are used to define $x_u = max( x_{r1}, x_{r2})$.

Berit Schreck tested this approach in her Bachelor thesis \cite{Schreck.BA} and presents results in Tab.5 on page 22. After normalizing the fit parameters she obtained time series which characterize the time evolution of the system, based on a collective property of the whole system. A seasonal structure seems to be visible in Fig.15 on page 28 in her work, but as the time series are shorter than one year we can not conclude this for sure yet. 

%%%
\label{ext.fig.DynamicThreshhold} 
\input{semanpix/DynamicThreshhold/imageLS}

We describe the link strength distribution as bimodal distribution. One part can be described by an Gaussian fit and the other part by an exponential fit with negative exponent. For a clear illustration a semi logarithmic representation is used and therefore the fit functions are now: $\mathcal{P} = a \cdot x^2 + b \cdot x + c$ a parabolic and 
$\mathcal{L} = m \cdot x + n$ a linear function with time dependent parameters $a,b,c$ and $m,n$. First we have to obtain the fit parameters from correlation data from each time step. Next, we have to find $t_{ds}$ in Eq. \label{eqthreshold}.

%\begin{equation}
% \mathcal{P} = \frac{1}{\sigma\sqrt{2\pi}} e^{ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2 }. 
%\end{equation}

%\begin{equation}
% \mathcal{P} = log( \frac{1}{\sigma\sqrt{2\pi}} e^{ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2 } ). 
%\end{equation}

\begin{equation}
 \int_{x=t_{ds}}^{x_u} \frac{1}{\sigma\sqrt{2\pi}} e^{ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2 } dx = \int_{x=I}^{t_{ds}} ( e^{-(a x^2 + bx +c)} - \frac{1}{\sigma\sqrt{2\pi}} e^{ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2 } )dx
 \label{eqthreshold}
\end{equation} 

\begin{equation}
 \int_{x=t_{ds}}^{x_u} \mathcal{L} - \mathcal{P} dx = \int_{x=t_l	}^{t_{ds}} ( \mathcal{L} - \mathcal{P} )dx
\end{equation} 

Finally I tested another approach, which uses data points in a 2D plane as shown in Fig.\label{fig.DynamicThreshhold}.b. By applying the k-means clustering algorithm it is possible to identify the two groups of significantly different links. The figure shows the group of strong links (A) in black and the weak, but sharp links in red (B). The threshold along the x-axis could be found based on a comparison results obtained from shuffled data and along the y-axis we use the additional quality metric. k-means clusters reflect the boundary between the weak but relevant and weak non relevant links. In the next chapter we also investigate how stable those clusters are as function of time.
 
%\label{ext.fig.CompareQualityMetrics} 
%\input{semanpix/CompareQualityMetrics/imageLS}

\label{ext.fig.DefineFilterThreshold} 
\input{semanpix/DefineFilterThreshold/imageLS}

\section{Structure based Filters}
The whole graph can be used as source for information. A very simple approach is to analyze properties of clusters, if cluster exists, as shown in Fig.\ref{fig.Filter13.2}.
A mixed approach uses a link strength filter and an algorithm to find connected components. In this case the size of connected components is influenced by the filter threshold. Only if this size is a stable property one can be assume to have a 
usable filter. Otherwise one can also test the filter threshold by measuring the change in the structural property caused by a variation of the filter threshold. 

\input{semanpix/Filter13b/imageLS}


%
% Equation: for error estimation 
%

Fan \textit{et al.} published a paper in 2008 titled: "Network of Econophysicists: a weighted network to investigate the developement of Econophysics". They use a weight to express special properties of network nodes, dependent on the node type.

We can use such weights also to define multi layer networks which can finally be projected or transformed into one graph for which finally classical network measures are calculated. Specific clusters of interest are percolation clusters and clusters, obtained from clique percolation as shown in the right image in Fig. (see also \cite{PalEtAl05}). 

\label{ext.fig.PercolationTypes2} 
\input{semanpix/PercolationTypes2/imageLS}

\section{Conclusion}

An appropriate selection of the right correlation measure and the right filter parameters are important aspects in event and time series data analysis. Therefore, calibration and a systematic analysis of external influences are required. Such influence can be, e.g, very strong peaks or patterns in raw time series are. Data inspection procedures as shown in this chapter should serve as a foundation for decisions regarding filter algorithm and parameter selection. 

One can assume that different measures emphasize different functional aspects of the underlying system. A structural comparison of this networks allows to identify differences between subsystems of complex systems and probably one can extract metrics to describe the coupling between subsystems by simple analytical functions, but all this is misleading if the selected procedures are not in line with the desired outcome, e.g, weak ties can be overseen and existing obvious structures could be over estimated. 

The individual significance of each link should be taken into account. This can be achieved by calculating a relative link strength as the ratio between the measure on real data and randomized data. Additional quality metrics allow further expansion from 1D to 2D space. This allows application of parameterless adaptive methods for dynamic link classification.

The distribution properties, such as average link strength, or the moments of the distributions are helpful for analysis on a macroscopic scale and allow fast access to time dependent properties on a system level, but they can not be used to characterize individual links.

If a similarity measure is used and only a high similarity between nodes is considered as a relevant information, the Maximum Spanning Tree and the Planar Maximal Filtered Graph are useful to extract informative sub-graphs. Alternative approaches are static or adaptive threshold filters, but one has to be aware of the fact, that weak links are systematically ignored by this methods. Therefore, an alternative is required, also 
if the correlation or dependency measure is within the range $[-1, ..., 1]$. In this case, one can separate groups, one with all values below 0 and one for all values above. Functional networks are finally created from both independent link groups. The layers are combined in a property graph, which means, links of different categories, types, or groups are merged to one single link layer in which each link strength is represented as a vector.

Another problem arises, if link strengths differ much, span a wide range of values, and are not normally distributed. Especially in case of time dependent analysis the whole ensemble of links can not be represented by the average value and standard deviation. One single fixed threshold can not be used than and calculation of reliable confidence intervals is not possible, if simply based on an assumed Gaussian distribution. One has to calculate adaptive filter thresholds to address such variations, instead it is recommended to calculate percentile-based confidence intervals for the link strength distributions from each time interval. 

%A relative measure, which expresses the difference between a temporary value and the long time average or even a normalized value, which maps all values into an interval between zero and one would even be a better measure for link definition.

As shown in this chapter, many different aspects influence the strength of a correlation link and the link strength distribution, especially if the system is not a closed system or if it is out of equilibrium. Finally, the right filter approach leads to a useful representation of the entire system. 




\chapter{Measuring Context Sensitive Relevance}
\label{chap.ContextsensitiveRelevance} 
\label{chap.RELEVANZ}

This chapter introduces a new method to calculate a time-resolved context-sensitive
measure for the relevance of interlinked digital resources like web pages, messages, news articles, or arbitrary documents. Such objects are usually embedded within a specific context. In Wikipedia contexts can be represented 
by category pages or by a local neighborhood graph (see figure \ref{fig.LocalNetworksDefineAnalysisScope} in section \ref{sec.contextgraph}) in Wikipedia. In general, they are defined by linked web pages. In the following we will define the representation index REP and the time resolved relevance
index REL, which both allow to distinguish content of a more local relevance from content
of a more general or even global relevance at
variable geo-spatial and temporal resolution. The data is collected from
Wikipedia in a semi-automated procedure, which allows a near real time
analysis of time series data. Continuous time series, event time series, explicit link structures, 
and implicit semantic annotations are used together.
Results are visualized as representation plots and relevance plots. Both charts
support an advanced interpretation of the results in various interdisciplinary
research contexts including econo-physics and socio-physics.

\section{Introduction}

\cite{Berberich2004} 
%ETOSHA.Link http://semanpix.de/opendata/wiki/index.php?title=Berberich2004

Since the numbers of hypertext pages and hyperlinks in the WWW
have been continuously growing for more than 20 years, the problem of finding
relevant content has become increasingly important. This led, for example, to
the growth of Google Inc. with its mission statement 'to organize the world's
information and make it universally accessible and useful' \cite{GoogleMission}.
Initially, the WWW was mainly a content network and did not reflect relations
between authors. It provided structured and connected information. However, the
appearance of Social Media Applications (SMAs), such as Facebook, LinkedIn, and
Twitter, with friendship and follower relations between individual users has led
to the creation and simultaneous evolution of novel user community networks
(social networks) together with content networks. Such SMAs can be regarded as
networks of networks \cite{Bakshy:2012:RSN:2187836.2187907}, since the
underlying user and content networks are closely inter-related with each other.
The collaborative creation of linked content became very popular. Another
impressive example for this is Wikipedia, a multilingual, web-based,
free-content encyclopedia project supported by the Wikimedia Foundation. Because of the
intertwined networks involved in the creation and presentation of information in
the WWW, the identification of relevant content has become increasingly
difficult. Additional problems arise if the time evolution of content relevance
shall be traced and if local and global relevance of content shall be
distinguished.




\subsection*{Ranking vs. Relevance}

Typically, ranking algorithms like (Google) PageRank
\cite{ilprints422,ilprints361} or the HITS algorithm
\cite{Kleinberg:1999:ASH:324133.324140} are used to calculate the relevance or
importance of a given page (node) in the WWW. However, 'relevance' is not an
exactly defined term and cannot be measured in a unique procedure. According to
\cite{hjorland_work_2002} relevance can be assigned to a thing or to information
named \textbf{A} in the context of a given task \textbf{T}. Only if \textbf{A}
increases the probability of achieving the goal \textbf{G} of task \textbf{T},
\textbf{A} is relevant to \textbf{T}. Without a task and a related goal,
relevance does not exist. The identification of relevant information thus
defines a context. In this chapter, we use the term 'relevance' in the same way as 
'importance' or 'meaningful within a given context'. Measuring relevance of a
node can be done according to (i) its intrinsic properties (e.g. text length of
a WWW page), or (ii) the relative value of an intrinsic property (e.g., text
length divided by the average text length of a group of related pages), or (iii)
based on structural properties of one of the networks embedding the considered
node. PageRank expresses the probability to find a random surfer in a given node
\cite{ilprints422,chebolu2008pagerank} and thus exploits mainly the structural
properties of the network. Similarly, the HITS algorithm classifies a node
either as a hub node (many outgoing links) or as an authority node (many
incoming links) \cite{Kleinberg:1999:ASH:324133.324140}. Both algorithms are
applied to directed graphs and require a dataset describing the full graph. This
is challenging for very large systems with billions of nodes. 

\subsection*{A new Approach}

In order to achieve a meaningful interpretation of analysis results all possible influencing 
factors have to be taken into account - which is impossible in practical applications. In socio-physics these are e.g. demographical and ethnological influences such as the embedding 
into cultural contexts, or political and economical influences such as availability of technical infrastructure or even 
access restrictions. Such aspects might influence the data collection procedure and cause a hidden bias. Our new approach
was developed to allow identification and qualitative interpretation of such properties.

Besides this, multiple research disciplines look at different parts of the data
set. In order to connect and compare results of diverse research
projects, scientific methods for social network analysis require robust and
flexible frameworks which enable and support interdisciplinary approaches.
Therefore, we aim at a comparable measurement of a node's relevance within a local graph
defined by the node's local neighborhood. Especially local link structures,
article length, user activity, and editorial activity are considered.
The key properties of the new method are:
\begin{itemize}
\item The local neighborhood defined by explicit links and implicit 
semantic annotations is examined.
\item The context can be defined by a common language or by any other set of 
semantic concepts. This enables a connection to cultural aspects, related 
to regions and languages used by specific groups of people.
\item The semantic relation between pages in different languages can be used to 
aggregate data related with a certain topic, e.g. studies related to news, market 
data, (Twitter) messages, or communication in the context of large important events 
or movements in societies.
\end{itemize}

\section{Definition of Representation Indexes}

Here, we define evaluate several parameters that measure the ratio of local 
representation with respect to global representation for the considered topics (see section \ref{sec.contextgraph} ). 
Our first approach is based on the numbers of articles (nodes) in each group, 
$n_{\rm LN}$, $n_{\rm IWL}$, and $n_{\rm GN}$, see table \ref{datasetlist} in the data set catalog. 
Specifically, we define the \textit{local representation index} for node degrees by
\begin{equation}
{\rm REP}_k = {n_{\rm LN} + n_{\rm IWL} \over r_{\rm GN} + n_{\rm IWL}} =
{k_{\rm CN} \over \langle k_{\rm IWL} \rangle}
\qquad {\rm with} \quad r_{\rm GN} = {n_{\rm GN} \over n_{\rm IWL}}.
\label{EQ.REPk} 
\end{equation}
Note that the nominator is identical with the so-called degree $k_{\rm CN}$ of 
the CN, while the denominator is the average degree $\langle k_{\rm IWL} \rangle$ 
of all nodes in IWL, i.~e., the average degree of the node regarding the 
considered topic in all other languages.

In our second approach, we consider text lengths $v$ instead of node degrees $k$. The 
total text volume per page and the average text volume per group are used as 
indicators of how well a certain topic is represented within a certain language. 
We assume that a topic is better represented in the language, in which it has a 
more comprehensive explanation. Specifically, for the total text volume $v_{\rm 
CN}$ of each CN and the total text volume $v_{\rm IWL}$ of all $n_{\rm IWL}$ 
other language versions, we define
\begin{equation}
{\rm REP}_v = v_{\rm CN} \frac{n_{\rm IWL}}{ v_{\rm IWL}},
\label{EQ.REPv} 
\end{equation}
since $v_{\rm IWL} / n_{\rm IWL}$ is the average text volume of all IWL pages.

Thirdly, we study \textit{time-dependent local representation indexes} ${\rm 
REP}(t)$ based on the time series of the hourly rates of user accesses $a_i(t)$ 
or editorial events $e_i(t)$ for each CN and each node in the corresponding IWL 
groups. A high number of page views (or editorial changes) can indicate an 
increased interest of the user community. Although page view data are anonymous, 
it is possible to use the relationship between a user and his/her preferred 
language to measure user interest per language. Specifically, for a time slice 
of width $\Delta t$ beginning at $t=t_0$, we define
\begin{equation}
{\rm REP}_a(t_0) = {n_{\rm IWL} \sum_{t=t_0}^{t_0+\Delta t-1} a_{\rm CN}(t) \over 
\sum_{i \in {\rm IWL}} \sum_{t=t_0}^{t_0+\Delta t-1} a_{{\rm IWL} i}(t)},
\label{EQ.G.REPa}
\end{equation}
where $i$ runs over all nodes in the IWL group corresponding to the considered 
CN. An analogous definition is used for the editorial events to define ${\rm 
REP}_e(t_0)$. Clearly, these indexes will be large if there is more user-access 
activity or more editorial activity, respectively, regarding the CN compared 
with the averages in other languages. 



\section{Definition of Relevance Indexes}

The local representation indexes (REP$_s$) are related to a given (or selected) 
semantic concept, expressed by a Wikipedia page (the CN) in a chosen language 
and all IWL pages. They indicate how well a topic is \textit{represented} in a 
given language, irrespective of its embedding within contexts in this language. 
Only the core of the neighborhood network is considered (see figure \ref{fig.PLOSONE} (a,b)). However, it turns 
out that text lengths, user-access activities, and editorial activities are 
hardly comparable across the different Wikipedias, i.e. across the language 
versions and cultures. Therefore, more meaningful results can be obtained if we 
divide by average quantities determined for articles within \textit{the same} 
language community. Such indexes will characterize how \textit{relevant} a 
topic is within the selected language or within the global context. 
%can be analzyed by a comparison of properties measured in the core of the topic's
%local wikipedia page-network and its local neighborhood. In general, the
%relevance of a document within a corpus is measured by comparison of its
%term frequency vector with the term vector of a given search phrase. This
%allows a different result for different search phrases which represent different
%contexts, because a document can be relevant in different contexts.

%Here we are interested in a context dependent ranking of single pages of the
%same meaning but written in different languages, so no search phrase is
%considered, and also term count statistics is omitted. 
Specifically, we study the ratio of the parameters ${\rm L.REL}_v^{\rm LN} =
v_{\rm CN} n_{\rm LN} / v_{\rm LN}$, representing the relevance of the CN in the
chosen language, and ${\rm G.REL}_v^{\rm GN} = v_{\rm IWL} r_{\rm GN} / v_{\rm
GN}$, representing the average relevance
of all IWL pages within their combined contexts, i.e. the relevance of the
selected topic within the other languages.
%${\rm r_{core}} = v_{\rm CN} n_{\rm LN} / v_{\rm LN}$, as the ratio of the text volume
%of a given page and the average text volume of the local neighborhood (LN), and
%${\rm r_{hull}} = v_{\rm IWL} n_{\rm GN} / v_{\rm GN}$ as the ratio of average
%textvolume of the inter-wiki linked pages and the average text volume
%of their neighborhood, which is the global neighborhood (GN). The average text volume
The corresponding \textit{relevance index} is thus defined as
\begin{equation}
{\rm REL}_v = \frac{{\rm L.REL}_v^{\rm LN}}{{\rm G.REL}_v^{\rm GN}} = {v_{\rm CN} 
v_{\rm GN} n_{\rm LN} \over v_{\rm LN} r_{\rm GN} v_{\rm IWL}}
= {{v_{\rm CN} \over 1} {v_{\rm GN} \over n_{\rm GN}} \over {v_{\rm IWL} \over 
n_{\rm IWL}} {v_{\rm LN} \over n_{\rm LN}}} = {\rm REP}_v
{{v_{\rm GN} \over n_{\rm GN}} \over {v_{\rm LN} \over n_{\rm LN}}}.
\label{EQ.RELv}
\end{equation}
% this value is used to set the size of the circles in Fig.3, Fig.4 Fig.5
% and Fig.6

In addition, we compare local \textit{time-dependent relevance indexes} ${\rm
L.REL}(t)$ with corresponding global time-dependent relevance indexes ${\rm
G.REL}(t)$ for user-access activity ($a(t)$) and for editorial activity ($e(t)$) respectively:

\begin{equation}
{\rm L.REL}_a(t_0) = {n_{\rm LN} \sum_{t=t_0}^{t_0+\Delta t-1} a_{\rm CN}(t) \over
\sum_{i \in {\rm LN}} \sum_{t=t_0}^{t_0+\Delta t-1} a_{{\rm LN} i}(t)} \qquad {\rm
and}
\label{EQ.L.REL}
\end{equation}

\begin{equation}
{\rm G.REL}_a(t_0) = {n_{\rm GN} \sum_{i \in {\rm IWL}}\sum_{t=t_0}^{t_0+\Delta
t-1} a_{{\rm IWL} i}(t) \over n_{\rm IWL} \sum_{i \in {\rm GN}}
\sum_{t=t_0}^{t_0+\Delta t-1} a_{{\rm GN} i}(t)}.
\label{EQ.G.REL}
\end{equation}

The width $\Delta t$ of the considered time slices must be optimized so that 
random fluctuations are damped while the temporal changes of the relevance indexes 
remain visible.



\section{Evaluation and Interpretation}

\subsection{Static Representation Index for networks of linked pages}

\begin{figure}[!t]
\floatbox[{\capbeside\thisfloatsetup{capbesideposition={left,top},capbesidewidth=8cm}}]{figure}[\FBwidth]
{
\includegraphics[width=3.5in]{semanpix/PLOSONE/images/REL3a.eps}
%\includegraphics[width=3.1in]{semanpix/PLOSONE/images/REL3b.eps}
%\includegraphics[width=3.1in]{semanpix/PLOSONE/images/Fig3newA.eps}
%\includegraphics[width=3.1in]{images/Fig3newB.eps}
}
{
\caption{
{\bf Comparison of three static $\rm REP$ measures.} The x-axis shows $\rm
REP_v$ the related text volume based relevance measure $\rm
REL_v$ is plotted on y-axis. The area of the circles represents the
$\rm REP_{k}$ value. In general, Wikipedia pages for German cities and UK cities show comparable $\rm
\rm REP_v$,  $\rm REL_v$, and $\rm REP_k$ values. This indicates that pages from this specific category 
are equally represented in both languages. \textbf{LINK TO DATASET ...}
}
\label{fig.REP.3}
}
\end{figure}

Figures \ref{fig.REP.3}, \ref{fig.REP.4}, \ref{fig.REP.5}, and \ref{fig.REP.6} compare the values of the local representation index, REP$_v$, 
and the relevance index REL$_v$ regarding text volumes $v$ (eqs. (\ref{EQ.REPv}) and
(\ref{EQ.RELv})) for datasets 1-4 (see data catalog, section \ref{chap.DATACAT.REL}) in double logarithmic plots. In addition, the local representation index REP$_k$ regarding the node degrees $k$ (eq. (\ref{EQ.REPk})) is 
represented by the areas of the circles.

For cities in Germany and the UK (see figure \ref{fig.REP.3}), one can see that 
Birmingham has clearly the largest REP$_k$, while all the others have a somewhat
similar REP$_k$. In particular, REP$_k$ for the most important city, Berlin, 
is smaller than the values for all other cities except the very small German
cities of Sulingen and Bad Harzburg (which would not even qualify as cities 
by English standards). This first result for a homogeneous group of topics
(cities) indicates, that the values of REP$_k$ are {\it not} comparable across 
language versions, although they may be useful for estimating representation 
among articles of similar topics in the same language version.

The text-volume based representation index REP$_v$ performs clearly better,
see Fig. \ref{fig.REP.3}. The sequence obtained by ordering according to REP$_v$ (Berlin 
- Oxford - Birmingham - Heidelberg - Bad Harzburg - Sulingen) reflects quite 
well the importance of the cities, also across languages, with Berlin and
Birmingham having more than 3 and 1 million of inhabitants, respectively, 
and Berlin, Oxford, and Heidelberg having major universities. The
corresponding relevance index, REL$_v$, which was designed to make results 
better comparable across language versions, actually performs much worse -- 
for example Bad Harzburg turns out to be above Berlin in this ranking.

How do the three static indexes perform when different kinds of topics are 
compared?  To answer this question, we use figure \ref{fig.REP.4}). It 
shows that both text-volume based indexes, REP$_v$ and REL$_v$, seem to 
distinguish between countries (large values) and persons (small values). 
In addition, both indexes are very large for the German versions of the 
articles regarding CNs 2.1 ('US') and 2.2 ('Germany') compared with the 
English versions, although one would have expected that the English versions 
are more important. Apparently, the articles on Germany and on the USA in 
the German Wikipedia are particularly long, more than 100 and 1000 times 
longer than those in other Wikipedias, respectively. This leads to the 
large values of REP$_v$. Simultaneously, both German articles are much 
longer than the articles in the corresponding local neighborhoods; this 
leads to the large values of REL$_v$. In English language, they are merely 
30-50 times longer, which may be due to some material moved to sub-articles.
These large differences between the German and English version of the articles
regarding the countries are not justified and indicate that both text-volume 
based indexes cannot be used for classification on their own.

\begin{figure}[!b]
\begin{center}
%\includegraphics[width=3.1in]{semanpix/PLOSONE/images/REL3a.eps}
\includegraphics[width=4.8in]{semanpix/PLOSONE/images/REL3b.eps}
%\includegraphics[width=3.1in]{images/Fig3newA.eps}
%\includegraphics[width=3.1in]{images/Fig3newB.eps}
\end{center}
\caption{
{\bf Comparison of three static $\rm REP$ measures.} The x-axis shows the $\rm
REP_v$ and the related text volume based relevance measure $\rm
REL_v$ is plotted on y-axis. The area of the circles represents the
$\rm REP_{k}$ value. The page for Angela Merkel (the Federal Chancellor of
Germany) is equally represented in both languages. The page named
Barack Obama is much better represented in English language than in the German Wikipedia sub project. This
shows the influence of the selected linguistic context on the local
representation index and allows a context dependent ranking for different terms
within one language or a comparison between multiple languages.
\textbf{LINK TO DATASET ...}}
\label{fig.REP.4}
\end{figure}


Interestingly, however, the articles regarding the two top politicians of 
both countries do not differ much in length between the language versions. 
They all thus have nearly identical local representation REP$_v$. However, 
the inset in Fig. \ref{fig.REP.4} shows that the English versions of both articles have 
a slightly lower relevance REL$_v$ -- contrary to expectations. This 
suggests than REP$_v$ is a better indicator for importance than REL$_v$,
in agreement with the observations in dataset 1. All cities in dataset 1 
are actually located in the same region of the REL$_v-$REP$_v$ plot as the 
chancellors. 

The local representation regarding the degree, REP$_k$, is not agreeing well 
with the expected importance of the CNs. REP$_k$ shows that CN 2.2 ('Germany') 
has many more links, i.e., a much larger context, in its German version than 
in its English version, while the opposite holds for CN 2.3 ('Obama'). For 
CNs 2.1 ('US') and 2.4 ('Merkel') REP$_k$ is similar in the German and the 
English versions of the article. REP$_k$ can thus not be directly related 
with importance of the considered CN. Nevertheless, it is rather independent 
of the representation and relevance indexes regarding the text volume.

The general trend it that text-volume based relevance and representation are 
approximately similar, $\rm REL_{v} \approx \rm REP_{v}$ with REL$_v$ being a 
bit smaller in nearly all cases, see Figs. \ref{fig.REP.4} and \ref{fig.REP.5} in particular. However, 
if one looks at detail, REP$_v$ seems to be much more indicative of a CN's 
importance that REL$_v$, see Figs. \ref{fig.REP.3} and inset of Fig. \ref{fig.REP.4} in particular. 
REP$_k$, on the other hand, is rather independent of the other two indexes, 
and its values are comparable only within a given kind of topics and within a 
language version.



\begin{figure}[!h]
\floatbox[{\capbeside\thisfloatsetup{capbesideposition={left,top},capbesidewidth=7.2cm}}]{figure}[\FBwidth]
{\caption{
{\bf Relevance plot} for data set three. The red circles show concepts with an
assumed higher local relevance and the blue circles represent pages for which a
global relevance is assumed. The articles 3.1.L and 3.1.G (label 13) are
highlighted for comparison. This plot illustrates that the indices alone cannot be used as absolute measures but only for a comparison of pages within a given topic or lingual context.}
\label{fig.REP.5}
}
{\includegraphics[width=2.9in]{semanpix/PLOSONE/images/check.eps}
}
\end{figure}

\begin{figure}[!ht]
\begin{center}
%\includegraphics[width=3in]{images/Fig11v3.eps}
%\includegraphics[width=3in]{images/Fig12v3.eps}
\includegraphics[width=6in]{semanpix/PLOSONE/images/fig1112.eps}
\end{center}
\caption{
{\bf Relevance plot} for most edited articles (in September 2009) from a)
German and b) English Wikipedia project.}
\label{fig.REP.6}
\end{figure}

The results for data sets 3 and 4 confirm this general picture, see Figs. \ref{fig.REP.5}
and \ref{fig.REP.6}. Furthermore, Fig. \ref{fig.REP.5} shows that none of the three indexes, REP$_k$, 
REP$_v$ or REL$_v$ can distinguish between articles of rather local relevance 
(red circles) and articles of rather global relevance within the selected data sets. Different cultural aspects and 
differences between the structure and usage of languages might be considered as reasons for this.  
 


\subsection{Temporal Relevance Index for time resolved relevance analysis}
\label{chap.TRI}
Wikipedia is a highly dynamic system. The page networks and the related user
networks grow and change their internal structure while new pages and links
are added or existing pages are edited. During a single page's life cycle
the article can be split into smaller, more specialized pages linked to the main
page where the content originated from. Links also go to other related pages, which cover
totally different topics.

Nevertheless, because the growth process of large Wikipedia projects is
not very fast, after they reached a saturation (see Fig. 16b in Schreck \textit{et al.} \cite{Schreck.2013b}) the system can be handled like a static system on a weekly or even monthly time scale. This allows a time-dependent analysis of the link
structures and of the content of pages.

In contrast to this, user activity varies with daily cycles. According to
\cite{10.1371/journal.pone.0030091} weekly patterns can also be found in Wikipedia
edit-event time series. Daily and seasonal patterns in access time series
have been reported in \cite{Schreck.2013b}.

This study is focused on context sensitive temporal relevance. In previous
studies the page groups have been selected based on the language,
according to the total access activity of the pages, or depending on a
classification as  `stationary' or `non-stationary' access rate
time series. Now we select page groups dependent on their meaning. Our novel approach 
uses relative measures to eliminate hidden biases within the
local neighborhood LN or the global neighborhood GN around a central node CN
and all pages with inter-wiki links IWL to CN.

A classification of access rate peaks in single time series was done to
distinguish `exogeneous' and `endogeneous' bursts (see Sornette
\textit{et al.} in \cite{Sornette.2008, PHYSA.2012}). Within a context network,
such bursts or peaks, detected in one single access-time time series might be
interpreted as a result of a sporadic information flow within the local network
or as the source of such a flow. Because our goal is to study external influences and
their origin in more detail, we take the context of a page represented by
the neighborhood into account. We can study, if an access rate peak has an
influence on the pages in the neighborhood or if it was triggered by an
increasing or dropping activity in the neighborhood. We thus want to
analyze if an excitation is propagated through the network.

User interest or attention to a certain Wikipedia article can easily be measured
by access rates, which are usually calculated from aggregated raw data for a
domain-specific appropriate time range. Fig. \ref{fig.TSDB} compares daily,
weekly and monthly acces-rates. During the last 3 months, the access rate for
CN decreases continuously. However, it cannot be determined from Fig.\ref{fig.TSDB}.a 
alone, if this is an intrinsic property of the node or some
external effect. Therefore, we use Figs. \ref{fig.TSDB}.b, \ref{fig.TSDB}.c, to and \ref{fig.TSDB}.d to 
find a stable access rate in the local and also in the global neighorhood of the selected page. The IWL pages
show a much stronger access rate peak, compared to the CN, with a
maximum in week 19 in 2009. At the same time the variance of the access rate
measured for the CN is much higher. How strong the international influences are and if such even exist cannot be 
explained with individual time series without a context. By taking the network structure and the time series data into account, our approach is a context sensitive comparison. This allows an explanation and even the separation of external and intrinsic
features, found in the uni variate time series. 
\begin{figure}[!h]
\begin{center}
\includegraphics[width=5.8in]{semanpix/PLOSONE/images/TSDB9.eps}
\end{center}
\caption{
{\bf Time-dependent relevance is measured by $\rm L.REL_a$ and $\rm G.REL_a$.}
The
access-rate time series are shown for the central node CN (Wikipedia page: Illuminati (book) from German Wikipedia) in a). The averages of
the logarithms of the access rates are shown for the local neighborhood LN in
b),
the inter-wiki linked pages IWL in c) and for the global neighborhood GN in d)
in
three resoltions (daily in gray, weekly in blue, and monthly in red color). }
\label{fig.TSDB}
\end{figure}

For a direct comparison of access-rates for groups CN, IWL, LN, and GN the
averages of the logarithms of access-rates are plotted into one chart (see
fig.\ref{fig.TRI}). Fig.\ref{fig.TRI}a indicates an exogenous increase of the
access rate for all languages except German, while for the German page the variance
of the access rate increases. Fig.\ref{fig.TRI}b shows endogenous bursts in
week 9 for the CN node and also for the IWL group but no strong change in the
average access rates for both neighborhoods. This is a clear indicator for an
exogenous burst, which is not influenced by a flow of attention coming from
other topics in the neighborhood (unlike the case in figure \ref{fig.TRI}a).

Beside this qualitative interpretation we use the relevance index, defined in
equations (\ref{EQ.L.REL}) and (\ref{EQ.G.REL}), to compare the local and global relevance of a topic in
a quantitative way. Fig.\ref{fig.TRI}c shows an increase of the global relevance
while the local relevance drops at the same time. This indicates, that in a
local context, the topic does not attract that much attention compared to the
global context. The decreasing value of the local relevance index also
indicates an increasing interest in the local neighborhood. Fig. \ref{fig.TRI}d
shows two peaks in the local and the global relevance index at the same time
which supports the classification as an exogeneous burst according to
\cite{Sornette.2008}.
Fig. \ref{fig.TRI}.e and Fig. \ref{fig.TRI}.f show the cross-correlation for the
two functions from Fig. \ref{fig.TRI}.c and Fig. \ref{fig.TRI}.d respectively.
Higher correlation values can be found in Fig. \ref{fig.TRI}.f while the trends of both functions are
comparable. Especially the presence of bursts in both indices yields strong correlations. On the other hand, low correlation values indicate different trends in both indices. They allow to conclude,
that there are also differences between the local and global relevance of the
selected topic.


\begin{figure}[!ht]
\begin{center}
\includegraphics[width=7in]{semanpix/PLOSONE/images/Graph7.eps}
\end{center}
\caption{
{\bf Comparison of local and global REL measures for two selected topics.}
The first row shows daily access-rate data for CN (black), IWL (green), LN
(blue), and GN (red) like those presented in figure \ref{fig.TSDB} for
page 3.1.G in (a), and for page 3.1.L in (b) according to \ref{Tab:1}. (c,d) The second row
shows $\rm L.REL_a$ (blue) and $\rm G.REL_a$ (black). (e,f) The cross-correlation functions for 
sliding windows of length 24 hours (gray), 7 days (green), 2 weeks (blue), and 3
months (red) are shown in the bottom row.}
\label{fig.TRI}
\end{figure}

\textbf{SHOW the differences of representation topics by language ...}

\subsection*{Language Dependent Interpretation of TRRI}
Figure 5a shows a comparison of the global and local relevance index for three selected Wikipedia pages. In case of the companies Oracle and Capgemini we find a stable value while in case of the Apache Hadoop page a significant increase in March 2011 is visible. The local relevance index starts to dominate over the global relevance after it exceeds the threshold $log($ L.TRRI $) > 1.0$. after this point in time the Hadoop project attracts more user traffic than the pages in the neighborhood. 
 
Finally, we show an example for the strong language dependency in figure 5b. The page about the French company Capgemini illustrates the impact of a wrong selection of the lingual context. The relevance index for English pages is relatively low, while for international pages, especially for French pages a high TRRI was measured. One can clearly conclude, that the context has to be adjusted according to the topic, otherwise an unknown bias still exists. The best context selection is given if the difference between local and global relevance index is maximized. The representation plot is used as a tool to evaluate and validate results from TRRI plots, especially in the context of multilingual content networks. 

\begin{figure*}[t!]
  \centering
      \includegraphics[width=0.89\textwidth]{semanpix/figure2.sm/Figure6new.png}
  %    \llap{  \parbox[b]{6.8in}{\Large \textsf a)\\\rule{0ex}{1.95in}  }}
  %    \llap{  \parbox[b]{3.15in}{\Large \textsf b)\\\rule{0ex}{1.95in}  }}
      \label{fig.sm2b}
      \label{fig.sm2a}
\caption{\textbf{Influence of CN language.} (a) shows the contextual time resolved relevance index (TRRI) for Wikipedia pages about companies Oracle (red), Capgemini (blue), and the open source software project Apache Hadoop (black) between January 2009 and December 2011 with weekly resolution. Representation and relevance index depend on the selection of CN. (b) shows the representation index for one Wikipedia page for 6 different languages with the highest representation in French language. According to (b) one can still see a high relevance, based on user access to English and French pages for the company Capgemini. In this case the local context can be defined as a hybrid context by two central nodes - French language because of the country of origin, and English language because of international IT business. If only one language is used, as in (a), one can not clearly differentiate between local (straight line) an global relevance (dashed line) as in the case of the pages for Oracle (red), and Apache Hadoop (black).   
}
\end{figure*}


\subsection{Conclusion}
Computed representation and relevance indexes' values can be used as ranking indicators within a set of given 
articles without the need of processing the whole Wikipedia text corpus. Static REP and REL values can be useful weights 
for ranking and filtering the results. Search engines (like e.g. Apache SOLR) could use those values to
enrich search results consisting of several hundreds of documents in near real time. Computation requires only partial information, not the full page graph (unlike in the case of the PageRank algorithm). 

The cross-correlation between $\rm L.REL_a$ and $\rm G.REL_a$ as shown in figure \ref{fig.TRI}(e,f) is another
measure to study local information flow. In case of high correlation, the core and the neighborhood network behaviour are comparable. Different properties or different dynamics can be assumed if the correlation is close to zero or even negative.
Therefore I suggest to use a threshold $t_s = 0.5$ to define a new property of a local context network, the \textit{context polarization}. Context polarization is zero if the cross-correlation between $\rm L.REL_a$ and $\rm G.REL_a$ is in the range above $-t_s$ and below $+t_s$. In case of a correlations higher than $t_s$ the core is \textit{aligned} with the context and in case of a negative correlation with values less than $-t_s$ core and neighborhood are \textit{complementary} to each other.  


\chapter{Structure Induced Stress: Measure and Quantify the Impact of Functional Networks}
\label{StructureInducedStress}

%%ETOSHA.LINK https://docs.google.com/document/d/1O3XcuKVmxoM_WDNKD0wUck2K3-DfmyBryeJtMlHYVss/edit
 
According to \cite{Fruchterman1991} the analogy to real world systems and nature is used  in order to draw an esthetical image for layouts of complex networks. If it looks like something known, often it appears also plausible. But one has to be careful. Such analogies do not replace the mathematical proof nor a clear theory. 

In this section I introduce a new concept which allows a quantitative layer comparison and comparison of dynamic processes if such are represented as graphs. 

Structure Induces Stress (SIS) is based on the idea of spring embedded elements of an n-body system. In a dynamic equilibrium the nodes have well defined position. Different properties of the system components, such as node properties, edge properties but also the link structure influence this equilibrium and different situations lead to different positions of the node.

\label{ext.fig.StructureInducedStress} 
\input{semanpix/StructureInducedStress/imageLS}

Two networks with different structure lead to a different layout. Each nodes' displacement is a result of the influence of the entire network. Now, we interpret the displacement as the result of the impact of a structural change. We do not measure the different forces, they are the inherent properties of the process which are compared to each other, rather, we measure the consequence of the different influence. This allows us to express the difference in the process defining forces which are not comparable in a direct way. The reason is, there is no real force and the characteristic of the process which leads to a certain placement of nodes can be different, e.g. the static link network and the access activity network are defined by very different concepts. 

We are interested in multilayer networks, in which layers represent different aspects. Generally speaking, we can now measure the impact of one aspect on another one, based on a transformation of node positions into a plane. Because no direct mathematical expressions and no simple property exists which describes the impact of one network layer on another, such a transformation is required. This transformation is achieved by a graph layout algorithm, such as the force directed layout. This approach allows us to embed the network layers in a comparable way, on which we now define a structural measure for process comparison. A comparable rather simple approach is called spring layout. The attracting forces between nodes are based on Hooke's law. Geipel \textit{et al.} published a new, more generic graph layout algorithm called ARF for "attractive and repulsive forces". This algorithm relies on the balancing of two antagonistic forces which is comparable with the approach, developed by Fruchterman and Reingold \cite{Fruchterman1991}. 
% Read More: http://www.worldscientific.com/doi/abs/10.1142/S0129183107011558

Even if a spatial embedding of graphs is not generally possible, the analogy of forces is often used. Examples are the 'Social Force Model' by Helbing et al. and the \textit{'Index Cohesive Force'} by Kenett et al. \cite{Kenett2011}. The later inspired this work and is the base for a more generic approach which is not specific to financial markets. It can be used in the context of arbitrary semantic networks and is called: \textit{'Context Cohesive Force'} (see equation \ref{eq.CCF}). 

The initial idea was to define a functional network from a similarity measure. We came up with several representations of such networks, depending on the selected layout and filter algorithms. From node positions we could not learn much - but relative positions and the overall pattern - which appeared over time we could already derive information which are also reflected by quantitative measures, such as clustering or modularity. How can two networks be compared to each other? An absolute measure would be required. Without a natural embedding into the same environment a calibration is not possible. 

Our new approach uses local properties only. We quantify, to what extend the functional network "disturbs" the structural network underneath. One can also see it the over way around. Therefore we calculate the graph layout as described by Fruchterman and Reingold \cite{Fruchterman1991}. The Fruchterman Reingold layout algorithm is available in many software packages. A modified version was implemented in Scala and released as a module for Apache Spark \cite{ScalableLayoutOnSpark}. The algorithm works in two phases: (1) The initial layout based on structural links is calculated. Node positions are not absolute and do not contain information alone. Functional links contribute not to this layout. (2) The second phase starts as soon as the algorithms converged and an initial layout exists. From now on, also the functional links contribute to the placement of nodes. In case of two equal networks which perfectly overlap, we expect no differences. But if the functional network is not well aligned with the structural network, it influences the node positions already in the next layout step. We calculate the influence for one more time step and interpret the displacement as the influence of the functional network on the the underlying structure.

This procedure is comparable to molecular dynamics simulations. The core difference is the absence of a coordinates in real space. Network nodes have by default no position - at least in most cases - even if geographical locations are known, this kind of data would not contribute new information. As a consequence we calculate the displacement of the positioned nodes on a plane. The initial positions are the results of a transformation on the raw data, e.g. a layout procedure, which uses the concepts of attracting and repelling forces, such as the one described by Fruchterman and Reingold. 
 
Displacement vectors are defined by the initial location and end at the point there the node would be moved to in a next layout step, under the influence of a functional network. For all nodes - or just for groups (clusters, partitions) - one can average the vectors length and the result is a complex number. A quantitative comparison of different systems is now possible, even if they represent different real world systems for which measured data is not comparable in a direct way. 

Figure \ref{fig.StructureInducedStress} shows a simple social network with a formal organizational structure and an informal communication network on top as example. Both of the two clusters have very central nodes (A and B). The ground-state of the system is defined by the structural network which was used for the layout calculation. The displacement of node $E$ is caused by external links, in this case links in the functional network. This informal communication network is not in line with the underlying organizational network and can cause a stress. Based in this measure we can not conclude if the impact is negative - which means it disturbs the function of the overall system, or if it is helpful and improves the system functionality. 

%#################################
%#
%#   THIS is the RESULTS Part ....
%#
%#   (1) First WIKIPEDIA Paper
%#   (2) Contect Sensitive Analysis
%#   (3) Data Driven Market Studies
%#
%#################################


\part{Applications and Results}

The results of a systematic analysis of individual page properties of Wikipedia pages will be presented in this chapter. Following Ratkiewicz and Crane we study the properties of Wikipedia access-rate bursts in more detail in section 15.4. Furthermore, we analyze the time series regarding the long-term autocorrelations. DFA ans RIS show long term-memory effects clearly for access-rate time series but not for the edit-event time series. This allows us to conclude that the collective editorial and the information consumption processes in Wikipedia are fundamentally different.

Ratkiewicz et.al. explain the fat tails of the typical distributions of dynamic properties on a macroscopic level, while Crane and Sornette provide a microscopic view into the behhaviour of a single resource - a YouTube video in this case - which is exposed to a complex system - the social community of interconnected YouTube users.

The fundamental model to describe each node's access-activity is called Hawkes process \textbf{(cite: 2 in Mitchell2010)}. The Hawkes process describes the deviation from a simple Poisson process. Mitchell2010 and Cates used computer simulations to study the Hawkes process systematically.
Their results show three classes of decay exponents as proposed by Crane2008. The numerical results show different distributions which seems to limit the universality of  the Hawkes-based analysis. \textbf{EXPLAIN better what this means.}

\section*{Statistical Time Series Analysis and Complex Systems Studies}
a. Extraction of Extreme-Events \\
b. Corelation analysis for single time series (autocorrelation, DFA, RIS)\\
c. Calculation of functional links from cross-correlation and event-synchronization for pairs of time series\\

Beside properties of individual time series we study the properties of several groups of pages and compare the the 
interest of people in Wikipedia based on existing text volume, contributions on several topics and the access count.
It is assumed to find differences between this perspectives, but how are they relate to each other?


\section*{Dynamics of Social Networks and Information Flow}

Information flow in complex is visible in the presence of extreme events.\\

\textit{a. Analysis of locality of Extreme-Events}
 
Are Extreme Events localized or delocalized in the network structure? 
The dashboard shows if EE are present in the neighborhood of a certain node with high traffic or if just a central
node is affecte by the increased interest.
a.i. Localized=articles are in the same topic or they are part of sub-network with a certain length 
a.ii. Delocalized=there are less articles linked or located in the same subnet\\

\textit{b. Analysis of time-series, Spread of Information in Social Networks}

Are shocks on one note inside the network transmitted over the network?
b.i. At what length do shocks disappear?
b.ii. Are there different types of nodes regarding the transmition properties?
b.ii.1. Can we define categories or filter based on this?
b.iii. Are the shockwaves dependend on the amplitudes of the shock?
b.iv. Is the form of the shock changing or is it conserved?

Right now we can see that linked pages have more correlation and shocks move along the static network links.

How can we find a wave like spread of external shocks in the wikipedia network?
The starting point for our analysis is a list of detected extreme events. A look 
at the 100 highest amplitudes gives us a list of articles (nodes) for which the 
local link based network structure has to be reconstructed from the database. 
This subnet is plotted in a special way. The size of the nodes is correlated 
to the number of accesses to this node in time. By rendering a video we can 
see if there is a wave going from the first node to other nodes in the surrounding. 
We can use different time scales and different numbers of link levels too.

All data to do this analysis is available. We have to find a way to visualize the 
results and the extraction of the static network structure has to be implemented 
based on available libraries.\\

\textit{c. Analysis of correlations between time-series of articles}

We define a dynamic network structure beside the static link structure. The weights 
of links of this network are defined be several correlation coefficients based on several parameters. 

What correlation method do we use?
c.i. Existence of extreme-events during a time frame (position and length are variable) gives us a link between the nodes.

c.ii.  The maximum cross-correlation coefficient of some defined length of the time series is the weight for the link. 

c.iii. How is this dynamic structure evolving over time if we change the type of correlation and the length of the time series?

This part is correlated to the work on climate networks done at the PIK  Potsdam. The software for the calculation of correlation coefficients between time series of nodes will be available at the end of October from the group of Reik Donner.

c.iv. Do links emerge after extreme events occurred?

c.v. Are linked nodes more correlated?

Does an extreme event change properties of the network structure? We can construct local 
subnets at every time step. One network, created before the extreme event (e.g. one week before ) 
and one, created after it (the last day of our data collection interval) are compared. 

We compare the properties of both networks. So we can see what changes are triggered by the external events. 
Later the cross-correlation between all nodes inside this local networks are calculated for the period 
before and after the extreme event.

Can an increased cross-correlation be detected? 

We define a timeframe, within this time an extreme event is seen as a link between all nodes with an extreme 
event in this frame. By shifting the position of the timeframe and by a change of the length of this frame 
we get different networks. The analysis of the properties of such partial networks can tell us more about the 
localisation or the distribution of extreme events. Do such relations dominate only in nodes of the same 
category or in local linked subnets? Is such a kind of coherence an indicator for an upcoming link between 
unrelated pages?   \\

\textit{d. Statische Netzwerk-Characterisierung}

1. Verteilung der Peaks innerhalb der thematischen Kategorien, denen die Artikel 
innerhalb von Wikipedia zugeordnet sind. [ P(categorie,t) ]

2. Wie häufig treten Peaks in in einem bestimmten Zeitintervall innerhalb einer 
bestimmten Verlinkungstiefe auf? [ N(dt,zlinks) ] 

Which pages are visited together? How close are they in matter of content?\\

\textit{e. Comparion of networks and measuring dynamic properties}

1. Gibt es zusammenhänge zwischen den Zugriffen benachbarter bzw. verlinkter Artikel?
2. Kreuzkorrelation zwischen Edit- und Access- Zeitreihen von Artikeln in einem bestimmten Subnetz (jeweils Bivariate ZRA)
3. Multivariate ZRA unter beachtung aller Knoten eines Subnetzes

4. Finden die erhöhten Zugriffe in verschiedenen Zeitintervallen eher lokalisiert oder delokalisiert statt?

5. Finden wir eine Korrelation zwischen häufig auftretenden \textbf{Key-words}firfox in Twitter und der Zugriffsstatistik auf 
Wikipediaartikel zu diesen Begriffen? Wird die Erstellung von Wikipediaartikeln ggf. durch Twitter getriggert? 

Based on the current results an available data we are looking foreward to further investigations regarding a more 
detailed static networkanalysis and regarding the dynamical network aspects. We will look at the distribution of 
detected extreme values in access time series over several different topics or categories in general and dependent 
on different time intervalls. An other interesting fact is: How many peaks are there during a period of time depending 
on the length of the periode and the link depth. Which pages are visited together? How close are they in matter of content? 
The dynamical view will be analyzed in more detail to find out, if there are any correlations in the access- and 
edit behaviour in related articles. The relation could be based on the link structure or based on the contemporaneously 
occured extreme events. The next question to be answered is: Can we find some kind of time dependent localisation or 
delocalistion of extreme events in matter of content? Finally we will correlate the content based social network 
Wikipedia with a more dynamic one, the message driven system Twitter. Are there any new created Wikipedia articles 
triggered by extreme events in the Twitter network?




%Dynamics of Wikipedia
\chapter{Dynamics of Complex Systems}
Recently the phrase "attention economy" became very popular. How much attention people give to things, ideas, aspects of personal life or society is related to how much time they spend in or on the respective context. Since time is limited, higher attention in one area leads to lower attention somewhere else. Such changes in attention follow recurring patterns, e.g., exogenous and endogenous bursts. Deviations from simple random models like the Poison process are caused by collective phenomenon, also called herding (\textbf{CITE HERDING}).  

How can the level of attention and changes in attention to resources, which have usually very different properties - following power-law distributions, be measured and predicted? Our approach is based on simple server logs, available as access-rate time series. This time series exhibit bursts and of different kinds. In this chapter we investigate the properties of such time series and burst in more detail. 

The article \cite{Ratkiewicz2010} \textbf{(Ratkiewicz2010)} describes a procedure for quantitative popularity analysis of online content, such as Wikipedia pages. They find that popularity dynamics of, e.g., Wikipedia pages is characterized by bursts. This leads to characteristic features like fat-tailed distributions of inter event times and magnitudes of measured properties like traffic, in-degree, and page size. Typically articles have a burst in the the early phases of the page life-cycle, while fluctuations are observed later on. They state, that articles have more edits as they have more in-links. 

We found, that a higher access-activity comes with a higher edit-activity.
(See image 3.b and 3.c in "Schweden-Report").



We apply a linear fit (fig. ... ) and a power law fit (fig. ...) to the data obtained from two neighborhood networks (2 and 4). Because of the low fit quality we can not conclude that a power law relation or a linear dependency between the both properties exists. 

\textit{How can the KS-Test be used to quantify the quality of a Power-Law fit?}
\textbf{Cite:} Goldstein2004 \cite{Goldstein2004}

Because the models given by Eq. ( \textbf{M1} ) and ( \textbf{M2} ) do not reproduce the long tail in the distributions of the mentioned metrics they propose the "rank-shift model" (see: [31,SM]). This way it is possible to capture how an article gains and accumulates popularity by a sequence of bursts. This which moves the page in to the focus of users.

\begin{equation}
P(i) \propto r_i^{\delta}
\end{equation}

\begin{equation}
P(i) \propto x_i^{-\gamma}   with   \gamma = 1 + \frac{1}{\delta}
\end{equation}

\begin{equation}
Rank-Shift-Modell
\end{equation}

\textbf{Cite 27 Omori},\textbf{Cite 28 Bak}


\textbf{Crane2008} found, that activity bursts of access-activity to videos on YouTube can be described as a Poisson process (for up to 90$\%$). For the remaining huge amount of videos (10$\%$) they identify power-law relaxation quotients which can be split into three categories. They state, this is consistent with an epidemic model, in which a power-law distribution of inter-event times (or waiting times) and epidemic cascades coexist. Epidemic bursts are the cause of future actions, which could again be bursts, forming clusters of extreme events \textbf{(see: EXTREME EVENT PAPERS).}

An extension to this growth or shock models, studied by \textbf{Crane2008}, Yu \textit{et al.} \cite{Yu2015} \textbf{(Yu2015)} proposed a phase representation of YouTube video popularity. They found that many videos go through multiple stages of popularity which can move towards both directions (increase and decrease) and last up to several months (see fig. 1 in \textbf{(Yu2015)}). Such phase information allows them to reduce the average prediction error for future view count gain significantly compared to state of the art methods. \textbf{"How is this working?"}



\include{part2a/textMK}

\section{Dynamics of Correlation Properties in Multi-Layer Networks}

%https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test

Wikipedia, the system we investigate here is a combination of a social network, formed by content nodes, contributors and content consumers. We represent it as a network of multi-dimensionally categorized and interlinked pages about any kind of topics in multiple languages.

In this chapter we analyze dynamics of the editorial process (content creation) and the content consumption as a function of time on a mesoscopic level based on correlation networks. Two different dynamic functional networks are created therefore as described in chapter \ref{CHAPTER_XYZ}. Link properties are obtained from time series sub-sequences of length 30 days after a daily binning was applied to the sequences in hourly resolution. To connect this part of the study to a previous project, we use the same Wikipedia pages as entry points (see Fig.1 in \cite{Kaempf2012b}) for data collection and analysis. 

\subsection{Assumptions and Hypothesis}


\subsection{Editorial Process}
Show properties of correlation links as a function of time for the different neighborhoods.

(A) - during the high peak and on other times as full image ...

(B) - for the whole period as function of 12 measurements


\subsection{Content Retrieval Process}
Show properties of correlation links as a function of time for the different neighborhoods.

(A) - during the high peak and on other times as full image ...

(B) - for the whole period as function of 12 measurements


\subsection{Individual layers vs. Multi-Layer Representation}

Show properties of correlation networks as a function of time.

(A) - during the high peak and on other times as full network image ...

(B) - for the whole period as function of 12 measurements





% Results and Discussion can be combined.
%
% RESULTS:      just describe facts
% DISCUSSION:   subjective interpretation is allowed here!!!


% gives us 11.4 and 11.5
\label{ext.fig.Fig11E1a2}
\input{semanpix/Fig11E1a2/imageLS}


% gives us 11.6 and 11.7
\label{ext.fig.Fig11E1a9}
\input{semanpix/Fig11E1a9/imageLS}


% gives us 11.8 and 11.9
\label{ext.fig.Fig11E1a10}
\input{semanpix/Fig11E1a10/imageLS}

% gives us 11.10 and 11.11
\label{ext.fig.Fig11E1b9}
\input{semanpix/Fig11E1b9/imageLS}


% gives us 11.12 and 11.13
\label{ext.fig.Fig11E1b10}
\input{semanpix/Fig11E1b10/imageLS}




% gives us 11.14
\label{ext.fig.Fig11E2}
\input{semanpix/Fig11E2/imageLS}







\subsection{CC, Dynamic Networks}

- Results from BA Berit \\
- Dataset description in Appendix \\
- focus on network properties \\

\subsection{ES, Dynamic Networks}
- Results from BA Arne; \\
- selection of groups \\
Characterization and classification of articles \\
Characterization of edit and access process  \\
Construction of functional networks for financial data;Conection between Wikipedia and financial data \\
 



\section{Information Flow in Correlation networks}
One of our initial research question was: \textit{Can we measure a
significant
higher correlation link strength for static linked pages?} For all nodes of the
example data sets, the pages and links for the local and
global neighborhood have been extracted from Wikipedia. Such a local subnet
describes a selected semantic concept in it's natural context. It defines the
scope for the following analysis. This selective approach takes only the
closest neighborhood into account and allows a massive reduction of the amount
of data that have to be processed.

The neighborhood of a Wikipedia article consists of nodes with different link
properties. One has to distinguish between simple page links, inter-wiki links,
and external links (which are omitted for simplicity). Wikipedia page links can
also be classified by the categories the pages are related to. In this case
the local links connect pages within a given category and remote links
are all links between articles in different categories. With this assumption
the selection of a context can be parameterized by category names depending on
the research topic one is interested in. In the example of a language dependent
analysis we simply separate `wiki links' and `inter-wiki links'.

According to Fig.\ref{fig.neighborhood.combined} an illustration of
different colored regions in the adjacency matrix, shows a sample network with a
 low local representation. The black areas (see
fig.\ref{fig.neighborhood.combined}) are skipped during the processing
procedures, which means for those
node pairs no cross-correlation analysis was done.
The distribution of the cross-correlation link strengths for pages with a static
link to the core (a,d), for all possible page pairs within the group IWL, LN,
and GN (b,e) and for all possible pairs of pages from two different groups IWL,
LN, and GN in (e,f) are shown in Fig.\ref{fig.RES.LinkDistr.timeevolution}.

In many cases, the number of edit events was too low and it was not possible
to calculate the event-synchronization. Therefore we skipped the calculation of
the functional networks which characterize the Wikipedia edit process.


\begin{figure}[!ht]
\begin{center}
\includegraphics[width=4.5in]{semanpix/PLOSONE/images/LinkDISTRPanel.eps}
\end{center}
\caption{
{\bf Time resolved link strength distribution for functional networks
calculated for the local network of a Wikipedia page.} The functional
network around page 3.1.L. (see table 1) was calculated for two non
overlapping time frames of length 28 days. The top row shows the link strength
distributions for the February 2009 and the bottom row for March 2009.
Colored lines show link strength distributions for measured data at a dayly
resolution and thea areas indicate the results of a surrogat data test based
on random shuffeling of the raw data series as significance test.
The blue curve represents the correlation between CN and the local neighborhood
and correlation between CN and the global neighborhood is shown in red in
panels a) and d). Panel b) and e) show the group internal correlations for IWL
(black), LN (blue), and GN (red). The inter group correlations are shown in
panels c) and d), for IWL and LN (black), IWL and GN (blue), and for LN and GN
(red).
}
\label{fig.RES.LinkDistr.timeevolution}
\end{figure}


\begin{figure}[!ht]
\begin{center}
\includegraphics[width=6in]{semanpix/PLOSONE/images/GroupCorrelation.eps}
\end{center}
\caption{
{\bf Time resolved average link strength for functional networks}
calculated for the local neighborhood network (orange) and the global
neighborhood network (gray) for functional networks a) around page 3.1.G.
and b) around page 3.1.L. (see table 1). The black lines show the logarithm of
the daily access rates for page CN (in both cases the page in German language) 
and the green line shows the average of the logarithm of
the daily access rates for corresponding IWL pages. 
The dashed gray lines show the confidence interval $\pm \sigma$ around the average 
link strength for the global neighborhood for comparison. 
During a periode of increasing international interest in the topic the access rate 
correlations drop to an average value around zero and the access time series correlations 
within the global neighborhood are increased significantly (see region A). 
}
\label{fig.RES.LINKSTRENGTH}
\end{figure}

Fig.\ref{fig.RES.LINKSTRENGTH} shows the time resolved average link strength for
functional networks calculated for the local neighborhood network (orange) and
the global neighborhood network (black) for functional networks around page
3.1.G. (a) and around page 3.1.L. (b). Region A indicates the time of
increasing attraction within the global context. At the same time the average
correlation between the access-rate time series increases and the correlation
between time series measured in the local context decreases. Correlation values
around zero, which means time series are uncorrelated (see region
B in Fig.\ref{fig.RES.LINKSTRENGTH}) is normal for the global context of a
page with high local relevance, such as 3.1.L., but in the presence of the
burst, also the correlation increases. Region C shows decreasing correlation
in local and global context after an exogeneous burst but within the local
context, the time series have a stronger correlation. This can also be seen in
fig.\label{fig.RES.LinkDistr.timeevolution}a,d and
fig.\label{fig.RES.LinkDistr.timeevolution}b,e .


\section{Interpretation of correlation link strength distributions}
Calculated functional links, for which the link strength distribution is
plotted in Fig.\ref{fig.RES.LinkDistr.timeevolution}, have to be filtered to
extract highly relevant links which carry information. Which links are and are not 
relevant depends on the selected topic, the context, and the selected
algorithm for link strength calculation, and it can be influenced by the selection
of an appropriate filter method as well.

A fixed threshold, adaptive thresholds, or graph based filtering like the planar
maximum filtered graph method (PMF) are common methods for this case. The usage
of a fixed threshold value is appropriate here, because the link strength was
calculated as the normalized correlation coefficient. The $l_s$ values are used
to create an adjacency matrix which represents a local functional correlation
network around a central node CN at a given time. The example in
fig.\ref{fig.statnet}.b shows a comparison between such a correlation network
and the underlying static network for node 3.1.L (see table 1) with ${\rm
CC}_a^{(i,j)}(t) > 0.75$.

One can assume to find stronger connected (or even less dense networks)
depending on the level of public interest of a given topic. Correlation
networks with larger connected components are the results of an access process
which results from higher correlated actions. Such correlation within
the user community can be the result of a stronger media presence of the
topics or caused by a natural phenomenon like an earthquake, floodings,
etc.

Uncorrelated usage of the pages, which means that there is no common interest
during a given period in time in the topic, would lead to a link strength
distribution like shown for the surrogate data tests.

We assume that exogeneous bursts in the access rates are caused by a
common interest of many users within a short time interval. This means,
access activity is highly correlated and leads to high  correlation
link strengths. Fig.\ref{fig.RES.LinkDistr.timeevolution} and the two sided
Kolmogorow-Smirnow test \cite{Young.1977} allow the
conclusion, that exogeneous bursts have a significant influence on the link
strength distribution of a functional network. During the month before the
exogeneous burst significant differences between the measured link strength
distribution and the surrogate data was determined for the groups 1 and 2 (see
indices in Fig.\ref{fig.neighborhood.combined} and for groups 4,5, and 8 we
could even find a very high significance with $p$ values less than $10^{-30}$.
In the presence of the burst, the significance of differences of link strength
distributions changes during the second month.

The correlation between pages within the core decreases but the correlation
between pages in the neighborhood increases. This indicates that the interest in
the topic increases and people access pages within the neighborhood more often
as a result of an increase of relevance of the topic.
\clearpage
\newpage


%\begin{equation}
%a=\frac{N}{A}
%\end{equation}%

%\nomenclature{$a$}{The number of angels per unit area}%
%\nomenclature{$N$}{The number of angels per needle point}%
%\nomenclature{$A$}{The area of the needle point}%

%The equation $\sigma = m a$%
%\nomenclature{$\sigma$}{The total mass of angels per unit area}%
%\nomenclature{$m$}{The mass of one angel}
%follows easily.




\chapter{Data Driven Market Studies}
\label{chap.DataDrivenMarketStudies}
%------------------------------------------------------------------------------------------
%TODO cite \cite{journal.pone.0026472} 
%E.CITE \cite{journal.pone.0026472}  http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0026472
%
% this triple tells me: a citation in this place has the key \cite{journal.pone.0026472} and the original document is
% located here: http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0026472
%------------------------------------------------------------------------------------------
The global financial crisis, which started in 2008 with the bankruptcy of \textit{Lehman Brothers}, can be seen as one of the reasons 
why systemic risk has received much more awareness in recent years. Andersen \textit{et al.} \cite{10.1371/journal.pone.0026472} emphasize the fact that "the excessive risk taking by major financial institutions pushed the world's financial system into what many considered a state of near systemic failure in 2008." further more they argue "the \nomenclature[R$I$]{IMF}{What is IMF?} IMF for example in its yearly 2009 Global Financial Stability Report acknowledged the lack of proper tools and research on the topic". This leads to the question, how such disruptions can be identified before they are propagated across globally connected financial markets. Further, description of such a process' dynamics is still an important but unsolved problem. Methods from complex systems research are considered to be appropriate, especially as interconnected financial markets can be modeled as complex networks. This allows combined studies using empirical data 
to derive analytic models, and simulations also known as "stress test", which are conducted using computational and simulation techniques to test several and influencing factors like changes in unemployment rates, interest rates, or \nomenclature[R$G$]{GDP}{What is GDP?} GDP. There are a variety of research questions which can be addressed via less specific and less complicated approaches, e.g. market studies based on customer surveys or public available statistical data. More and more companies use such data sets for market analysis. 

%\textbf{What are the goals of data driven market studies?}
Market studies can be done in many different ways with different focus and different data collection techniques. Two different types are considered here. The first example is one selected economic sectors, which is defined by entities of different types, like companies, technologies, products, and community projects. All those entities define a markets, in this case "the emerging Big Data market". The second type primarily exists of entities which are all of the same type, in this case companies for which stocks are traded in international stock exchanges and market indices, which can be seen as groups companies. 

Data is collected indirectly in the first case and direct data collection can be applied in the second case. In one case, the interest of people can be studied, while in the other case also system internal aspects, like different behavior in price changes can be used for predictive models. It is important to define in advance, what kind of information is required and this can only be done if the research question is clear. In our case, we want to find out, how well several companies are represented in Wikipedia. This allows a qualitative interpretation of collected usage data, which for itself should be used as an indicator for user interest in the pages, representing the topic. Large scale studies from international markets can be build on top of this data collection approach only if the influence of the languages can be understood and used for normalization of collected raw data.

Dynamic properties like size, growth rate, connectivity between markets, or even public representation in several media channels can be derived form public data sets. Can data from Wikipedia or Google Trends be used as a reliable source for market models is analyzed in this work. Methods from previous chapters, like e.g. calculation of time resolved relevance index (see \ref{EQ.REL}) %TRI 
are applied. The results show limiting factors and a new approach for data set quality assurance is provided. 

This chapter addresses fundamental qualitative questions. Can social media applications like Wikipedia and global search log data, provided by Google Trends (which is build around the world most popular search engine) be used for trend and correlation analysis? What are the differences between both types of data sets and what can be learned from both? 

The main idea behind this pre-analysis is, that only access-rate data series from pages with comparable representation can be used for further comparison and correlation analysis. This requires at least identification of languages and regions on earth, which should not be considered as a valuable contribution to such data analysis procedures, even if it sounds interesting to use all possible languages, sometimes it gives no new information, but on the other hand side, data should not be skipped without a plausible argument, one which is stronger than: "This data was not available."

Some important aspects, which might have been hidden in the flow of events during 2008, when Lehman Brothers has been in trouble and unexpected market movements happened will be outlined now. What happened in 2008? During the first half of the year, the value of Lehman Brothers' stocks decreased by $73\%$ \cite{wiki.lehman.6}. This was leading to an official announcement about the future strategy of the company. The number of employees should now be decreased by $6\%$ which means 1500 people should loose their job. This kind of bad news can be see as a direct connection between business activity and social life or even society. When the Korean government announced plans to buy the tumbling company Lehman Brothers, the value increased by $5\%$ on one day, and even by $16\%$ during the period of a week, but as soon as problems had been reported, the value decreased by another $45\%$ \cite{wiki.lehman.9}. 
This numbers illustrate the influence of news, and even if the message is wrong or incomplete, an immediate negative effect can be recognized. The strong relation between companies in financial markets is highlighted by the impact, the losses had on the large stock indices. As a result of the missed deal with Korea not only the companies value decreased, also the S\& P 500 index lost $3.4\%$ on one day and the Dow Jones lost 300 points ($\approx 2.6\%$) and some days later the Dow Jones lost $4.4\%$ on one day, which was the highest loss of the stock index, which covers the 30 most important American companies, since the attacks on September 11, 2001.
How such a disruption of the financial sector can be spread via several network channels is illustrated in figure  \ref{fig.CommunicationProcessesAndCouplesNetworks}.    


\section{What Wikipedia \& Google Trends Tell About Market Dynamics}

%\subsection{Motivation and Introduction}
Financial markets can be seen as examples of highly interconnected systems with a variety of obvious facets or observable variables 
and also many hidden layers of non obvious relations. Many studies have investigated either structural \cite{a1},\cite{a2},\cite{a3},\cite{a4}, 
%TODO SHOW studies about structural props 
or  dynamical properties \cite{b1},\cite{b2},\cite{b3},\cite{b4} 
%TODO SHOW studies about dynamical props 
using mathematical models and tools which have been developed in the context of climate research \cite{c1},\cite{c2} and physiological research \cite{d1},\cite{d2}. Nowadays, as more and more large data sets are publicly available to researchers, a convergence of such 
methods can be recognized and a set of key measures has been established \cite{BOOK.NEWMAN},
%TODO cite NEWMAN
but those measures often over emphasize the structural properties of underlying networks (see figure \ref{fig.FinancialMarketDataStructure}), and ignore their complex dynamics and also their embedding into other surrounding networks, e.g. communication networks, as shown in figure \ref{fig.CommunicationProcessesAndCouplesNetworks}.

\label{ext.fig.FinancialMarketDataStructure} 
\input{semanpix/FinancialMarketDataStructure/imageLS}

%TODO find companies which are connected based on investments from large external investors.
Financial market networks do not only contain internal links or connections. Very important are also relations to external systems
like media companies, resource markets, technology companies, and maybe most importantly to the society in general. 
Such markets can not function without information flow. Trading decisions are usually based on information which is available to traders. A Trader can be a person, acting on a timescale of minutes down to seconds, but not much faster. Sine stock market transactions can also done via automatic or semiautomatic processes trading activity can be measured on a timescale which in the order of milliseconds. This kind of automatic trading is called "high frequency trading" and it is out of the scope of this work. A study by Kirilenko \textit{et al.} \cite{Kirilenko2011} analyses the market events on May 6, 2010 which is called: "The Flash Crash". Even if high frequency trading can not be seen as the reason for this crash, one has to notice, that such automatic trading systems can probably influence markets, even if not much detailed information about applied strategies is available. 

%http://www.ftm.nl/wp-content/uploads/content/files/Onderzoek%20Flash%20Crash.pdf   


\label{ext.fig.CommunicationProcessesAndCouplesNetworks} 
\input{semanpix/CommunicationProcessesAndCouplesNetworks/imageLS}
The majority of people, no matter what there role or position is, consume a lot of information via peer to peer communication, nowadays based on Internet applications like messaging services, provided by different complementary communication networks like Email services, Twitter, Facebook, Google+, commercial financial service portals, or public web pages like the encyclopedia Wikipedia. By analyzing the representation of stock markets in Wikipedia and measuring correlations between stock market data (like trading volume, or volatility) and the access rates to corresponding groups of Wikipedia articles one can study the role of a protruding social network in the economic cycle. Can a measurable increase of interest in a special topic represented by a news article or a Wikipedia page be used as an indicator for changes in demand in financial markets? Questions like this one are important for individual decision makers but can we also identify a global state of markets  and their dynamics based on such time series analysis?

A recent study by Preis et. al. \cite{Preis.GoogleStockMarketRelation} 
%TODO cite PREIS GoogleStockMarketRelation
showed that following query volume for financial search terms on Google could predict stock market movement and another study by  Alanyali et. al. 
 \cite{Alanyali.GoogleStockMarketRelation} 
%TODO cite Alanyali GoogleStockMarketRelation 
%http://www.nature.com/srep/2013/130425/srep01684/full/srep01684.html
demonstrates a significant change in correlation between the daily mentions of companies in the Financial Times in the morning and how much they were traded on the stock market during the day. Those results support the hypothesis of an existing mutual influence between financial markets and the news which is also illustrated in figure \ref{fig.TwoTypesOfSocialMediaMarketRelation}.

Furthermore it was previously shown that increases in the number of searches for a company name made on Google are correlated with increases in trading volume for that company’s stock \cite{Preis.GoogleStockMarketRelation}. More importantly, it has been demonstrated that during the period of eight years (2004 to 2011), increases in searches for financially related terms tended to be followed by decreases in the price of the Dow Jones Index Average. This finding is in line with the proposal that Google search data may provide insight into the process of traders seeking information to help them determine optimum future decisions. Since data from Google Trends is heavily used also in other domains, such as analysis of epidemic dynamics (see \cite{GoogleFLU}) one can clearly see the value of such type of analysis but on the other side a critical reflection and a quality discussion is more important than just processing more and more data. Lazer \textit{et al.} \cite{Lazer2013} discuss typical problems and show that typical mistakes like overfitting a small number of cases,  temporal autocorrelation which leads to non randomly distributed errors, and a lag of stability of the applied method are critical factors which all lead to wrong models. In case of Google Flu the flu prevalence has been overestimated in 100 weeks out of 108 (starting in August 2011, see figure 1 in \cite{Lazer2013}).

However, other online data sources may also possibly provide insight into trader information gathering processes. Whilst many Internet users rely on Google to locate a range of different useful information sources online, the online encyclopedia Wikipedia is a widely-used central reference source for information across a number of subjects. As such, one can consider Google as a provider of data that gives insights into what information Internet users are looking for, whereas Wikipedia data provides insights into what information Internet users in fact use. Thus, we have investigated whether changes in frequency of views of certain Wikipedia pages also anticipate subsequent changes in stock market prices.  

A few key practical differences exist between data on Wikipedia usage and data on Google search keyword usage. 
Firstly, some search terms have multiple meanings. For example, the term "Apple" is widely recognized both as the fruit and as the technology company. Google data, as retrieved from Google Trends \ref{URLGoogleTrends}, provides little insight into which meaning was of interest to the Internet user. Recent versions of Google Trends software provides a list of related searches, but a clear semantic context is not provided. In contrast, a Wikipedia page, other than those designed specifically for disambiguation, is about one topic only. In some cases a pages like the page with title "SOLR" (from English Wikipedia project) redirects to another one, here to a page with title "Apache SOLR". Even if a user is not aware of the specific title, it's interest will be counted and the relevant information is provided. The influence of keyword selection will be discussed and illustrated in a later section of this chapter. Disambiguation pages make it possible to consider changes in the number of views of the page about Apple the company separately to changes in the number of views of the page about apple the fruit. Redirect pages provide an implicit aggregation of several click trajectories.

Secondly, where data on Google usage largely relates to per-week changes in search volume, we are able to access data on hourly changes in Wikipedia usage. Thirdly, Wikipedia data describing access of all pages across the Wikipedia encyclopedia since 2007 is freely available, whereas some restrictions exist on accessing large volumes of Google usage data.

We are interested in the role of Wikipedia as a public crowd based source for news in the context of market activity and we are especially focused on the readers side, because the number of article downloads reflects the state of a larger part of the society which can be less influenced by a single opinion of a single publisher using a shiny picture or provocative headline on page one. Readers select articles intentionally and they are not flooded by topics which just sell well because Wikipedia is not a commercial system.\\

\subsection{Description of the Analysis Procedure}
\label{sec.STEPS.MS}
The proposed method for a data driven social media based market analysis is build on the assumption, that there is a direct relationship between trends in emerging markets especially in technology oriented markets, or between movements in financial markets and changes in the user activity, e.g. in Wikipedia or Google search volume. The approach combines a qualitative description and a quantitative analysis of connected information flows which are manifested in Wikipedia content, stock market prices and in usage data, which is derived from server logs by converting individual page request log messages into access-rate time series. We apply time series analysis paired with network analysis in the context of interconnected systems. Such systems and their possible interactions are illustrated in \ref{fig.TwoTypesOfSocialMediaMarketRelation}. System A shows a well studied social media system Wikipedia, which consists of many linked articles (\textit{i},\textit{j} and \textit{k}) organized in categories (which are omitted for simplicity). Wikipedia works as a data collection stub and represents the process of personal information retrieval. So far we identified two classes of systems about those people collect information via Wikipedia. System B represents an emerging market which has not yet a well defined nor a stable structure. What established companies (global players or local leaders) belong to an emerging market and what company might be considered to be part of such a market in the future has to be derived from the data set without a priori knowledge. Figure \ref{fig.TwoTypesOfSocialMediaMarketRelation} illustrates three different elements driving the evolution of such an market: established companies, new fast growing technologies and new companies which are filling the gaps between the big players and sometimes disturb them. Selection of companies is much easier in the case of financial markets. Either one selects nodes by region or by industry sector and usually the stock market indices represent such groups well.\\

\subsubsection*{Preparation: Define the Scope}
An appropriate selection of representative Wikipedia pages is crucial and influences the analysis results. Depending on the subject and objectives of the research project one has to select and to characterize the data sources carefully. But on the other side, it is relatively easy to inspect the data and to measure the quality in order to identify biases caused by the growth of the underlying system or dissimilar distribution of properties like text volume per page or number of links on page, which is typical for heterogeneous networks like the content networks we study in this work.

	Although large computing clusters are available in many places, one has to care about efficiency of the procedures because computational resources are still a limiting factor. Calculation of single nodes properties is an uni variate analysis and can be done on any group of time series. Obtained results are grouped and sorted depending on several parameters which are considered to be influencing any measurement. This type of calculation is usually of a linear complexity. Bi variate or multivariate procedures (like calculation of link strength or dependency links) require a precise definition and characterization of the chosen input data. One has to think about the number of required time series pairs (or n-tuples with $n=3$) which have to be processed in order to create correlation matrix, which is a symmetric matrix, and self-loops can be omitted (for each pair, only one direction has to be calculated) is a slightly more expensive operation of the order $O(N^2)$. A dependency network will be created from an dependency matrix, which is usually unsymmetric and which is calculated not for pairs, but from groups of three time series. For all page pairs the influence of a third time series, which is not already a part of the pair, has to be calculated. This leads to a much higher complexity of the algorithm, which is of the order of $O(N^3$). It is important to classify analysis algorithms in such a way because this allows the estimation of its' runtime based on simple benchmarks. The point in time at what filtering, aggregation and grouping is done differs for many algorithms and has also a strong impact on the overall performance. Intermediate results can be stored and reused in some cases but the more parameters have an influence on a certain procedure, the more expensive will all the organization of intermediate data be. Beside those considerations regarding required computational resources and optimized procedures the most important factor for content selection is the selected topic itself. 
	
Creation of the analysis scope is done by defining a representative subsets of Wikipedia pages. Any Wikipedia page can be selected as the entry point for data collection. Starting from one central node (CN) all connected pages can be retrieved and depending on the depth parameter, specified for the crawling procedure, a small local or even a very large, maybe the full network, can be collected.

%
% A semantic network, or frame network, is a network which represents semantic relations between concepts. 
% This is often used as a form of knowledge representation. It is a directed or undirected graph consisting of 
% vertices, which represent concepts, and edges.[1]
%   Jump up to: a b John F. Sowa (1987). "Semantic Networks". 
%   In Stuart C Shapiro. Encyclopedia of Artificial Intelligence. Retrieved 2008-04-29. http://www.jfsowa.com/pubs/semnet.htm

A restriction to the next neighbors is often useful but sometimes an inclusion of the second neighborhood is required, as it enables a comparison or even a relative measurement of a certain property within a naturally given well defined context. Wikipedia offers different types of links and although it is not a semantic network in terms of the definition given by \cite{SOWA1987} the different link types each express a special meaning which allows the definition of different neighborhoods. A generalization of the method and an application to arbitrary semantic networks, which are usually stored in triple- or quad stores is possible. In this case, the group definition will be done via SPARQL queries \footnote{SPARQL: according to \cite{wikipedia.sparql}, it is a recursive acronym for \textbf{S}PARQL \textbf{P}rotocol \textbf{a}nd \textbf{R}DF \textbf{Q}uery \textbf{L}anguage) and a query language for semantic databases, which is able to retrieve and manipulate data stored in Resource Description Framework format.}  which replace the need of a the web crawler. 
   
Wikipedia pages contain external links to resources which are hosted on servers outside Wikipedia. Those links are not used in our procedure because no click count data is available for such resources. This is not a general limitation, because in a future version 
one can easily extend this approach if page access statistic would be provided by any webserver.

Here we limit click-count data collection to page with page links and so called inter-wiki links. According to figure \ref{fig.networkGroupsIllustration}
%
%E.DEP.IMAGE network_groups_illustration
% This image shows the scope and the groups (see chapter methods)
we define and work with four groups: CN, IWL, LN, and GN. All pages, available via inter-wiki links starting at the central node, are considered to form the group IWL. A local neighborhood with group label LN is defined around the central node. All pages linked to any page from group IWL define the global neighborhood with group label GN. 

This implicit segregation of groups by topic and language enables a language dependent analysis without the additional effort of parsing, stemming and translating multilingual texts, and if one is interested in a semantic concept independently of language specific differences, one can define a network core which consists of CN and IWL nodes and a network hull which is formed by LN and GN. In all cases the neighborhood is defined by the next neighbors which are only one step away from core nodes. In order to collect primarily nodes which are really in the context of the research scope a careful verification is required. If no links and no page volume was loaded by the crawler, this might be an indicator for a redirect page or a wrong page title. 

\subsubsection*{Step 1: Structural Analysis}
The first step is a qualitative analysis, which is based on page content. It shows, how well a market - or even more general: a topic - is represented in Wikipedia. Therefore we apply the relevance analysis (introduced in chapter \label{chap.ContextsensitiveRelevance}). According to intermediate results one has to refine the selection of nodes. Finally the bias which might be introduced by connections to influencing nodes in the close neighborhood can be evaluated.\\

\subsubsection*{Step 2: Temporal Relevance Analysis}
As a second step a more quantitative analysis is applied to pre processed and cleaned time series data. For all selected central nodes which define the scope of the study, one calculates the time dependent representation index (see chapter \label{chap.TRI}). This allows an identification of ranges in time during which significant changes in peoples' interest can be recognized. This approach goes beyond extreme event analysis as it does not need a definition nor an extraction procedure for events. The time resolved relevance index is a relative measure which allows extraction of large changes regarding the local neighborhood no matter if the system is in an equilibrium. Especially in emerging markets this robustness is an important aspect.\\

\subsubsection*{Step 3: Internal Correlations}
Directed permanent connections between nodes are expressed by page links which have been used for an implicit group definition. But temporal correlations (see C1 and orange arrows in figure \ref{fig.TwoTypesOfSocialMediaMarketRelation}) reveal more information especially in the presence of short term activities which do not have a direct influence on the underlying link structure. 

\subsubsection*{Step 4: External Correlations}
Finally external correlations between interlinked systems are analyzed by partial correlation (see C2 and green arrows in figure \ref{fig.TwoTypesOfSocialMediaMarketRelation}) and by meta correlation analysis (see C3 and black arrow in figure \ref{fig.TwoTypesOfSocialMediaMarketRelation}). 

\label{ext.fig.TwoTypesOfSocialMediaMarketRelation}
\input{semanpix/TwoTypesOfSocialMediaMarketRelation/imageLS}
%TODO add the figure caption from the Google doce document to this caption here.

We use a data set which can easily be obtained from existing databases and public archives as time series. Large data sets allow analysis of individual properties of lots of single elements which form a complex system. The time series data is also used to reconstruct correlation and dependency networks which expresses the strength of intra-component correlations and the direction of influence. Inter component correlations are calculated for pairs of different element types and lead to bipartite networks, which expresses the coupling of different subsystems forming the system of complex system. One remaining but nether the less very important questions is: How to define links between elements and subsystems? How are time delays handled the correct way and how can disturbing artifacts be identified? 

In this work primarily the cross-correlation as well as the event-synchronization have been used as a measure to define link strengths which are the fields in a (re)constructed adjacency matrix and represent the underlying complex system. This allows us to apply several filter techniques and a characterization of dynamic properties of those networks in time. Our approach is in line with existing time series analysis methods, which also depend on the correct filtering of raw data and our results show, that these aspects can not be handled in a single unified method - it depends on the properties of the measured data and the kind of effects that should be analyzed. As a consequence we introduce two categories of systems for which either only the internal analysis steps 1, 2, and 3 (System B) or all four steps (System C) of the proposed procedure can be applied as illustrated in figure \ref{fig.TwoTypesOfSocialMediaMarketRelation}.

Before the results of three different case studies are shown a critical discussion of data from Google Trends is given. The first case study is focused on an emerging technology, and the related Big Data market, the second example covers financial markets and finally some quantitative results from an early study underline the importance of better data collection techniques. 

\newpage
\section{Critical Reflection on Google Trends: Common Sources of Errors in Trend Analysis}
%http://www.mathworks.de/de/help/econ/trend-stationary-vs-difference-stationary.html
%http://www.mathworks.de/de/help/econ/detrending.html
Stationary processes are studied on time series data from a variety of different sources. One has to assure, that trends are removed
appropriately because results would be biased and unreliable otherwise. The trend can also be of primary interest. 
Time series decomposition is applied than to decompose the measured time series into different trend sources. 
E.g. seasonal components and influences of the daily cycle have been removed as described in previous chapters using 
a non parametric filter approach and finally we calculate sliding averages in order to eliminate the stochastic part of the data. 

The theoretical description of non stationary processes is given in chapter \ref{chap.NonstationaryProcess}. In many cases, there are obvious linear trends or even trends of a higher order in the series, which should be incorporated into models of such a process. 
Here we are interested in trends because they show, how a system evolves over time in which it is not in an 
equilibrium and this means, the system for itself is involved in a process, e.g. a growth or restructuring process.

Trend detection methods which measure trends in people's interest based on search volume in the world's largest web search engine
can be useful for arbitrary interdisciplinary studies, which are built on data which expresses a relation between people and 
any type of entity or topic via socio-technical systems. This also shows how important a critical reflection about the data collection, pre processing, and analysis process is, especially if data sets are large, widely accessable and not well documented. Reproduceability is one of the major unresolved problems in modern data analysis projects. This problem can be addressed by automatic creation of machine readable process documentation and integration with log data analysis like explained in chapter \ref{chap.ETOSHA}.     
 
 
 A first obvious problem comes from different timescales or different sampling rates, e.g. data from Google Trends is available at a weekly resolution and allows trend analysis in the range of months to years while Wikipedia data is not limited that much. Hourly data is available for access-rate data, but because of the daily cycles which are caused by time zones (see chapter \ref{chap.WIKICYCLES}) it is better to use daily averages instead. A direct correlation analysis is not possible with such data before an appropriate preprocessing, e.g. aggregation or filtering is done as shown in section \ref{sec.preprocessing}.  

If a deterministic mean trend is detected and if it can be estimated, it can be removed from the data. One gets a residual series which 
describes a stationary stochastic process. If the mean trend is stochastic (in case of a difference stationary process), one has to differentiate the time series and the result yields a stationary stochastic process. This step would emphasize the stochastic part but a reliable result can only be found if a sufficient number of realizations is available for analysis. In case of Wikipedia or Google Trends this seems to be possible, e.g. if search volume for a specific topic can be measured for several regions, and if for all regions the same process characteristics can be assumed, but this is usually not the case. Regions differ regarding cultural aspects, density of inhabitants, quality of internet connectivity, and demographics which also influences usage patterns of social media applications. According to \ref{mathworks.trend-stationary-vs-difference-stationary} "the distinction 
between a deterministic and stochastic trend has important implications for the long-term behavior of a process: "\textit{Time series with a deterministic trend always revert to the trend in the long run (the effects of shocks are eventually eliminated). Forecast intervals have constant width. Time series with a stochastic trend never recover from shocks to the system (the effects of shocks are permanent). Forecast intervals grow over time.}" And Hamilton \textit{et al.} \cite{Hamilton1994} show that for any finite amount of data there is a deterministic and stochastic trend that fits the data equally well. 
% http://www.mathworks.de/de/help/econ/trend-stationary-vs-difference-stationary.html
% [1] Hamilton, J. D. Time Series Analysis. Princeton, NJ: Princeton University Press, 1994.

Such trends are caused by natural properties of the system's life cycle phases which can be identified on several scales, such as life cycle phases of individual Wikipedia articles, which have been investigated by Kittur and Kraut in \cite{Kittur2008} or full Wikipedia projects, which have been discussed in chapter \ref{chap.WikiLifeCycle} and \ref{Schreck2012}. In such statistical analysis procedures a small number of observable variables is used to describe the collective behavior of a large system. In case of the page or project life cycle it is obviously clear, that appropriate management procedures can have positive impact, while negative aspects like exaggerating media coverage can have very negative impact. One reason for this are existing closed feedback loops which are also characteristic properties of complex systems. This idea was already illustrated in figure \ref{fig.CommunicationProcessesAndCouplesNetworks} and can even be generalized to news articles in any media channel, to products, and even to companies, because such entities or systems show comparable life cycle patterns as reported in \cite{Castillo2014}, \cite{Wood1990}, and \cite{Wang2006}.

% Product Life Cycle: http://www.atkearney.com.au/documents/10192/7506875d-5e6b-4c9e-b9b0-a981142be5a6
% News Life Cycle: http://arxiv.org/pdf/1304.3010v3.pdf

From Wikipedia access-rate time series and Google Trends search volume data one can clearly extract long term trends as well
as fluctuations on shorter time scales (daily and weekly fluctuations respectively). But before such results can be interpreted or processed in advanced analysis procedures, the quality of the collected data and several influencing factors have to be reviewed. 
According to Weidema and Wesn\ae s in \cite{Weidema1996} a set of "\textit{independent data quality indicators are suggested as necessary and sufficient to describe those aspects of data quality which influence the reliability of the result. Listing these data quality indicators for all data gives an improved understanding of the typical data quality problems of a particular study. This may subsequently be used for improving the data collection strategy during a life cycle study.}" 

One of the prominent use cases, which is built on Google Trends data is the Google Flu project \cite{GoogleFLU}. But as Lazer \textit{et al.} show in "The Parable of Google Flu: Traps in Big Data Analysis", one has to look clearly on collected data, especially to eliminate bias and to strengthen own arguments, because in most cases one has no control about the measurement procedure. So it is even more important to study the system from which the data is derived from with precise and transparent and traceable methods as requested in  \cite{Weidema1996}. 

\label{ext.fig.GoogleTrends1}
\input{semanpix/GoogleTrends1/imageLS}

Data is just as good as one understands it's origin, or in other words, the data creation or measurement procedure is very important and influences the final result and if the internal operational modes of a social media application like Google Search or even much simpler web applications like news portals are changed, e.g. in the context of business development, all results 
of data collection procedures are directly influenced. Only if it is possible to distinguish between external influences and such internal aspects those data sources can be considered as a reliable resource for research. Google offers keyword recommendations derived form a given search term. This new feature is one example for an internal instability of the data collection procedure. Furthermore one should consider an implicit source for correlations in keyword usage series preferably on a short scale but also on a large scale, because user profiles are stored with many details about a user's trajectory through the Internet in web browsers. For weekly data, this might be more likely to be a reason for correlations than suggested keywords, but anyway, the assumption of an stochastic process is not correct in this context since this process has memory.
  
  

An appropriate selection of domain specific search key words according to the goals of a study and the research hypothesis will have 
an important influence as well. Non sufficient keywords or ambiguous selection of terms are two more critical aspects. Figure \ref{fig.GoogleTrends1}.a gives an illustration using the search terms \textit{Zookeeper} and \textit{Apache ZooKeeper}. Some of the small peaks in the black curve are caused by events in zoos, e.g. on  September the 2-nd, 2009 NBCNEWS reports: "\textit{A teenage boy visiting a zoo on Wednesday fell into a lion pit that’s part of a set for the upcoming Kevin James and Adam Sandler movie \textit{The Zookeeper}, but he wasn’t badly hurt.}" This statement is not a clear explanation but it contains the key to understand the peak in summer 2011, which is caused by the increasing interest in the mentioned movie, which has no relation to the software project.

Unknown normalization factors have to be corrected, either be renormalization or standardization like shown in chapter \ref{chap.Math}. Figure \ref{fig.GoogleTrends1}.b shows data for keyword (A) \textit{Hadoop} and keyword group (B) \textit{Apache Hadoop}. The exponential trends have been fit with the model $y(t) = A_1 \cdot exp( \frac{t}{\tau})$ to estimate the growth rates $\tau_A=225.04$ and $\tau_B=169.78$ and although the constant factors $A_1$ are comparable (4.63 for A and 4.177 for B) the significantly larger fluctuations of the curve with the higher values indicates the need for a renormalization. The reason for this difference can be explained by the way how data was retrieved. For two different request individual scaling is done before data is provided but scaling factors are not part of the response and later on, it is often not clear, what data came from which request. Because of this and some more technical limitations large reproduceable analysis projects based on Google Trends data are complicated.

\label{ext.fig.GoogleTrends3}
\label{ext.fig.RawVSNormalizedTrends}
\input{semanpix/GoogleTrends3/imageLS}

Figure \ref{fig.RawVSNormalizedTrends} shows trends fitted to raw data from Google Trends compared to standardized time series (for details about the preprocessing see section \ref{sec.standardization}). In figure \ref{fig.RawVSNormalizedTrends}.a one can clearly see increasing linear trends, but the very small slope in the case of the blue curve is misleading. According to figure  \ref{fig.RawVSNormalizedTrends}.b there is no significant trend. This image illustrates the market dynamics in the No-SQL data base sector. While user interest in the topics \textit{Apache Cassandra} and \textit{Apache SOLR} shows a saturation and no significant growth during the last years, MongoDB, Neo4J and Hadoop attracht constantly more interest with some differences in seasonal patterns but with the same linear growth rate. 



This section highlighted some of the most important problems with influence on trend analysis and how they can be solved or avoided. The following three sections will show how the data driven market studies can be applied to multiple systems of different types, e.g. emerging technology markets, financial markets and even connected systems like social media applications an financial markets. 

%\label{ext.fig.GoogleTrends2}
%\input{semanpix/GoogleTrends2/imageLS}

%\clearpage
%\newpage
\section{Case Study I: Identifying Driving Forces in an Emerging Market}
We study the \textit{Hadoop ecosystem} from an economical and sociological perspective using social media data, e.g. Wikipedia content and click count data, which both represents user interest in a topic which can be scientific, political, or even a commercial entity like a company or product. User interest can also be derived from sales statistics, like the number of sold mobile phones or even the trading volume of stocks at stock exchanges. In many cases it is not possible to get such data, as it is internal and mission critical for many companies and stock market data is an exception because of several regulations.
To represent the Hadoop market a set of 42 Wikipedia pages from six categories was selected. Those groups represent multiple facets of the Hadoop market. We choose the following identifiers using the pattern (short label, color code). The set contains 5 of the global players in IT business (GP, blue), 3 hardware vendors, two which have relevant offerings around the data processing platform Hadoop and one without (HV, green), and also 6 important early adopters (EA, violet) of the new technology. The majority of selected nodes represents the 17 fast growing young start-up companies (NC, red) which drive this emerging market. The goal is to find out: How those entities are related to each other and if they can be grouped or clustered based on social media metrics? We add 5 more pages to the corpus in order to cover the probably influencing core technologies (TEC, cyan). This is important because the whole field around the Hadoop market defines a strong dependency build on existing business relations. Because the most innovation comes form open source software projects we add 7 pages about Apache software projects (SW, orange). Some of those projects have been started already ten years ago and some others are really young fast growing projects. All page names are listed in table  \ref{tab.CS1.tab1}. 

%TAB DATA
\label{ext.tab.CS1.tab1} 
\input{semanpix/table.HadoopMarket/imageLS}

\subsection{Data Set Preparation}
All local neighborhood networks for pages listed in table \ref{tab.CS1.tab1} have been loaded from Wikipedia using the Mediawiki-API \cite{API.mediawiki}. The corresponding access-rate time series have been extracted from the public data repository, provided by the Wikimedia Foundation using the Hadoop.TS \cite{Hadoop.TS} tools (see section \ref{sec.preprocessing} and chapter \ref{chap.ETOSHA} for processing details). Time series data is available at an hourly resolution for years 2009 to 2011 and has been transformed to daily resolution which represents the total number of clicks per page per day. Details about the influence of the aggregation on time series properties are discussed in section \ref{sec.dataaggregation}.

\subsection{Representation of Competitors in Wikipedia}
The number of available pages within each neighborhood network around selected central nodes are shown in table \ref{tab.CS1.tab2}. Some of the new companies, like MapR, Hortonworks, and Karmasphere are not yet represented in Wikipedia with an own page but they are cited or mentioned on other pages which shows at least a direct but even weak connection to the topic. 
The small number of inter-wiki links (IWL) for companies from category NC is a clear indicator for their early state in the life cycle. The expected strong representation of the global players IBM is also visible, the number of linked pages for the Wikipedia page about IBM is about 12 times higher than the average number of linked pages those about other companies which also offer a Hadoop platform.
  
  
\label{ext.table.RepresentationOfCompetitors} 
\input{semanpix/table.RepresentationOfCompetitors/imageLS}


The relevance-plot in figure \ref{fig.RelpotHadoopMarket} shows  the representation index and relevance index of all selected Wikipedia pages, calculated for plain text pages from term statistics and for link structure as introduced in chapter \ref{chap.RELEVANZ}. The
static page network which represents this market is shown in figure \ref{fig.MarketNetwork}. 
This type of presentation allows a segregation of selected pages into groups according to an intuitive interpretation. Some companies like Dell and Facebook have a very strong focus on end users. They are well represented in the media and obviously also in Wikipedia. This is one reason for their higher representation index compared to other companies which are much bigger, but are more enterprise oriented, like Oracle or IBM. From figure \ref{fig.RelpotHadoopMarket} one can conclude, that pages with a relevance index $REL_v > 0.5$ are about those companies which are active in the B2C (business to customer) environment and companies with a lower relevance index $REL_v < 0.5$ are more B2B (business to business) oriented.
Wikipedia pages for \textit{Apache ZooKeeper}, and \textit{Apache Mahout} have a positive representation index $REP_v$ but a negative relevance index $REL_v$. This can be understood by looking deeper into those projects. Both projects are well established with a large developer community and they are older than five years. They provide core technology which is very fundamental but shiftetd into the background. Both are not attracting end users directly, and this means they are not generally adopted by a wide user community like the other projects Apache Hadoop, and Apache SOLR. A very high level of representation of the Apache UIMA project can be explained by it's closeness to IBM, which pushed and sponsored the project and finally gave it to the open source community. 
A negative representation index indicates an emerging topic which has not that amount of attraction yet, compared to it's local neighborhood in which it is embedded in. Especially pages with a very weak embedding show this property. Based on data from 2014 one can see the page of the company Cloudera with a negative relevance and representation index, which is a clear indicator for a new member in the global market. All the other Hadoop vendors have no international representation in Wikipedia yet. Because of this, 
the relevance and representation index for those pages can not be calculated and compared. 

% 
\label{ext.fig.RelpotHadoopMarket} 
\input{semanpix/RelpotHadoopMarket/imageLS}
% file:///Users/kamir/Documents/Cloudera/github/publications/2014.wikipedia.financial.markets/hadoop.market.report/fig.RELPLOT.hadoop.market.html
\clearpage 

\subsection{Page Network Properties}
What entities are related to each other and what are the most important clusters if such clusters exist? Such questions can be answered using network analysis on implicit page link networks from multiple interconnected local networks around the manually selected central pages as shown in figure \ref{fig.MarketNetwork}. One can see a strongly connected cluster of nodes which all describe open source software projects (top right, node color: light green). Multiple nodes cover programming languages, such as Java and C++. They form a cluster in the lower half of the chart (violet and dark green nodes). The majority of pre selected companies can be found in between those clusters. This is not a surprise and underlines the importance of a context sensitive analysis which applies relative measures and correlation measures instead of absolute values with unknown bias. An influence from strong clusters and dominating nodes in the neighborhood can be one reason for trends or even peaks in individual time series. The relative measure aims to eliminate this effect. 

\label{ext.fig.MarketNetwork} 
\input{semanpix/MarketNetwork/imageLS}
 
\subsection{Trend Analysis and Dynamic Relevance}
For the majority of the new companies in the Hadoop market no Wikipedia page was available prior to 2012. Pages about the central technologies, which can be seen like seeds around which the marked emerges and pages from global players which define some kind of boundary conditions have been used instead as a stub for a time resolved relevance analysis of the whole topic. The time resolved relevance index was calculated for nine pages as shown in figure \ref{fig.TRIHadoopMarket}. 

Companies like Oracle, New York Times, and Capgemini are considered to be the stable factors in the market and can be seen as a kind of reference while the Apache projects which are related to the Hadoop ecosystem are highly dynamic and therefore in our focus. 

We start with traditional trend analysis with data from Google Trends and single page click-count data from Wikipedia as shown in figure \ref{fig.GoogleTrends4}.a and \ref{fig.GoogleTrends4}.b which illustrates major problems and weakness of both approaches. The increasing interest in the topic Hadoop seems to be much stronger in keyword usage data than one can find in Wikipedia data. In order to get reliable results a re-normalization of the data is required, but anyway, trend analysis based on single time series is an error prone approach, because the neighborhood is not taken into account. Therefore the context sensitive relevance analysis seems to offer more reliable results which also consider the embedding of the topics into their local neighborhood.
 
\label{ext.fig.GoogleTrends4}
\input{semanpix/GoogleTrends4/imageLS}

Figure \ref{fig.MarketTrend} shows the local compared to the global time resolved relevance index calculated for daily access rates for the years 2009 to 2011. We consider $L.REL$ values larger than one as highly relevant. The highest representation index was measured for Apache Mahout followed by Apache SOLR and Apache Hadoop (see figure \ref{fig.TRIHadoopMarket}). The first two are directly related to Apache Lucene, a Java based search library which was started in 1999 and became an Apache top level project in February 2005. We selected the English Wikipedia pages as the central nodes because in IT business the English language dominates. Although English is a global language and stands for the global representation of the topic, we use the label $L.REL$ in order to be consistent with formula \ref{EQLREL}. Local representation means now: "within the IT business context" and the global representation covers all other languages with less importance in the IT sector.

\label{ext.fig.TRIHadoopMarket} 
\input{semanpix/TRIHadoopMarket/imageLS}

Figure \ref{fig.TRIHadoopMarket}.b shows the G.REL values for some of the pages, which already have many non-english pages in Wikipedia. In general those companies have a very low relevance in other languages than English.  

In the case of Apache Solr we can clearly see a significant change in the last quarter in 2010 where the average value increased by one order of magnitude and became stable at this level. For Hadoop and Zookeeper, an increasing trend can be recognized as well but more data has to be evaluated before a final statement is possible.

An interesting example for language dependency is the page about the French company Capgemini which illustrates the impact of a wrong selection of the context. The relevance index for English pages is low, but for all the international pages the high relevance index from French pages leads to higher value. We can clearly conclude, that the context has to be adjusted according to the topic, otherwise an unknown bias still exists. The best context selection is given if the difference between local and global relevance index is maximized.
\clearpage
\subsection{Diskussion}
From a marketing and business development perspective it is interesting to compare the new companies, which are improving the new and innovative technology with each other and most important with established global players. Companies with a strong commitment to Apache Hadoop, like Cloudera, MapR, Hortonworks, and Pivotal are the most important young companies in the market but regarding their representation in social media, especially in Wikipedia they are far away from early adopters which use the Hadoop ecosystem since the beginning. 

%MarketTrend
\label{ext.fig.MarketTrend}
\input{semanpix/MarketTrend/imageLS}

Because Google, Yahoo!, and Facebook use the public open source software Hadoop (or a comparable closed source technology in the case of Google) for their own services they should not be considered as direct competitors in the Hadoop market. But one question remains: Who is the global player which might attract the most Big Data experts? This question is important not only for investors but also for people who want to enter this market, either by running own services or by looking for a valuable position. 

We can identify sectors in the relevance plot which might be used for classification of topics. For now we can not generalize those results. More market studies like this one have to be conducted in other markets, like the automotive, mobile communications, pharmaceutical, or energy supply sector.

%BR
\label{ext.fig.BusinessRelevance} 
\input{semanpix/BusinessRelevance/imageLS}
%
% https://docs.google.com/a/cloudera.com/document/d/1X6Ni3fREo-P_vxkSjpV6Bt7iyxWBDi-2hoE6RZVOtus/edit#
%

This part of the study illustrates how Wikipedia allows tracking of public attention and public recognition of emerging topics based on content, contributed by a public crowd, which consists of self motivated editors in a self organized process - which sometimes might be influenced by very active and focused editors in a business context - and by access rate statistics which can be considered as a reliable data source. This approach allows to calculate the time delay between the increase of relevance of several contexts, e.g. the local and global context. There seems to be a critical value which can be an indicator for a transition in the projects life cycle, e.g. a break through in public acceptance.

We could draw a clear picture of a very young fast growing market. The relevance plot and also time dependent measures are in line with statements from domain experts and public recognition derived from several information channels which unfortunately can not be used for a quantitative comparison. 

Our approach can easily be generalized and extended to analyze content relevance and public recognition in arbitrary types of social communication and content networks. One of the most important technical requirements is availability of content together with editorial history beside access-rate data with at least daily resolution. If those different types of data sets would be available on all web servers the proposed approach could also be extended to any type of web resources. Web servers would have to provide content, collect metadata and publish such metadata in a reliable way. Especially in the context of the growing linked data cloud it seems to be very useful and promising because it would allow to investigate many more processes in a well connected society. 

\clearpage

\section{Case Study II: The Movement of Interest in Financial Market Data During the Global Crisis}
The bankruptcy of Lehman Brothers in 2008 was a shock for the global economy and especially for financial markets. Could this event have been identified using social media data and can we see language dependent or cultural differences to differentiate local from global impact? Furthermore we are interested in how information is distributed or how diffusion of information in social networks can be described and measured. The new methods are part of a framework which allows a comparison of multiple facets of complex systems. Subsystems can be studied without the need of a full separation from the surrounding neighborhood in a context sensitive analysis procedure. 

\subsection{Preparation of Wikipedia Data}
For this use case a set Wikipedia list-pages has been selected instead of single pages about a specific topic. Such a  list-page about a stock index contains several links to special pages about all companies included in the specific index. Traversing the inter-wiki links guides the crawler to list-pages about the same index in different languages using implicit semantic meaning. 

\label{ext.tab.CS2.tab1} 
\input{semanpix/table.StockIndizes/imageLS}

\label{ext.fig.REPPlot2Finanz} 
\input{semanpix/REPPlot2Finanz/imageLS}
\clearpage

Here we have a slightly different situation compared to use case one in the previous section. Because list-pages and their related nodes are covering the same topic the embedding in an off topic neighborhood is weaker here. The links from normal pages usually have a stronger embedding into off-topic content which forms the close neighborhood. Table \ref{tab.CS2.tab1} shows the page names of all list-pages for this part of the study. 

Such a purely data driven extraction method uses implicit properties included in the structure of local networks. This lowers the barriers especially in a multilingual global research context which can now easily be analyzed using data from a multilingual environment like Wikipedia. In order to achieve a high level of accuracy the data has to be verified and as a first result the relevance-plot is shown in figure \ref{fig.REPPlot2Finanz}. 
 
\subsection{Comparison of Correlation Networks and Page Link Networks}

How are those local networks inter-connected within and is the link structure between Wikipedia pages reflecting the market connections which have been found by Kennet \textit{et al.} \cite{Kenett2012}? To analyse this, the two networks are shown in figure \ref{fig.AccessActivityCorrelationForWikipediaPagesAboutFinancialMarkets} and compared to each other. 

%
% Abbildung von DROR
%
%\label{ext.fig.FinancialMarketData}
%\input{semanpix/FinancialMarketData/imageLS}

\label{ext.fig.AccessActivityCorrelationForWikipediaPagesAboutFinancialMarkets}
\input{semanpix/AccessActivityCorrelationForWikipediaPagesAboutFinancialMarkets/imageLS}

Figure \ref{fig.AccessActivityCorrelationForWikipediaPagesAboutFinancialMarkets}.b shows four groups of local page networks from Wikipedia which are obviously intensively used as a source for information about financial markets. Multiple Wikipedia projects in several languages are inter-connected by so called inter-wiki links and the network shows also the first neighborhood around four central nodes for the stock market indices Nikkei 225 (Japan, red), DAX (Germany, blue), NASDAQ 100 (U.S. green), and BSE 200 (India, orange).
The highest link density is found in the local network around the Japanese index Nikkei 225. The list-pages for the two Asian markets (red: Japan, orange: India) are linked directly via one intermediate page while the pages for the German index DAX (blue) and the American index NASDAQ 100 are not connected directly to each other. 
 
\clearpage
\subsection{Language Dependency of Market Representation in Wikipedia}
Before the multilingual data set can be used in appropriate way to study dependencies and correlations between international financial markets, one has to understand what language specific properties in regarding topic representation and usage patterns exist. Figure \ref{fig.RelevancePlotForStockmarketWikipages} show a relevance-plot for pages in German language in panel a) and in panel b) for the corresponding English pages for selected stock market indices shown in table \ref{tab.CS2.tab1}. The average relevance index for pages in German language is higher than for English pages, this means one should two different results can be found, depending on the selected language of the study. In order to study and compare the interest of users by language such a context differentiation is useful and has to be repeated for all languages included in the study. Instead of context sensitive segregation a global aggregation would help to collect more individual time series per topic (in this case per company, included in an market index). A context differentiation can be done based on Wikipedia categories or one can also use the second neighborhood, which usually is defined by off topic pages. Instead of a simple correlation analysis for each individual time series one can now use the time resolved relevance index for each individual market. This approach is comparable with meta-correlation analysis presented by Kennet et al. in \ref{Kennet.METACORRELATION}.



%RelPlotStock
\label{ext.fig.RelevancePlotForStockmarketWikipages} 
\input{semanpix/RelevancePlotForStockmarketWikipages/imageLS}




Figure \ref{fig.TRIFinanz} shows a language specific difference in the long term properties of the time resolved relevance measure derived from Wikipedia access-rate data. While for the local index (pages in English language) only a short shock can be recognized in the global index much stronger trends can be found. A time resolved index would have to be calculated for all languages to localize the reason for the trends. 
 
%Thank you for downloading. Please keep a record of the following
% information for future reference:

% Product: Mathematica Student Edition
% Platform: Macintosh

% License number: L4846-5085
% License expiration: 27-MAR-15
% Activation key: 	4846-5085-6JHP2R (enter during installation to activate product)

The comparison of a dependency network and a Wikipedia page network illustrates how important a qualitative investigation of network properties is and how it supports the interpretation of final results. In some cases a language specific segregation is not useful, especially in case of a global financial market, in which people and traders from all countries using all languages operate. Measuring the impact of a global shock in a language specific context is good example using the new relevance measure. 
 \label{ext.fig.TRIFinanz} 
 \input{semanpix/TRIFinanz/imageLS}
 
 
 
\clearpage
\section{Case Study III: Correlations in Stock Trading Time Series and Wikipedia Access Rate }
\label{CASEIII}
High market volatility and spontaneous shocks in stock markets are related to increasingly heavy news coverage. Such activity can be considered as one reason for increasing interest in financial topics also in Wikipedia as well. Time series correlation analysis is applied to study the coupling between markets and social media applications. Even if a causal dependency analysis is not possible we expect to find indicators for existence of significant correlation or event synchronization during different market phases, such as times of high market volatility, during shocks, or during stable market periods. 

This third case study summarizes preliminary results. An obvious time dependence of cross-correlation link strengths in bi-partite functional networks could be found. The time delay between the two time series for which the cross-correlation is maximized shows also a significant difference if compared to randomly shuffled data. It is important to note that link strength distributions are not stable over time because the underlying process is non stationary.

One can assume, that during periods of massive price changes in stock markets, the correlation in measured access activity increases as well because people use the content network more often to lookup background information. Because many different media channels, especially news channels might be considered to play the role of mediators in a global communication process, a time delay of one at least one up to several days can be expected. A delay shorter than one day can not be applied because of technical reasons. The resolution of available financial data is limited. Strong daily patterns would have to be normalized according to time zones but click count data contains no details about the location of the user.  

One has to relate Wikipedia pages about companies included in stock indexes to the financial time series of those indexes. If the network structure of the internal dependency network of a market is known, the Wikipedia neighborhood networks can even be taken into account. This allows a differentiation between the elements which are involved in a coupling process and such which are loosely connected in the neighborhood but not directly involved in information flow. Even if the average correlation between the time series data sets is weak, one can show a systematic change in internal correlations compared to inter correlations that way (see figure \ref{fig.ICFvsNeighborhoodCorrelation}.a). The core of the local context network attracts significant more traffic compared to the surrounding neighborhood networks. This leads to a stronger correlation between core pages. 

We adopted the concept of the "Index Cohesive force" (ICF), introduced by Kenett \textit{et al.} \cite{Kenett2012}. In section \ref{adopt.ICF} we introduced the "Context Cohesive Force" (CCF) based on a comparison of the cross-correlation within core and hull of local neighborhood networks. This allows a differentiation between a local and a global communication context defined within the local neighborhood network and seems to highlight properties of the underlying information flow. Preliminary results are shown in figure \ref{fig.ICFvsNeighborhoodCorrelation}.b for Wikipedia pages around three financial markets. 

What is the time shift between stock market activity and related Wikipedia access? Using a sliding window technique we could find the strongest peak at a delay of six days for short window lengths. For longer windows the peak gets less expressive but the location is stable. This shows clear differences between Wikipedia usage and Google keyword search statistics. According to Bordino \textit{et. al.} \cite{Bordino2012} there is a clear delay of one day between the Google keyword time series and financial data. This results lead to the conclusion that ad-hoc search via web-search engines and retrieval of background information from Wikipedia are different processes on different time scales in the range of one day to one week. 


%Furthermore daily access-rate time series and daily trading data is used to reconstruct correlation networks. Such networks expresses the strength of intra-component correlations and central nodes in such networks might be important factors which influence the overall process, therefore it is interesting to measure how correlation networks and structural networks differ Such differences might be useful to explain dynamics of complex processes.\\ 
%
%Inter-component correlations are calculated between different types of elements and the results are bi-partite networks and describe the coupling of different subsystems which form a complex system implicitly. One important questions remains: How to define links between elements and subsystems? How are time delays handled correctly and how are artifacts identified? In this work the cross-correlation as well as the event-synchronization algorithms have been investigated and used for computation of adjacency matrices, either for single snapshots or by using sliding window techniques as time dependent networks. \\ 
%
%This allows us to apply and compare several filter techniques and to characterize properties of these networks as a function of time. Such fucntions can be helpful response functions for system studies on a larger scale. Our approach combines time series analysis methods, which include the correct filtering, cleaning, and aggregation or even transformation of measured raw data. Our results show, that these aspects can not be handled in a unified method - it depends on the properties of the measured data and the kind of effects that should be analyzed, e.g. global interest from all users or even localized effects which can be studied using the context sensitive approach which is based on the semantic meaning of several different link types in local page networks. 

\subsection{Data Analyzed}
For two pairs of data sets, each containing one subset from the social network Wikipedia and one from historical stock market data, we reconstruct two functional bi-partite networks. The two sub data sets for this case study are the access-rate time series from Wikipedia and the trading data of the German stock index DAX and the American index Standard and Poor's (S\&P) 500 recorded daily over a period from 1st of January 2009 to 30th of September 2009. The DAX data set contains time series for 30 companies and the S\&P 500 data set contains data series from 500 companies. In this initial study we considered only Wikipedia pages if the title matches the company name. Such pages are not available in all languages because of the representation-bias, found in many topics. The embedding of pages into their neighborhood was taken into account for the correlation analysis between Wikipedia and market data, only for the Wikipedia internal correlation analysis as reported in figure \ref{fig.ICFvsNeighborhoodCorrelation}.
%Here it is not relevant if the Wikipedia page is well embedded in the local neighborhood network or not. 
The data set catalog in chapter \ref{chap.DATA.Initial} provides more details about the data set.

Several time series types are available form stock market trading activity via Yahoo! Financial Services \cite{Yahoo.Financial.Services}. Beside trading volume (TV) the logarithm of daily returns (LRP) have been used in this work The absolute value of LRP ($\vert \textrm{LRP} \vert$) of traded stocks has been used as a stub for stock volatility.

But which is the most reliable metric to be used for a correlation or dependency analysis and what time scales are relevant? 

According to \cite{Krings2012} % Krings2012} : Effects of time window size and placement on the structure of an aggregated communication network 
the selection of an appropriate window size is a critical factor in sliding window techniques. A good selection should contain time  windows of a length, which fits to the assumed process and also such, for which no significant correlation is expected. Seasonal trends like the weekly access patterns in Wikipedia access data can cause artifacts. Such misleading
results can be identified by a comparison of results obtained from multiple time scales. Time series episodes have to be of the same resolution and length and they have to be aligned, which means the start time must be equal. This is a very important requirement and can be achieved either by removing values from both series or by filling in missing values to hide gaps. Such gaps, or missing data can be caused by technical problems or in case of trading data, by the nature of underlying processes. During bank holidays and weekends no data is available. Removal of such data points influences the natural properties of time series. The frequency of an underlying oscillation will be increased and the pattern is also changed. This leads to artificial frequencies in the data series. Here we calculated the average cross-correlation links strength $s_\textrm{p}$ and the standard deviation for the link strength distribution for non overlapping episodes of length $l \in \lbrace  20, 40, 60, 80, 100 \rbrace $ days as a function time and the delay $\tau \pm 5$ days. The selected delay is related to a standard trading week of 5 days. For comparison with result presented by Bordino \textit{et al.} in \cite{Bordino} we extended the range of delay values to $\pm 20$ and the length of episodes to $l \in \lbrace  50, 100, 150, 200, 250 \rbrace$.
%{Web Search Queries Can Predict Stock Market Volumes}, 
%http://www.plosone.org/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0040014

\subsection{Preliminary Results and Discussion}
So far, only a weak indicator for a significant correlation between Wikipedia pages about financial topics and stock markets was found. Preiss \textit{et al.} reported a kind of an indirect connection between Wikipedia and stock markets in a recent study \cite{Preis2014.WIKIPEDIA.STOCK} using Wikipedia user activity as an influencing factor to a trading strategy. The strategy is called 'Google Trends strategy' and seems to be much more successful than the traditional ‘random investment strategy’. % http://www.nature.com/srep/2013/130425/srep01684/full/srep01684.html


One reason for the low quality of our current results was a technical limitation. The amount of data was not sufficient in the beginning. An even more important negative influence was identified as the \textit{"hidden bias"}, also called representation-bias. This bias is caused by non comparable representation of topics in Wikipedia such as companies, stocks and stock indexes. Representation of Wikipedia pages varies by topic and even more by language. A solution to this problem was developed and is illustrated in figure \ref{fig.REPPlot2Finanz} in the previous section. 

The first problem can easily be addressed in future projects, because monthly Wikipedia access-rate data is now available at an hourly resolution. The data set is updated each month. The representation-bias can be analyzed as shown in chapter \ref{chap.RELEVANZ}. Such a representation-bias is visible in a representation plot as a wide spread cloud of bubbles of different size. But if all bubbles are within a small area and of a comparable size, no such bias or even a negligible small bias exists. 

\subsubsection{Comparison of Link Strength Distributions for Different Metrics}
In chapter \ref{chap.CREATION} we investigated several computational approaches for creation of functional networks. Here we compare the link-strength distributions for the Pearson correlation and the normalized correlation with a delay of $\pm 5$ and later also $\pm 20$ days for a bi-partite network from stock trading time series and Wikipedia access-rate time series to illustrate the hurdles towards a meaningful interpretation of such result. 

Figure \ref{fig.TimeDependentLinkStrengthDistr} shows the link strength distributions for episodes of different length (n=20, blue line; n=60, black dots) from raw data and from surrogate data (blue and black filled area respectively) for three different financial data series (TV, LRP, $\vert \textrm{LRP} \vert$) from German stock index DAX. 

%The Shapiro-Wilk test was applied as a significance test to show if the distributions are different from the top row are of a
% Gaussian distribution. For both types of distributions we use the Kolmogorov-Smirnow test to show if both distributions are
% significant different. Table \ref{fig.table.CaseStudyIII.stests} shows calculated test results.


%%%
\label{ext.fig.TimeDependentLinkStrengthDistr} 
\input{semanpix/TimeDependentLinkStrengthDistr/imageLS}

%\label{ext.fig.table.CaseStudyIII.stests} 
%\input{semanpix/table.CaseStudyIII.stests/imageLS}
  
We found that the distribution of link strengths from surrogate data is very stable over time while the link strengths from raw data obviously change as a function of time. This indicates that information about dynamics of the coupling process might be extracted from that kind of data. Even if this approach gives a first indication about information which might be included in the data it is no clear result yet. It is known, that a plain cross-correlation analysis is not very stable. Especially the influence of peaks - as discussed in section \ref{sec.inflPeaks} - has to be taken into account. Another improvement was achieved, by using the median of the cross-correlation value for a set of 30 episodes with hourly resolution to represent the correlation during a month - as shown in section \ref{sec.medianLinkStrength}. A meaningful adaptive filtering, e.g. based on a time dependent threshold as shown in section \ref{sec.adaptiveThreshold} can also improve the quality of the analysis as it might produce more stable results, independent from unknown influences which systematically change the average link strength. 

As figure \ref{fig.TimeDependentLinkStrengthDistr} suggests we choose the metrics TV and $\vert$LRP$\vert$ for further analysis because the differences between correlations calculated from real data are significant different from surrogate data for short and also for longer episode. The metric LRP shows such a difference for short episodes only. This distributions already indicate a strong influence of the episode length. We investigate this aspect more in the following subsections.

\subsubsection{Influence of Strong Peaks on Cross-Correlation} 
Here we investigate the influence of strong peaks on correlation functions. On requirement for application of the cross-correlation is, that all values have to drawn from a Gaussian distribution. The Shapiro-Wilk test is used together with a filter threshold $p_t = 0.9$. For a high p-value ($p > p_t$) the criteria matches, in case of smaller p-values ($p < p_t$) we skip the results for this time series. Figure \ref{fig.SWTestInfluence} shows a comparison of average correlation functions for those two groups applied to DAX data set.
\label{ext.fig.SWTestInfluence} 
\input{semanpix/SWTestInfluence/imageLS}
For trading volume time series with small p-values the correlation is slightly higher for delays larger than $\tau=2$. The overall shape is not changed by the filter in all three metrics. This allows the conclusion that the influence of the peaks is visible but it is not changing the average correlation function in this case. 

One can use this filter threshold to split the time series into two parts, a continuous part and an event time series for further analysis. We set $p_t = 0.95$ according to the level of accuracy which is $5\%$ in this case. All values higher than a variable threshold $t$ are extracted and stored in an event time series. If the p-value for the restricted time series is still smaller than $p_t$ one decreases $t$ and extracts more values which are stored in the event time series iterative until the filter criteria matches. The gaps produced by this procedure are filled using interpolation. The interpolated values are removed from the event time series. The two time series describe two different aspects of the underlying process and can be analyzed independently.

\subsubsection{Correlated Information Flows During Global Financial Crisis}
In this case study we investigate two different types of connections which are considered to be results of information flows. The first one is the internal correlation for pairs of access-rate time series within Wikipedia and the second is the correlation between Wikipedia access activity and stock market metrics (see the following subsections). 
%\label{ext.fig.ICF.SAMPLE} 
%\input{semanpix/ICF.SAMPLE/imageLS}
\label{ext.fig.ICFvsNeighborhoodCorrelation} 
\input{semanpix/ICFvsNeighborhoodCorrelation/imageLS}
%\label{ext.fig.ICFvsNeighborhoodCorrelation2} 
%\input{semanpix/ICFvsNeighborhoodCorrelationPart2/imageLS}
We start with a comparison of link strength distributions as a function of time (for episodes of length $l=28$ days. Figure 
\ref{fig.ICFvsNeighborhoodCorrelation} (a,c,e) show the results for the stock indices DAX (DE, top), NASDAQ100 (US, middle), and Nikkei (JA, bottom). The Context Cohesive Force is calculated and presented in figure \ref{fig.ICFvsNeighborhoodCorrelation} (b,d,f) for the local communication context (black line) and the global communication context (colored line). This comparison allows an interpretation of correlation properties which seem to be related to events in the real world. The shape of the curves is to some extend similar and smaller deviations illustrate differences in the local and global properties. Local and global are defined by the chosen languages. In this case local means all pages in German language and global refers to the group of all other languages in Wikipedia. This curve allows also a comparison of market or topic specific properties. Here I do not interpret the features of the CCF. This figure shows a first step towards a measure, which allows a comparison of topics, such as financial markets based on correlation properties. How those properties are related to structural properties and to real world events has to be analyzed in future projects using the context cohesive force as a generic metric which expresses a collective property of an ensemble of nodes in one single value which takes the individual properties of all elements into account.


%\newpage
\subsubsection{Influence of Time-Delay $\tau$ on Cross-Correlation} 
From all cross-correlation functions from for episode of different length (20 ..., 40 ..., 60 ..., 80 ..., 100 ...) $F_{cc}$ we calculated the average fluctuation function $ \langle F_{s,\tau} \rangle$ shown in \label{fig.CCtauA} for the DAX data set (top row) and the S\& P500 data set (bottom row).

\label{ext.fig.CCtauA} 
\input{semanpix/CCtauA/imageLS}

Data was filtered based on properties of the input series. Only if the values of the 
raw time series were of a Gaussian distribution and $t_{SW} > 0.85$ - the correlation-function contributes to the final result, otherwise it is skipped. This approach allows to identify artifacts, if such are caused by input data with properties, non suitable for cross-correlation analysis.


%\label{ext.fig.CCtauC} 
%\input{semanpix/CCtauC/imageLS}

A different view is provided by the average of the maximum of the cross-correlation functions plotted as a function of the delay. Results are shown figure \ref{fig.CCtauD}. In this case the maximum link strengths are obtained, as we shift the Wikipedia access-rate series to the past by one or two days. In all figures we can see a clear difference between the results obtained form filtered data and surrogate data.

%For a delay $\tau=1$, which means that the stock market time series are shifted by one day to the future, one can see a maximum in the average cross-correlation strength in row 1 and 2 on page 0 and in row two in page.

\label{ext.fig.WikiFinanceCorrelations} 
\input{semanpix/WikiFinanceCorrelations/imageLS}

After I extended the range of analyzed time delays to $\pm 20$ to test the hypothesis that Wikipedia acts as a long term memory because it takes some time until Wikipedia pages reflect changes in reality. This might be because people lookup background information not instantly but after a period of time. Than they come back and read more details. A second reason might be the media life cycle. It takes some time to transfer information and if people react on information which arrived later they contribute to such a time delay as well. What is the time range for such delay? Figure\ref{fig.WikiFinanceCorrelations} shows this effect for five different window lengths with a clear maximum at $\tau = 6$ days. The longer the time episodes are, the weaker the maximum is for metric $\vert$LRP$\vert$ (orange and green line). For the index DAX (green line) with less companies this effect appears already for episodes of length 150 days. For the large index S\&P500 only the intensity is decreased while the overall shape of the curve especially the location of the maximum is stable. This shows, that for smaller groups this method can only be used with short episodes with $ l <= 100$ days. 
 
This delay of six days is a much longer delay compared to Google keywords usage. Google is obviously used as an ad-hoc method for information retrieval. While Wikipedia is more a long term memory. Sometimes Wikipedia is considered to be a global brain. This results show, if such a global brain exists, it might be build from multiple complementary parts. Such a complex global brain contains long term memory which is accessed via contextual associative access strategies such as web-pages, web-portals, Wikipedia, blogs, etc., and short term memory. This short term memory supports a kind of fast random access to content, like web-search engines and personal information management systems (PIM) do.

Reasonable results can be found for the metric $\vert$LRP$\vert$. We can conclude, that Wikipedia access-activity is about six days behind the stock market activity. But surprisingly, the local minimum at $\tau = 0$ and the small local maximum at $\tau = -2$ indicate also a certain amount access activity with an opposite behavior. Those contributions precede the stock market activity. This means not, that Wikipedia influences the stock market directly but we can see at leas a connection between both systems on two different time scales. 

Furthermore this case study shows, that for the relation between information access on Wikipedia and a change in stock market trading characteristic time delays exist. One has to be careful with this preliminary result. Especially using more Wikipedia categories with obvious relations to financial topics and also such with no direct relations should be considered 
to strengthen the conclusion based on systematic comparison. How such data sets will be selected for a future study is explained in the final subsection. 

%
% Include this as a citation ...
%

\subsection{Further Improvements}
Our initial questions: "Has Wikipedia an influences on financial markets?" and: "Is information flow via Wikipedia related to market dynamics?" are interesting but not specific enough to be research questions. As a result of this work I have to rephrase those initial questions. Furthermore, I suggest an extended data preparation procedure. We will collect and extract more data about historical events, such as strategic activities of large companies, political events like elections, the begin and the end of a war, or cultural events like the presence of a song in music charts, a premiere of a movie, the Olympic games, or the world championship of several sports disciplines to define episodes. Such episodes must contain data from two phases, e.g. some time before and after the event. One can define such events automatically based on extreme event detection as shown in section \ref{sec.EXTREVENTS}. An automatic detection of extreme events was successfully applied to the Wikipedia access-rate time series as well as to other social media applications like YouTube and Twitter. Such an approach is more general and independent from any particular topic.

Correlation analysis can be applied to such episode pairs and results can be compared across disciplines or domains. Using statistical tests one can now find if properties like link strength distributions, the time resolved relevance index or any other property is significantly different for both time frames.

Currently the significance test is based on randomly shuffled time series, because all correlation is destroyed by the shuffling procedure. Randomly chosen time series from other topics, which are assumed to be independent in the given research context, can also help to identify a systematic bias, which might exist but not quantified yet. If a change in correlation-strength can be detected for the manually selected data set and also for some sets of randomly selected pages, one has found an example which shows the opposite. This means a casual dependency can not be identified that way for the data about the chosen topic.  

Instead of an expensive calculation of long time dependent properties during long stable phases an event driven approach is more efficient. It allows a higher number of episodes which show specific properties - an extreme event in this case - no matter what the reason or the meaning of the event is. Such an approach was already used to analyze the fluctuation properties of Wikipedia access-rate data (see section \ref{sec.DFA}) for two different groups. The time series of one group was considered to have stationary access-rate time series. Time series containing extreme events have been grouped together in the second group.

As a conclusion of this work we recommend a normalization of the access-rate time series data. Instead of the average correlation function for all pages one should calculate the correlation between the time resolved relevance index and the financial data series. The major benefit from this approach is a larger input data set consisting of all access-rate time series for all pages about the same topic in all available languages and the direct neighborhood - also from all available languages - as a reference. This allows a relative measurement instead of a biased absolute measure for which no calibration exists.

A more detailed quantitative analysis and a more systematic interpretation is required. The methods developed in this work will help to automatize the software tools for advanced studies on larger data sets. The focus of such a study will be on the role of Wikipedia as an important element in global communication processes. Beside the passive role as a global multilingual memory Wikipedia has proved to be a useful stub to measure interest of people in a variety of topics. Several studies illustrate how extreme events in Wikipedia access activity, no matter if endogenous or exogenous, are caused by real world events. 

In a future study the new method will be applied to the same set of pages but instead of the daily access rate, a normalized data set, the time resolved relevance index, as defined in equation (\ref{EQ.RELv}) is used. Because the new methods take language diversity into account and because they enable fine grained context selectivity more robust results can be expected in future projects. 



%
%
%
%
%  Now I am ready!!! What are the final conclusions????
%  ====================================================


\chapter{Conclusion and Summary}
This section addresses all topics from INTRO

(1) Response Theory

(2) LSI, GSI


\textbf{DICUSSION OF WHAT WE FOUND ....}
%ETOSHA.TASK Create a slide to illustrate the LSI and GSI

\textbf{Results and Conclusion: }
absolute and relative salience index

absolute:  general dominance of media
related to relations between countries (economic, political, and cultural)

relative:
media in the context of one country
related to specific characteristics of a country (p. 50)

relative approach incorporates the context; enables a better comparison

former studies emphasize an US dominance 
former study shows Bosnia in the highest rating as coverage at the time period was high (ibid.)
Generating a global and local salience index (GSI and LSI)
GSI show US as being the most significant throughout tested media
LSI shows that Israel is best imbedded in its news (meaning mentioning itself in its own news) 
interesting differences between outer and inner salience (see e.g. Russia)


- in global context countries can be more/less significant than in their local (homeland) context

- context important feature of evaluation

takes difference between GSI and LSI and compares it with Failed state index (FSI)

- instable countries (after FSI) tend more to have a higher difference between GSI and LSI than stable states

Does not take into account, that events (same semantic concepts) can be reacuring over a certain period of time, therefore changing the outcome (e.g. natural catastrophy can issue day-long and recurring coverage; counted multiple times)





(P1)
(P2)
(P3)
(P4)

C1,...,7)

Address all \textit{Scientific Distributions} and connect to what comes next - WHICH ADVANCED APPLICATIONS.

\textbf{This is related to the introduction !!!}










\textbf{\textit{Here I have to show, how this new method is in line with the theoretical work from Zhu Wang (Learning, Diffusion and Industry lifecycle).}} \\

\textbf{I have to explain the importance of context differentiation and multilingual approaches.} \\

\textbf{I want to explain what is wrong with approach in Case Study II.} \\

 
Content of Wikipedia pages as well as the link structure between
pages are often handled like static properties, measured at a
fixed point in time. But neither Wikipedia nor the World Wide Web are static,
both evolve over time. This implies that analysis methods and results have
to be time dependent as well.

In order to allow such a time resolved analysis of page content and structural
properties of link networks, also metadata, e.g. the usage data has to be
aggregated and stored as part of the core data set.
 
Many public data sets are already available but often they are unknown to
potential users in the scientific community and the documentation of the data
collection procedures is of a low quality or not available at all.
Right now, there are no supporting tools available for management of 
multifaceted layered data sets. But it will be an important task within many
future projects, to collect descriptions of best practice. This will
improve software tools and the data management procedures for data set
integration, especially in interdisciplinary research projects on a large 
scale.

An exemplary time resolved analysis procedure is already implemented for the
time-series analysis part (cross-correlation and event-synchronization) in the
Hadoop.TS package. The Mediawiki-API allows easy access also to historical data
of older versions of Wikipedia pages. This feature was used to load the text
corpus of a local network and it's neighborhood around a center node. Temporal
snapshots are required to do a time resolved analysis also for text volume and
node degree analysis. Right now we are limited to a fixed time. A future
version of the Mediawiki-Explorer will work with a more efficient caching
system for page content, page history, and time series data. This will allow a
flexible real time analysis of information flow based on Wikipedia usage, which
works as a stub in this case.

Just by looking at a graph structure, one has no access to implicit semantic
knowledge
because this is encoded in the type of a link, in our case it is defined
only for inter-wiki links. To define different types of context,
beside the linguistic context we apply here, the links have to be filtered
and interpreted based on other very specific semantic meaning or even just
based on simple measurable properties which can define a sensible context.

The underlying idea of equal semantic concepts which are linked to each other
within the core of the local network by inter-wiki links and a
surrounding hull-network which defines the local and global neighborhoods by
simple Wikipedia page links supports the assumption for our current context
sensitive analysis, but this should be generalized.

In the future, more advanced applications of this approach will be possible. The
page groups will be selected by parametrized queries or by semantic
queries. Such applications use the SPARQL Protocol
and the RDF Query Language (SPARQL) \cite{SPARQL} because this allows a
very flexible definition of any appropriate research context based on an
ontology.

Some more steps are necessary in order to create a generalization of the
current approach towards an integration with other types of social networks
especially in conjunction with semantic networks like DBpedia and Google
freebase. Such systems define the scope or the context a network node is related
to but they have a more technical focus compared to Wikipedia which is build by
human users for human users. Even if such a network node is not a text page
but arbitrary data, derived from a social network node or even a technical
system its context can be well defined.

Based on such a generalization the proposed analysis framework will work with
data from many different sources and is not limited to content networks like
the network of Wikipedia pages used in this initial study.

Right now, the core functionality of our tool set is limited to specific
analysis questions and a certain type of data. It is not yet
possible to incorporate Twitter messages or Facebook posts into our analysis
easily. But in the future, an integration of multiple data sources will enable
us to measure the interdependence of multiple concurrent Social Media
Applications.

Researchers from social science, physics and computational science are getting
closer. The appearance of the new field called socio-physics is also a really
strong indication for this and the large number of joined interdisciplinary
research projects is a welcome result our work should support.

\subsection{Dynamic Ranking of Search Results}

\textit{\textbf{Idea: Instead of the query-corpus relation we use a query-corpus * (query - reference corpus) relation which introduces time dependent weights, which directly influence the results ranking.}}

One can pre-calculate relevance networks for several topics in advance. Highly active nodes would For several topics

The salience index was used by Segev et. al. \cite{Segev2010} to analyze news
articles from different countries and different media channels to study the
representation of Israel and the self-representation of different nations in the
global media.

Because such data is not easily accessable nor can it efficiently be analyzed on
a large scale, a new approach is proposed in this paper. It  uses the global
Wikipedia database. Therefore, Wikipedia can be seen as global brain which
remembers many things in an associative network
\cite{Schnabl.1972,Russell.1983}. In such associative networks an ontology can help to describe the meaning of individual nodes and
network links. The relevance of nodes, in the case of
Wikipedia these are the terms or topics represented by individual wiki pages,
can be calculated and compared to each other with complex network techniques. 
 
Importance ranking algorithms in search engines like Google search, Yahoo, or
Bing use many different data sources and data types. Their major goal is, to
provide a well localized search experience to users, often dependent on their
current location. But this is just one of many possible contexts.

Search results vary strongly depending on the search
query and the conditions in which the query is submitted. The TF-IDF algorithm
is usually used in search engines, which are build with software libraries like 
Apache Lucene. The term frequency (TF) and the inverse document frequency (IDF)
are calculated for the full text corpus and stored in an index. The TF-IDF value
defines a value for each search-term document pair, which expresses
the importance of this term within a document in the context of the full
text corpus. But this approach is limited
to the page text and can not express how relevant a term is within a certain
subset of documents of the corpus.

In order to create an alternative context sensitive ranking of documents within
a
search result set, one can use the local representation index of topics derived
from Wikipedia, even if the
documents, which are part of the result set of a search request, are not
Wikipedia articles.

Hopefully in the future, after this approach is generalized, data from other
social networks can be used to calculate such a SMA dependent representation
index. This would allow a faceted ranking of given search results where Social
Media Applications and users can define the facets depending on existing data
and their own needs.

\subsection{Hierarchical Language Trees and Relevance Maps}

\cite{Chen2010}


How relevant is a topic or a web page, e.g. a Wikipedia page, for a certain
region on earth? A relevance map like shown in Fig.S2 and Fig.S3
can help to answer this kind of questions.

% \begin{figure}[!h]
% \begin{center}
% \includegraphics[width=6.5in]{images/fig_10.eps}
% \end{center}
% \caption{
% {\bf A future application of $\rm REP$ and $\rm REL$ measures
% are.\textit{relevance
% maps}} A relevance map for the Wikipedia page \textit{Bundu\_dia\_Kongo}
% (a) which has the highest representation in english language and one
% for the page \textit{Hema\_people} which has the highest representation in
% french language (b) show a higher localized representation of the concepts.
% For comparison we show in (c) a relevance map for the Wikipedia page
% describing the company Deutsch Bank with a clear global representation.
% All three panels show the country related to the language, for which the
% highest local representation is determined in light blue color.
% This type of visualization can not show the importance of
% a given page or concept as a function of time because the content is
% aggregated
% in time. Therefore the relevance map are calculated based on $\rm REL_a$
% measurements (see supplementary material \cite{suplementary}).
% For simplicity an exact context dependent mapping of language usage to regions
% on the globe is not considered here, only the regions there a given language
% is
% the main language are considered. The application of Hierarchical
% Language Trees (HLT), which is out of scope of this paper, will close this
% gap.
% }
% \label{fig.APP.10b}
% \end{figure}

A relevance map shows, how important a semantic concept or a term for which a
Wikipedia page exists is for a given region at a given time based on a
weighted geo-spatial mapping. One has to calculate a projection of the data
measured in lingual space to the surface of the globe, which is structured by
regions. Such regions can overlap and build a hierarchy of multiple layers
which allow a variable resolution. Results can be shown for
continents, countries or even smaller regions like counties and even cities. 

Color coded regions on earth are plotted, based on the assumption, that
importance or relevance of a semantic concept or a term is related to it`s usage
in human language. This is measured by the volume of text which is available in
a language or by the access rate which shows the amount of attention that users
pay to this topic. The creation of a relevance map for a document
requires the following steps:

\begin{enumerate}
 \item selection of relevant terms (with corresponding Wikipedia page)
 \item extraction of static networks and access time-series data
 \item calculation of $\rm REP_v$, $\rm REP_k$, and $\rm REP_a$ for each term
and its
related IWL pages
 \item build term-layers based on a projection from lingual space to real space
\end{enumerate}

Not all languages are spoken and understood in all regions equally. For each
region on earth a multi-lingual relevance vector of dimension $z_l$ (number
of Wikipedia projects) can be defined. The components of this vector are the
measured values $\rm REP_z$ for each individual language ($\rm REP_z = 0$ if a
page in language $z$ does not exist) as a weight which represents the relevance
of the term within each available language depending on the page properties.
The total relevance is calculated as the weighted sum, there the weights are
derived from the importance a language has within a region. In our examples (see
Fig.S2 and Fig.S3), the weight is $\rm REP_z=1$ if the language is an official
language otherwise $\rm REP_z=0$.


%%%
\label{ext.fig.SemanticSearch} 
\input{semanpix/SemanticSearch/imageLS}
 

The salience index was used by Segev et. al. \cite{Segev2010} to analyze news
articles from different countries and different media channels to study the
representation of Israel and the self-representation of different nations in the
global media.

Because such data is not easily accessable nor can it efficiently be analyzed on
a large scale, a new approach is proposed in this paper. It  uses the global
Wikipedia database. Therefore, Wikipedia can be seen as global brain which
remembers many things in an associative network
\cite{Schnabl.1972,Russell.1983}. In such
associative
networks an ontology can help to describe the meaning of individual nodes and
network links. The relevance of nodes, in the case of
Wikipedia these are the terms or topics represented by individual wiki pages,
can be calculated and compared to each other with complex network techniques. 
 
Importance ranking algorithms in search engines like Google search, Yahoo, or
Bing use many different data sources and data types. Their major goal is, to
provide a well localized search experience to users, often dependent on their
current location. But this is just one of many possible contexts.

Search results vary strongly depending on the search
query and the conditions in which the query is submitted. The TF-IDF algorithm
is usually used in search engines, which are build with software libraries like 
Apache Lucene. The term frequency (TF) and the inverse document frequency (IDF)
are calculated for the full text corpus and stored in an index. The TF-IDF value
defines a value for each search-term document pair, which expresses
the importance of this term within a document in the context of the full
text corpus. But this approach is limited
to the page text and can not express how relevant a term is within a certain
subset of documents of the corpus.

In order to create an alternative context sensitive ranking of documents within
a
search result set, one can use the local representation index of topics derived
from Wikipedia, even if the
documents, which are part of the result set of a search request, are not
Wikipedia articles.

Hopefully in the future, after this approach is generalized, data from other
social networks can be used to calculate such a SMA dependent representation
index. This would allow a faceted ranking of given search results where Social
Media Applications and users can define the facets depending on existing data
and their own needs.
\newpage



\newpage
\bibliographystyle{unsrt}   
% this means that the order of references
% is dtermined by the order in which the
% \cite and \nocite commands appear
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{LV_V3}  


%#################################
%#
%#   THIS is the SECONDARY RESULTS Part ....
%#
%#   (1) Evacuation SIMULATION paper 
%#   (2) Traffic Phase Analysis
%#
%#################################
\appendix 
\part{Further Applications}

%
% Here I include the full paper, published during SOCIONICAL 
%

\include{part3a/wa7incl}

\include{part3b/v22MK}

\clearpage
\newpage
\addcontentsline{toc}{chapter}{List of Figures}
\listoffigures

\clearpage
\newpage
\addcontentsline{toc}{chapter}{Nomenclature}
\printnomenclature

%
% 
\include{part3c/thanks}
%
\include{part3c/affirmation}


\end{document}
